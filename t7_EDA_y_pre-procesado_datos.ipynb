{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bd0ad5-cdfe-433e-9d38-a4b26cc2de12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=6>\n",
    "\n",
    "<b>Curso de Análisis de Datos con Python</b>\n",
    "</font>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "Curso de formación interna, CIEMAT. <br/>\n",
    "Madrid, Junio de 2022\n",
    "\n",
    "Antonio Delgado Peris (Cristina Labajo Villaverde)\n",
    "</font>\n",
    "\n",
    "https://github.com/andelpe/curso-python-analisis-datos\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293763e5-0dfb-4c1e-bb8b-379aa0b19446",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tema 7. Análisis de datos exploratorio (EDA) y pre-procesado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff5d8b-9bbf-4b16-91a4-46a3e71cbcbd",
   "metadata": {},
   "source": [
    "Lo más importante a la hora de analizar datos y trabajar con datasets es disponer de unos buenos datos. No solo tienen que ser lo suficientemente representativos para satisfacer nuestros objetivos, si no que también requieren de un pre-procesado para limpiarlos, reescalarlos, agruparlos, convertir a un formato específico, etc. y así evitar errores en los resultados o resultados poco fidedignos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dbd02-8cdc-4a74-aafb-a08544548354",
   "metadata": {},
   "source": [
    "# Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d51ff-6279-4aee-83ac-339232a692e4",
   "metadata": {},
   "source": [
    "- Identificar y limpiar los datos de valores _omitidos_\n",
    "- Estandarizar el formato de los datos\n",
    "- Normalizar los valores de los que disponemos\n",
    "- Agrupar valores (_binning_)\n",
    "- Variables categóricas y variables numéricas\n",
    "- Explorar los datos de los que disponemos \n",
    "- Encontrar relación entre las distintas variables\n",
    "- Identificación de _outliers_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb315c-9991-49fe-82f6-aff2415fdd92",
   "metadata": {},
   "source": [
    "### Importación de librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeeeb5e-3265-445f-8703-eb736c984b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd9d1b-0ad3-41f3-bd9e-fe3e2285b8b0",
   "metadata": {},
   "source": [
    "##  Estadística descriptiva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1eb7f-5c44-46b8-bf5e-78f4795f3354",
   "metadata": {},
   "source": [
    "Es importante explorar nuestros datos antes de empezar a hacer cálculos complicados con ellos. \n",
    "Una vez que tenemos nuestros datos guardados en un dataset podemos aplicar distintos métodos para tener una primera idea del tamaño, rango y otras medidas estadísticas de interés, tales como el valor medio, máximos, mínimos, etc., que luego podemos necesitar para pre-procesar nuestros datos. \n",
    "\n",
    "La estadística descriptiva ayuda a describir las características básicas de un conjunto de datos de manera rápida.\n",
    "\n",
    "El modo más común de obtener estos datos es la función `df.describe()` de Pandas aplicada a nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9218b-07e2-432c-badb-0070230a0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EJEMPLO COCHES\n",
    "coches = pd.read_csv('data/auto-mpg.data',sep='\\s+', header=None)\n",
    "coches.columns = ['mpg','cylinders','displacement','horsepower','weight',\n",
    "                  'acceleration','model_year','origin','car_name']\n",
    "coches = coches.replace('?', np.nan).dropna().reset_index(drop=True)\n",
    "coches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0901c3-30b7-4e12-b519-009541bf35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "coches.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de2c28-e9d8-4e68-83e7-c0aada38a7f9",
   "metadata": {},
   "source": [
    "Esta función muestra estadísticas básicas de cada variable, tales como la media, el total de datos, la desviación estandar, los cuartiles y el máximo y mínimo. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cd284-83e4-4253-993c-69ca3d411054",
   "metadata": {},
   "source": [
    "También podemos usar la función `.info()` que nos dará otro tipo de información, como el rango del índice de nuestros datos (número de filas) y datos de nuestras columnas, como el tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb2992-5843-4ed3-8f1c-d17d95b82856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EJEMPLO TRABAJADORES EMPRESA\n",
    "datos = [\n",
    "    {'Nombre': 'Juan', 'Sexo':'Hombre','Edad': 42, 'Departamento': 'Comunicación'},\n",
    "    {'Nombre': 'Laura', 'Sexo':'Mujer','Edad': 44, 'Departamento': 'Administración'},\n",
    "    {'Nombre': 'Pepe', 'Sexo':'Hombre','Edad': 37, 'Departamento': 'Ventas'},\n",
    "    {'Nombre': 'Carlos', 'Sexo':'Hombre','Edad': 15, 'Departamento': 'Ventas'},\n",
    "    {'Nombre': 'Esther', 'Sexo':'Mujer','Edad': 62, 'Departamento': 'Administración'},\n",
    "    {'Nombre': 'Álvaro', 'Sexo':'Hombre','Edad': 62, 'Departamento': 'Ventas'},\n",
    "    {'Nombre': 'Rosa', 'Sexo':'Mujer','Edad': 50, 'Departamento': 'Comunicación'},\n",
    "    \n",
    "]\n",
    "\n",
    "empresa = pd.DataFrame(datos)\n",
    "empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49a3b1-5cbb-40f4-839e-9728d631675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdffc5-c9fb-4627-b2ac-184a4f6ede04",
   "metadata": {},
   "source": [
    "Aunque para saber las dimensiones de nuestro dataset la manera más rápida es usar la función `.shape` que nos devuelve dos números, el primero es el número de filas y el segundo el número de columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e76553-5869-417d-8b44-fc41aba39577",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d75dfa-d5b7-4e3f-8423-38d4f81d4830",
   "metadata": {},
   "source": [
    "Dos funciones útiles para conocer los valores que tenemos en nuestras columnas son `value_counts`, y `unique`, especialmente para el caso de variables categóricas.\n",
    "\n",
    "La función `df.columna.unique()` nos indica los valores únicos presentes. La función `df.columna.value_counts()` nos dice cuántas veces se repite cada valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b441d82-a8a9-4a3d-b3ac-3c49dc2fbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresa.Departamento.unique()  # O: pd.unique(empresa.Departamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837d752-0b47-4fec-b839-22c1004a5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresa.Departamento.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22b6f5-b3ac-4ee4-89f3-fc1f48df459f",
   "metadata": {},
   "source": [
    "</p>\n",
    "\n",
    "Otra forma de ver la distribución de nuestros datos es crear un **histograma**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4bcca-86cf-44c1-b2eb-0370731fadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EJEMPLO Coches CO2 emissions\n",
    "df4 = pd.read_csv(\"./data/FuelConsumption.csv\")\n",
    "coches2 = df4[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] \n",
    "coches2 ## Datasat reducido con variables de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9b158-08eb-4f54-87d1-5084cffa06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "sns.histplot(\n",
    "        data    = coches2,\n",
    "        x       = 'ENGINESIZE',\n",
    "        stat    = \"count\",\n",
    "        kde     = True,\n",
    "        color   = 'green',\n",
    "        line_kws= {'linewidth': 2},\n",
    "        alpha   = 0.3,  # transparency\n",
    "        ax      = axes[0])\n",
    "axes[0].set_title('Engine Size', fontsize = 10, fontweight = \"bold\")\n",
    "axes[0].tick_params(labelsize = 8)\n",
    "\n",
    "\n",
    "sns.histplot(\n",
    "        data    = coches2,\n",
    "        x       = 'CO2EMISSIONS',\n",
    "        stat    = \"count\",\n",
    "        kde     = True,\n",
    "        color   = 'orange',\n",
    "        line_kws= {'linewidth': 2},\n",
    "        alpha   = 0.3, \n",
    "        ax      = axes[1])\n",
    "axes[1].set_title('Emisiones CO2', fontsize = 10, fontweight = \"bold\")\n",
    "axes[1].tick_params(labelsize = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6bda3-eb91-4391-a632-242dbcccbabd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87bba2-83e0-4572-a3ce-f9465d177454",
   "metadata": {},
   "source": [
    "Podemos encontrarnos con que faltan valores en nuestros datasets, esto puede deberse a que no se han guardado algunas variables de un evento, y podemos encontrarlos representados de diferentes maneras (p. ej. 0, NaN, espacios en blanco, o símbolos de puntuación).\n",
    "\n",
    "Primero, es necesario detectar si tenemos valores nulos u omitidos en nuestro dataset y eso lo podemos hacer usando la función de pandas `.isnull()`. También podemos combinarla con el método `.any` que nos dirá si hay algún nulo o ninguno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ae1e2-8665-4d24-9ff2-cddd7cbf6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EJEMPLO Dataset Titanic\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f29a3a-3dde-4882-aac3-0de793117c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cac1e-02d3-4cd2-80b6-5c10bc4c6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head().isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ab723-c947-477a-be26-155d122cb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay algún nulo en mis columnas? \n",
    "titanic.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124cf1d-5ddc-4450-99ab-12495b5f1b49",
   "metadata": {},
   "source": [
    "<p/>\n",
    "\n",
    "También podemos contar el número exacto de datos nulos por columna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f204e25-8dff-41b8-89fc-fa8dbac755c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652133f-f2b6-40fd-887b-6348b22b1671",
   "metadata": {},
   "source": [
    "\n",
    "Se pueden considerar diferentes estrategias para enfrentarnos a estos valores omitidos y eso va a depender de la situación, el tipo de dato y la experiencia del investigador. Los distintos métodos para solucionar los espacios en blanco son los siguientes: \n",
    "\n",
    "- Revisar los datos e intentar recuperar el valor desconocido. \n",
    "- Eliminar los datos omitidos\n",
    "    - Eliminar la variable entera \n",
    "    - Eliminar esa entrada de datos (la fila)\n",
    "- Sustituir valores\n",
    "    - Variables numéricas:\n",
    "        - Reemplazarlos por la media de la variable \n",
    "    - Variables categóricas:\n",
    "        - Reemplazarlos por la moda   \n",
    "        - Reemplazar los valores basandonos en conocimiento previo\n",
    "- Dejar en blanco los valores que faltan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da0112-7e41-474a-a5be-a4d5bb4baf25",
   "metadata": {},
   "source": [
    "### Eliminar los datos omitidos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a281ca-5c29-4361-9582-a6641c00c377",
   "metadata": {},
   "source": [
    "Pandas tiene una función que se encarga de eliminar datos que no son válidos o en blanco: `df.dropna()`\n",
    "\n",
    "Es necesario especificar el eje que queremos eliminar:\n",
    "- `axis=0` elimina la fila entera, la entrada que presenta problemas\n",
    "- `axis=1` elimina la columna, la variable\n",
    "\n",
    "Otros de los parámetros de la función que hay que configurar son la columna en la que se encuentra el NaN y si queremos mantener el índice cuando eliminemos los datos vacíos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a823ed8-d948-4393-aa5b-1176e0044a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_titanic = titanic.copy()\n",
    "\n",
    "# Borramos las filas (axis=0) que tengan NaN en la columna 'deck' (subset)\n",
    "del_titanic.dropna(subset=['deck'], axis=0, inplace=True)\n",
    "del_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c1ba3-5fd6-4dcc-98f2-7176fa0a7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos \n",
    "print(f'Hemos pasado de {titanic.shape[0]} a {del_titanic.shape[0]} filas.')\n",
    "print('\\nLos nulos por columna tras el borrado:\\n')\n",
    "print(del_titanic.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e25626-84b5-41be-b6b1-16960fb9f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos reiniciar el índice para que tenga valores consecutivos\n",
    "del_titanic.reset_index(drop=True, inplace=True)\n",
    "del_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4361e-c96f-4886-9353-83a5c4a99663",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sustituir valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d8d52-c13c-40f6-ba51-e7970664db53",
   "metadata": {},
   "source": [
    "Para sustituir un dato vacío por otro valor se puede usar la función `df.replace(valor_omitido, nuevo_valor)`.\n",
    "\n",
    "También existe una función especifica para los valores omitidos:`df.fillna(nuevo_valor)`\n",
    "\n",
    "Pongamos que el ejemplo del dataset anterior queremos reemplazar el valor NaN por el valor de la columna `deck` que más veces se repite, o lo que es lo mismo, la _moda_.\n",
    "\n",
    "Primero habría que calcular la moda con `df.col.mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c36bb1-3a7d-4478-9e21-00310d3eef28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hallamos la moda\n",
    "md = titanic.deck.mode()[0]\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2998a2f-2b8d-455a-88ca-355afb2466bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Sustituimos valores NaN por la moda hallada\n",
    "md_titanic = titanic.copy()\n",
    "md_titanic.deck = md_titanic['deck'].fillna(md)\n",
    "md_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79e654-0ad5-437a-8aff-b3553dd65ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que los NaN han desaparecido\n",
    "md_titanic.deck.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca2549-a6c5-4cc8-b85b-3a7beb351e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pero todavía tenemos NaN en algunas otras columnas\n",
    "md_titanic.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08aa7d8-37ab-4b9b-8ddb-62345b343215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el caso de la columna 'age', vamos a aplicar la función 'replace', para cambiarlos por 0s\n",
    "# La situación inicial es que hay varios NaN, pero ningún valor 0\n",
    "print('Num. filas con age==NaN -->', md_titanic.age.isnull().sum())\n",
    "print('Num. filas con age==0 -->', md_titanic[md_titanic.age == 0].age.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2ff62-478b-4356-a962-0fdda1e365be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el cambio\n",
    "md_titanic.age = md_titanic.age.replace(np.nan, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1eb520-4cc3-4579-be4b-ad8b40d0374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y vemos sus efectos\n",
    "print('Num. filas con age==NaN -->', md_titanic.age.isnull().sum())\n",
    "print('Num. filas con age==0 -->', md_titanic[md_titanic.age == 0].age.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41a586-27e4-4561-b0aa-c9741a81f351",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e7_1:** \n",
    "\n",
    "Dado el dataset `ejer1` remplaza los NaN de cada columna por la media de su correspondiente columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd252c3-e3c2-4b73-a3b6-31943f590fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ejer1 = pd.DataFrame({'a':[None, 3, None, 5, 6], 'b':[1, 3, 4, 6, None], 'c':[54, None, None, 32, 21]})\n",
    "ejer1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2488aac-edcf-46cb-ab50-779b07cba398",
   "metadata": {},
   "source": [
    "## Formateo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c51b2-a24d-429f-b8c4-8699e36c1d35",
   "metadata": {},
   "source": [
    "Puede darse el caso de que recolectemos datos de distintas fuentes, o que los registren diferentes personas por lo que los datos pueden presentar distintas nomenclaturas o no ser constantes en términos de unidades y formatos. En este caso resulta difícil comparar los datos o agruparlos por lo que es necesario formatearlos y definir un formato único que haga más fácil las futuras operaciones. \n",
    "\n",
    "En el caso en que se requiera una conversión de unidades, por ejemplo pasar los datos de peso de libras a kg, debemos dividir las libras entre 0,45359237. Podemos modificar la columna `libras` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219d175-2172-4124-a230-6d7b0474031d",
   "metadata": {},
   "source": [
    "`df['libras'] = df['libras']/0,45359237`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07202822-15eb-4eef-b082-9b471213cbcf",
   "metadata": {},
   "source": [
    "Conviene en este caso renombrar la columna:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05796ec7-ed05-4265-9057-dd901af3ee43",
   "metadata": {},
   "source": [
    "`df.rename(columns={'libras':'kilogramos'}, inplace=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cdd96-ab64-4f10-86a7-174e64162d23",
   "metadata": {},
   "source": [
    "Es importante prestar atención al tipo de datos que tenemos. Si no son los correctos puede haber errores en las operaciones cuando construyamos modelos. \n",
    "\n",
    "Cuando introducimos datos en python puede ocurrir que se registren como un tipo distinto al que deseamos. Es necesario comprobar el tipo de datos y ver si se corresponden con lo que debería ser para que no interfiera en nuestros futuros cálculos, si este no fuera el caso, se debe de cambiar el tipo de datos. \n",
    "\n",
    "Para identificar qué tipos de datos tenemos usamos la función `dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd07d5-6da0-408d-be9b-e2e0e1d5883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b473366-d8c8-46d0-860e-02a5b09740a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coches.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e72dc7-a634-4f2c-a6dc-cb8f80856810",
   "metadata": {},
   "source": [
    "Para cambiar el tipo de una columna se puede usar la función `astype` especificando el tipo al que vamos a convertir la variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8716e-3dca-4359-80ab-72062f00c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos todas las columnas numéricas a float\n",
    "coches.iloc[:,:-1] = coches.iloc[:,:-1].astype(float)\n",
    "coches.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863515f-6b30-46f7-b6f3-690428d4f099",
   "metadata": {},
   "source": [
    "NOTA: Un caso particular a tener en cuenta es si una columna con valores numéricos tiene tipo `object`. Eso suele provenir de un error de importación de los datos, que se han considerado _string_, y dará problemas en futuras operaciones matemáticas. Se debe corregir con `astype`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2feede2-b907-4450-95a0-026ae5d4fc0a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e7_2:** \n",
    "\n",
    "Convertir la columna `mpg` (_miles per gallon_) del dataframe `ejer_coches` a litros cada 100km.\n",
    "    \n",
    "La fórmula a sefuir es la siguiente $L/100km = 235 / m.p.g.$\n",
    "\n",
    "Cambiar el nombre de la columna a `L/100km`\n",
    "\n",
    "Comprobar el tipo de la columna modificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c91df4-19b0-4882-a354-6e6f0b5dfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejer_coches = coches.copy()\n",
    "ejer_coches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af1cd1-0695-448e-a6d6-3c3d6aa3503e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b65ed9-9cf5-40a8-9520-d6cf4a97f524",
   "metadata": {},
   "source": [
    "Muchas veces, dependiendo de la naturaleza de los datos, nos encontramos con que hay gran variación de rango entre una columna y otra. Por ejemplo en el siguiente dataframe de casas, entre el precio y el número de habitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e5e7d-679b-47bc-a951-bff38df1ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EJEMPLO CASAS\n",
    "casas = pd.read_csv('./data/kc_house_data.csv', header='infer')\n",
    "casas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89d398-84df-471b-959f-4b048ff73cc6",
   "metadata": {},
   "source": [
    "\n",
    "En estos casos puede venir bien normalizar nuestros datos. Normalizar significa, en este caso, comprimir o extender los valores de la variable para que estén en un rango definido y hacer que los datos sean más uniformes, para facilitar futuros cálculos estadísticos con nuestro dataset. \n",
    "\n",
    "La normalización hace posible la comparación entre distintas variables (_features_) y hace que todas tengan el mismo impacto en los cálculos.\n",
    "\n",
    "Pero no existe un método ideal de normalización que funcione para todas las formas de variables. Es trabajo del científico conocer cómo se distribuyen los datos, saber si existen anomalías, comprobar rangos, etc.\n",
    "\n",
    "Hay varios métodos para normalizar, pero a continuación explicaremos tres y veremos cómo se implementan en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01464b3d-1a36-4149-9ac6-8af248070b1e",
   "metadata": {},
   "source": [
    "### Escalado de variables (Feature Scaling):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017f602-c1ec-4b8d-b921-5d86436dd965",
   "metadata": {},
   "source": [
    "#### Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b15dda-cfa1-4e15-a960-791fcee74c65",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "$$x_{new} = \\frac{x_{old}}{x_{max}}$$\n",
    "\n",
    "Se divide cada valor por el máximo valor de esa variable. El nuevo rango de la variable es entre [0,1].\n",
    "\n",
    "Esto aplicado a nuestro dataset sería de la siguiente forma:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90424cf8-6a4f-4cc3-a450-ae93d2e92add",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = casas['price']\n",
    "var.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d172a8-867b-4497-a724-65dc713821f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_norm_max = var / var.max()\n",
    "price_norm_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ebfa5-8a14-40e4-a876-3420fad64d06",
   "metadata": {},
   "source": [
    "#### Min-Max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0b91a-57fa-4595-ac86-6fb72f7eaf22",
   "metadata": {},
   "source": [
    "\n",
    "$$x_{new} = \\frac{x_{old} - x_{min}}{x_{max}- x_{min}}$$\n",
    "\n",
    "El nuevo valor es el resultado de dividir la diferencia entre el valor original menos el valor mínimo, por el rango de dicha variable (máximo - mínimo). El nuevo rango de la variable será también [0,1].\n",
    "\n",
    "Siguiendo el ejemplo anterior pero aplicando la normalización Min-Max:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25c4cc-38a5-489d-80ea-d5e3b0d9893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_norm_minmax = (var - var.min()) / (var.max() - var.min())\n",
    "price_norm_minmax.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955e08a-be62-440a-a001-9f52cfcfdf63",
   "metadata": {},
   "source": [
    "### Escalado estándar (Z-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b2cac-2d0f-44bf-b0c2-1bf9868ed3c4",
   "metadata": {},
   "source": [
    "Otra forma de normalización es la llamada _Z-score_, que es una medida de cuántas desviaciones estándar por debajo o por encima de la media se encuentra un valor concreto.\n",
    "\n",
    "Se calcula restando la media y dividiendo por la deviación estándar.\n",
    "\n",
    "$$x_{new} = \\frac{x_{old} - \\mu }{\\sigma}$$\n",
    "\n",
    "Un valor Z de cero indica que los valores son exactamente la media, mientras que un valor de +3 indica que el valor es mucho más alto que la media.\n",
    "\n",
    "En una distribución gaussiana nos indica también en qué percentil de la distrubución nos encontramos. En ese casi todos los valores normalizados quedarán dentro del rango [-3, 3].\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/t7_zscore.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3dbafd-08e0-41f2-bd48-1a76e79ef3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = coches2['CO2EMISSIONS']\n",
    "co2_norm_zscore=(var - var.mean()) / var.std()\n",
    "co2_norm_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4f803-6dd4-4efe-a038-f01e991c9c00",
   "metadata": {},
   "source": [
    "También la librería Scipy nos ofrece una fórmula que nos lo calcula directamente: `stats.zscore()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057399d-ca66-46b0-b63c-aebc89378392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "ZS=stats.zscore(var)\n",
    "print(ZS[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8ac7d-f939-4967-811f-8f6fc6ff0d0c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e7_3:** \n",
    "\n",
    "Normaliza el dataset ejer_coches2 utilizando la función Min-Max. Cada columna debe ir normalizada usando el máximo y mínimo correspondiente. \n",
    "\n",
    "Comprobar que los valores resultantes están en el rango [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220578f-be29-40e9-8f23-dd7a94b8597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejer_coches2=coches2.copy()\n",
    "##Escribir aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eee45b-1d1d-4f55-bc29-580eb9026190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discretización (Binning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc18ef-4f5c-4961-bf25-c514545cb065",
   "metadata": {},
   "source": [
    "Binning es la agrupación de los datos en 'bins', o porciones de acuerdo con criterios definidos.\n",
    "\n",
    "Uno de los casos más comunes de binning se realiza entre bastidores al crear un histograma.Un histograma es una representación gráfica de una variable en forma de barras, donde la altura de cada barra dependerá de la frecuencia de los datos representados. Ya hemos visto con anterioridad como hacer histogramas en pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec51d9-ae3c-4892-8ad3-66aae0a1552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "coches2['CO2EMISSIONS'].plot(kind='hist',edgecolor='black', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afa45b-db33-4caf-b7fb-486887152c7b",
   "metadata": {},
   "source": [
    "Esta división nos permite hacernos una idea de cuantos elementos tenemos en cada grupo o *bin*. Así podemos ver la distribución de nuestros datos. \n",
    "Existen dos funciones en pandas que se usan para dividir nuestros datos en bins y cada una tiene sus características y usos particulares.\n",
    "- [`cut`](https://pandas.pydata.org/docs/reference/api/pandas.cut.html)\n",
    "   \n",
    "   Usamos la función `cut` para indicarle a python que queremos dividir nuestras muestras en intervalos de igual tamaño y esta función se encarga de hacer los cálculos necesarios para definir los límites de los bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd023f5-df5e-4cc6-8c11-5ca7145a97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coches2['CO2EMISSIONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be940a6-758a-4864-871c-29c35d9156f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(coches2['CO2EMISSIONS'], bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b65d3-dcd1-4f34-a37d-3fd08f717533",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(coches2['CO2EMISSIONS'], bins=4).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0dfd0-1ad9-4a0d-930b-42e60bcdc0f9",
   "metadata": {},
   "source": [
    "Además, con esta función se puden especificar los límites de los bins que se quieran usar.Podremos saber exactamente cuantos elementos hay en cada bin usando la función `.value_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc90083-c971-44fa-8087-11a9250b6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.IntervalIndex.from_tuples([(0.0, 90.0), (91.0,251.0), (252.0, 488.0)])\n",
    "cont=pd.cut(coches2['CO2EMISSIONS'], bins)\n",
    "print(cont)\n",
    "cont.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479e8bc-cdd9-4a73-88fc-da2dfad08237",
   "metadata": {},
   "outputs": [],
   "source": [
    "coches2['CO2EMISSIONS'].plot.hist(bins=4,edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d296e5-a9d7-4963-9f98-9891ffd573fe",
   "metadata": {},
   "source": [
    "- [`qcut`](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html)\n",
    "La documentación de pandas describe `qcut` como una \"función de discretización basada en cuantiles\". `qcut` calculará el tamaño de cada bin para asegurarse de que la distribución en los bins es la misma (más o menos) en todos ellos. Esto supondrá que los bins no serán de igual tamaño, el rango variará, pero todos tendrán el mismo número de observaciones. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c59bf0-19f2-46f4-abb8-afd4d8c2b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "h= pd.qcut(coches2['CO2EMISSIONS'], q=5)\n",
    "print(h)\n",
    "type(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebf838-5dc1-43dd-8633-0508cc592509",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ddf6d8-265d-47ef-ba65-e6c8408365a5",
   "metadata": {},
   "source": [
    "Con el método `qcut` puedes dividir los bins basandote en los cuartiles, pero no puedes definir los límites de los bins explícitamente, a diferencia del método `cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e63a14-09d3-4d6c-8c4b-417663571230",
   "metadata": {},
   "outputs": [],
   "source": [
    "coches2['CO2EMISSIONS'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02304fdc-fcaf-48a2-9678-b8c8e68baeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2, bins = pd.qcut(coches2['CO2EMISSIONS'], q=[0, 0.25, 0.5, 0.75, 1], retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26552f-0df4-4cbd-9fb9-c8481ba5a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f598c6-fc7d-4a32-a00e-77c35cdc9863",
   "metadata": {},
   "source": [
    "Algunas veces agrupar nuestros datos puede mejorar la precisión de nuestros modelos predictivos y ayuda a tener una visualización más representativa de la distribución de nuestros datos. \n",
    "Otra aplicación que puede tener el hacer binning es la de poner etiquetas a los distintos grupos y convertir variables numéricas en variables categóricas. \n",
    "\n",
    "Por ejemplo, usando `cut` podemos clasificar los coches del ejemplo anterior en 'poco contaminantes' - 'muy contaminantes' dependiendo de las emisiones de CO2: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18b64c-c8f6-4944-8a81-deb68e6da82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_emissions=['Muy poco','Poco','Normal','Mucho']\n",
    "contaminantes_bins = pd.cut(coches2['CO2EMISSIONS'],bins=4,labels=labels_emissions,retbins=True)\n",
    "contaminantes_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037bc015-dd7d-498c-bc25-1e97192b68ec",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e7_4:** \n",
    "\n",
    "Clasifica a los trabajadores de una empresa en grupos de edad (categóricos) usando la función cut. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fc76f-c968-4102-9100-bbd9a813f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejer_edad=empresa.copy()\n",
    "##Escribir aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e843e9-fba1-469b-8f2f-b6b80b8af456",
   "metadata": {},
   "source": [
    "## Pasar de variables categóricas a variables numéricas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6d3b0-6f00-45dc-a854-808959c98a64",
   "metadata": {},
   "source": [
    "Para poder hacer operaciones sobre nuestros datos o construir modelos (clases de python que representan funciones matemáticas, ej:LinearRegression) la gran mayoría de veces requieren que nuestras variables sean numéricas. Por lo tanto, cuando tenemos variables categóricas (strings) tenemos que convertirlas a numéricas. Existen diferentes métodos para esto:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b348ba-9476-4d45-8cd3-d0a11655c7b2",
   "metadata": {},
   "source": [
    "### Convención"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe54785-4795-4c2e-be5f-4bf173821086",
   "metadata": {},
   "source": [
    "Se puede establecer un código para poner etiquetas a las distintas categorias que tengamos de forma manual, por ejemplo, si quisieramos cambiar la variable 'Sexo' del siguiente dataset a numérico, podrías acordar previamente que 'Mujer'=1 y 'Hombre'=2.\n",
    "\n",
    "\n",
    "Nombre | Sexo | Sexo_num\n",
    ":--------: | ------- | --------\n",
    "Juan | Hombre | 2\n",
    "Laura| Mujer | 1\n",
    "Pepe| Hombre | 2\n",
    "Carlos| Hombre | 2 \n",
    "Esther | Mujer | 1 \n",
    "Álvaro | Hombre | 2 \n",
    "Rosa | Mujer | 1 \n",
    "\n",
    "\n",
    "De este modo tendríamos la variable convertida a numérico. El código para conseguirlo sería el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35d5ac-56a2-4cb8-9fb3-f5744b36c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresa[\"Sexo_num\"] = empresa.apply(lambda x: 1 if x[\"Sexo\"] == 'Mujer' else 2, axis=1)\n",
    "empresa[['Nombre','Sexo','Sexo_num']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f361886b-5d22-43da-ac95-5a0ec22acdef",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a2d9a-492d-420a-9306-8ab754ceba1c",
   "metadata": {},
   "source": [
    "Este método consiste en crear unas variables extras con el nombre de cada etiqueta e indicar con 1s y 0s la categoría a la que pertenece cada evento (1=si, 0=no).\n",
    "Por ejemplo, fijémosnos en la clase en la que viajaban los pasajeros del Titanic. Una variable categórica que puede ser Primera, Segunda o Tercera (First, Second, Third):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a18288-1d8e-4de8-be35-375efaf66558",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b438dc-9532-4877-9654-205a220ec74e",
   "metadata": {},
   "source": [
    "Al hacer One Hot Encoding crearemos tres nuevas variables, cada una con el nombre de uno de los valores de titanic['class'] y estas clases son excluyentes, por lo que hay que tener en cuenta, que no importa el número de categorías o etiquetas que tengamos, sólo una de ellas puede ser 1 siendo el resto 0. \n",
    "\n",
    "Para poder crear estas variables ficticias o \"dummy variables\" se usa la función de pandas [`.get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f1787-2795-4040-91c8-27bef2db53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(titanic['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56293dce-1180-44e9-b749-2dda4c4fc139",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e7_5:** \n",
    "\n",
    "Dado el dataset `ejer_empresa` convierte a numérico los departamentos a los que pertenecen los trabajadores usando One-Hot-Encoding. \n",
    "Muestra en pantalla el número de trabajadores que trabajan en cada departamento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d2218-ad37-4968-a7bc-24e56ee9a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejer_empresa=empresa.copy()\n",
    "##Escribir aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5d0cc-4111-41b4-bce3-05c75cdf98d4",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db97b7d-3ac0-4aca-950c-b42a0d932f6a",
   "metadata": {},
   "source": [
    "El diagrama de cajas o *boxplot* es un método estandarizado para representar gráficamente una serie de datos numéricos a través de sus cuartiles. De esta manera, se muestran a simple vista la mediana y los cuartiles de los datos y también pueden representarse sus valores atípicos.\n",
    "También proporcionan una visión general de la simetría de la distribución de los datos; si la mediana no está en el centro del rectángulo, la distribución no es simétrica.\n",
    "\n",
    "Usaremos la librería seaborn para construir los diagramas de cajas, más concretamente la función [`.boxplot()`](https://seaborn.pydata.org/generated/seaborn.boxplot.html)\n",
    "\n",
    "Si lo que queremos es agrupar los datos dependiendo de otra variable, la definiremos en el eje x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6f206-64c3-4c00-bbc7-0d2764b77f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot vertical\n",
    "figure,axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "sns.boxplot(y=empresa.Edad, data=empresa, ax=axes[0])\n",
    "axes[0].set_title('Boxplot Edad',fontsize = 10)\n",
    "\n",
    "sns.boxplot(x=empresa.Sexo,y=empresa.Edad, data=empresa, ax=axes[1])\n",
    "axes[1].set_title('Boxplot Edad dividido por Sexo',fontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e67a2f-f944-40f0-924f-e0b29cfb7eb7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e7_6:** \n",
    "\n",
    "Dado el dataset `ejer_titanic` visualizar usando un boxplot la edad de mujeres y hombres que había en el titanic, clasificados por la clase en la que viajaban (Lo tenemos numérico en la variable `pclass`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85293e13-f1b5-4675-b6ff-a8cc3c178e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejer_titanic=titanic.copy()\n",
    "## Escribir aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4176c-8f35-49cb-9752-01f244bab643",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40b0ab-7660-4c5f-97e6-023630b74455",
   "metadata": {},
   "source": [
    "La correlacción es un método usado en estadística para saber hasta que punto dos variables son independientes entre sí, cómo afecta el cambio de una de estas variables a la otra.\n",
    "\n",
    "Por ejemplo: \n",
    "  - Calor -> Venta de ventiladores\n",
    "  - Navidades -> Consumo de turrón\n",
    "\n",
    "Esta relación no tiene por qué ser de causalidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db293b48-93bc-4f39-a6c1-10a1d331a6af",
   "metadata": {},
   "source": [
    "Vamos a ver el ejemplo práctico de correlación entre el precio de una casa y la superficie habitable de esta. Para visualizarlo de la mejor manera posible usamos un scatter plot, o gráfico de dispersión en el que mostraremos además la recta de regresión lineal. Para ello usaremos la función `sns.regplot`  que nos lo hace directamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69785a43-8486-4884-894c-d74b4cafc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.regplot(x='sqft_living',y='price',data=casas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c727c9d-e846-453d-8817-4269e5db5fae",
   "metadata": {},
   "source": [
    "En este ejemplo, vemos que la recta tiene una pendiente positiva, que nos indica que el valor medio de los edificios tiene relación con el número total de superficie habitable. Una pendiente negativa indicaría una correlación inversa y una pendiente cercana a cero (línea horizontal) nos indicaría que no hay correlación entre esas dos variables. \n",
    "No se puede usar variables que tengan poca correlación para predecir valores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fa51b-17fe-4cac-a053-96aba992dbd6",
   "metadata": {},
   "source": [
    "### Coeficiente de Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad9e52-cc76-4598-be4f-3ca100c0631a",
   "metadata": {},
   "source": [
    "La correlación de Pearson es la medida de cuán fuerte es una correlación. Nos da dos valores: El coeficiente de correlación y el P-value\n",
    "- Coeficiente de Correlación\n",
    "    \n",
    "Nos muestra el grado de correlación entre dos variables. El rango de este coeficiente es [-1,1]. Si el valor se acerca a -1 nos indica que hay una fuerte correlación y que esta es negativa, si el valor es cercano a 1 es una correlación fuerte y positiva y si es 0 significa que no hay correlación.  \n",
    "\n",
    "- P-value\n",
    "\n",
    "Nos dice lo seguros que estamos de la correlación dada por el coeficiente de correlación, o lo que es lo mismo, cuánto de verdad hay en nuestros cálculos. \n",
    "Un P-value < 0.001 indica gran veracidad de los cálculos. Un valor entre 0,001 y 0,05 nos da una certeza moderada. Un valor entre 0,05 y 0,1 nos dará una certeza débil. Y un valor P mayor que 0,1 no nos dará ninguna certeza de correlación. \n",
    "\n",
    "Una correlación fuerte tiene que cumplir ambos criterios, un coeficiente cercano a 1 o -1 y un P-value lo más bajo posible.\n",
    "Vamos a usar la función [`.pearsonr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html)de la librería scipy que contiene herramientas y algoritmos matemáticos y vamos a ver lo fuerte o débil que es la correlación entre el número de habitaciones y el precio de los edificios del ejemplo anterior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab15ad-3909-4550-9c7b-8d16dd997095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "pearson_coef,p_value=stats.pearsonr(casas.price,casas.sqft_living)\n",
    "print('Coeficiente de Pearson:',pearson_coef)\n",
    "print('\\nP-value:',p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f571d8c-da29-4d27-bd81-615c3a367afa",
   "metadata": {},
   "source": [
    "Otra forma de representar estos coeficientes de Pearson son los llamados *\"heat maps\"* o mapas de calor, donde de manera visual podemos hacernos una idea de los grados de correlación entre las variables de nuestro data set entero. \n",
    "Es una matriz de colores, en los que cada color representa un valor del coeficiente. \n",
    "Utilizaremos el método `heatmap()` de Seaborn para trazar la matriz.\n",
    "\n",
    "Para crear los heat maps usamos la función `.corr` de pandas que, por defecto, calcula el coeficiente de correlación de Pearson pero también podemos especificar la utilización de otros métodos de correlación pasando un valor apropiado al parámetro “método”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcc145-ecbb-4d5a-bb14-2eb575ce8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=casas.iloc[:,2:9]\n",
    "correlation_mat = sub.corr()\n",
    "correlation_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b7af3-2f97-48df-a095-d4f1a6057733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=casas.iloc[:,2:9]\n",
    "correlation_mat = sub.corr()\n",
    "sns.heatmap(correlation_mat, annot = True)#El parámetro ‘annot=True’ muestra los valores del coeficiente de correlación en cada celda.\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b1038-e7a2-44b8-83cd-452e49716c79",
   "metadata": {},
   "source": [
    "Si nos fijamos , la diagonal de la matriz es todo 1s, que es la correlación de cada variable con sí misma, obviamente, esta es máxima.\n",
    "\n",
    "Un **valor positivo grande (cercano a 1)** indica una fuerte correlación positiva, es decir, si el valor de una de las variables aumenta, el valor de la otra variable aumenta también.\n",
    "\n",
    "Un **valor negativo grande (cercano a -1)** indica una fuerte correlación negativa, es decir, que el valor de una de las variables disminuye al aumentar el de la otra y viceversa.\n",
    "\n",
    "Un **valor cercano a 0 (tanto positivo como negativo)** indica la ausencia de cualquier correlación entre las dos variables, y por lo tanto esas variables son independientes entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0afaf-f14d-48f4-bc0c-0b6f697c7d96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7601852-5e49-44a6-b392-7134eed8541b",
   "metadata": {},
   "source": [
    "Un valor atípico o *outlier* es una observación que difiere de los datos que de otro modo estarían bien estructurados. Es un valor que numéricamente es muy distinto al resto de los datos, lo que puede afectar a nuestros datos. Por ejemplo, si tenemos datos de emisiones de CO2 de coches y uno de ellos resulta que tenía un problema de fábrica y emitía más. \n",
    "Ese dato nos va a afectar a la media y a los cuartiles, y puede ser que queramos identificar ese dato para poder eliminarlo de nuestro dataset.\n",
    "\n",
    "**¡OJO!** Es muy necesario conocer la naturaleza de nuestros datos, y saber si la eliminación de esos outliers va a perjudicar a mi objetivo. No es lo mismo eliminar un dato, como el ejemplo anterior en un estudio sobre la emisión de CO2 de coches en buen estado que en un ensayo científico donde eliminar datos anómalos puede considerarse falseo de datos. \n",
    "\n",
    "\n",
    "Podemos hacernos una idea de nuestros datos haciendo un histograma, pues veremos la distribución. En los boxplots también están representados los outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730c0f6-25b5-4c49-ac79-a300f4353eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "casas.price.plot(kind='hist',edgecolor='black', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877acb2-d957-43fb-92f8-9649284dac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60,20))\n",
    "plt.boxplot(casas.price, vert=False)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(60,20))\n",
    "# plt.boxplot(coches2.CO2EMISSIONS, vert=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcdc89-2009-4e3a-b442-c836a85fa959",
   "metadata": {},
   "source": [
    "Una manera de eliminarlos es usar los Z-score que hemos visto con anterioridad. Si se alejan más de 3 deviaciones estándar de la media pueden considerarse outliers, con lo que podríamos eliminar todos aquellos datos que no cumplan esa norma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3de387-64ba-4e26-ba46-3513681edc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero calculamos los Z-scores de la columna que nos interesa analizar.\n",
    "ZS=stats.zscore(casas.price)\n",
    "ZS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b8443-367a-4823-97b3-a14f0ca68152",
   "metadata": {},
   "outputs": [],
   "source": [
    "casas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efa095-c26c-4ee2-bfa7-18466720852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "casas['abs_z_scores'] = np.abs(ZS) #Nos quedamos con el valor absoluto\n",
    "new_casas=casas[casas['abs_z_scores'] < 3] #Seleccionamos sólo aquellos que tienen un Z-score menor de |3|\n",
    "new_casas.shape #Hemos eliminado elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a5803-0ea1-4eaa-9e7f-8031336738d1",
   "metadata": {},
   "source": [
    "Sin embargo, en otras ocasiones precisamente lo que nos interesa no es eliminar los outlier si no detectarlos, por ejemplo, en un log de un ordenador valores anómalos pueden indicar una sobrecarga del sistema o un ataque cibernético. Si este fuera el caso podríamos proceder de la misma manera que en el ejemplo anterior, pero en vez de eliminar esos datos los guardaríamos en otra variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd23356-89e8-486e-a323-bcfa44b1b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "casas['abs_z_scores'] = np.abs(ZS) #Nos quedamos con el valor absoluto\n",
    "outliers_casas=casas[casas['abs_z_scores'] >= 3] #Seleccionamos sólo aquellos que tienen un Z-score mayor o igual de |3|\n",
    "print(outliers_casas.shape) #Nos hemos quedado con los outliers\n",
    "outliers_casas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862b3c6-4acb-4950-b447-31b914f6071a",
   "metadata": {},
   "source": [
    "## Análisis estadístico con Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc53b6-9cb7-40c2-982d-b91607ee0eb8",
   "metadata": {},
   "source": [
    "Para el tratamiento con datos usando Python las librerías más comunes son **Numpy**, **Pandas** y **Scipy**, como ya hemos visto anteriormente.\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src=\"./images/t9_numpy.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td> <img src=\"./images/t9_pandas.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td> <img src=\"./images/t9_scipy.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Pero para un análisis estadístico más complejo o para trabajar con Machine Learning (ML),  **Scikit Learn** (sklearn) es una librería muy importante, pues pose muchas herramientas útiles.\n",
    "<center>\n",
    "<img src=\"./images/t9_sklearn.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "</center>\n",
    "\n",
    "Características de Scikit Learn: \n",
    "\n",
    "- Librería de Machine Learning para lenguaje Python\n",
    "- Gratuita\n",
    "- Mayoría de algoritmos de regresión, clasificación y clustering que se conocen\n",
    "- Diseñada para trabajar con las librerías numéricas, científicas y de datos (Numpy, Scipy y Pandas)\n",
    "- Bien documentada con gran cantidad de ejemplos\n",
    "- Algoritmos que abarcan el amplio proceso de trabajo con ML\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![ML_pipeline](./images/t9_pipeline.JPG)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de1cc8-688c-4143-878a-ff2fe1d1e44d",
   "metadata": {},
   "source": [
    "## Sklearn Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63527a-8e5a-4620-af7f-d5db4732c8de",
   "metadata": {},
   "source": [
    "Aunque ya hemos visto como limpiar y preprocesar un dataset usando pandas, también existe la posibilidad de usar funciones de la librería sklearn. A continuación se mencionan las más útiles:\n",
    "\n",
    "<ins>**Estandarización**</ins>\n",
    "\n",
    "La estandarización de los datasets es un requisito fundamental que exigen muchos algoritmos de machine learning que se implementan usando sklearn, pues si las variables individuales no se parecen a una distribución normal no funcionarán correctamente.\n",
    "\n",
    "Para ello el módulo preprocessing de la librería sklearn contiene la clase `StandardScaler` aplicable a datos dados en la forma *array*. \n",
    "Esta función transforma los datos de manera que su distribución tenga un valor medio 0 y una desviación estándar de 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bafd5c-b4bc-445f-bfce-b40d3da47d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f7e8d-7692-467b-8f9e-9383d97158b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "sc=scaler.fit(X_train)\n",
    "print(sc.mean_)\n",
    "sc.scale_\n",
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249578d3-62b6-4f25-a3dd-2a94d1a9f612",
   "metadata": {},
   "source": [
    "<ins>**Scaling**</ins>\n",
    "\n",
    "Una estandarización alternativa sería ajustar las variables para que su rango vaya del mínimo al máximo, siendo estos 0 y 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03069653-2e8b-4830-b229-037bf91feda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax ## Hace el ajuste por columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb943e-bf19-4da8-90ee-e4f3abb5f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "minmax = min_max_scaler.fit(X_train)\n",
    "X_train_minmax=minmax.transform(X_train) ## Hace el ajuste por columnas.\n",
    "X_train_minmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d49f5-facf-4bfd-9b30-9d0f5c20a026",
   "metadata": {},
   "source": [
    "<ins>**Normalización**</ins>\n",
    "\n",
    "La normalización es el proceso de ajustar los valores para que cada elemento (fila en el ejemplo siguiente) tenga una norma igual a 1 (puede ser norma l1, l2, o max)).\n",
    "\n",
    "Lo hacemos con la función `normalize` del módulo preprocessing. En este ejemplo usamos norma l2 (euclídea).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a932da-7b37-43db-aba2-ce4e9f776e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized ## Por fila\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be068b21-a763-4c9a-8fe5-d20f68969719",
   "metadata": {},
   "source": [
    "<ins>**Conversión de variables categóricas a numéricas**</ins>\n",
    "\n",
    "\n",
    "Para convertir variables categóricas a números enteros, podemos usar **OrdinalEncoder**. Este estimador transforma cada variable categórica en una nueva variable de enteros (0 a n - 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb7e18-8259-4f88-bc01-73aecb26ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "print('categories:', enc.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5830312-e64c-4d85-951e-d8e187a20843",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([['female', 'from US', 'uses Safari']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e3ec1-e800-4168-bf6e-342ddf5b1f0e",
   "metadata": {},
   "source": [
    "También podemos usar la transformación siguiendo el método **OneHot Encoder**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfb384-5a0c-4033-a8cf-1804420f1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "print('categories:', enc.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbbe27-7267-4adf-921e-198d68d18ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Safari']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1689068-51d6-480b-9691-08f882d59e00",
   "metadata": {},
   "source": [
    "<ins>**Discretización**</ins>\n",
    "\n",
    "La discretización (también conocida como agrupamiento) proporciona una forma de dividir variables continuas en valores discretos.\n",
    "`KBinsDiscretizer` discretiza variables en k grupos:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f36e98-93aa-4c2a-bd30-4c38ba4900cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ -3., 5., 15 ],\n",
    "              [  0., 6., 14 ],\n",
    "              [  6., 3., 11 ]])\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X) #Por columna\n",
    "                   \n",
    "print('Limites de los bins:',est.bin_edges_)\n",
    "est.transform(X)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5b897-b1a9-4abc-a398-87adb991620e",
   "metadata": {},
   "source": [
    "### Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5bb427-dea2-4e98-a25e-79fd9ae9cd51",
   "metadata": {},
   "source": [
    "En aprendizaje automático la creación de un modelo es un proceso complejo que requiere llevar a cabo múltiples pasos. Siendo la preparación de los datos uno de los que más tiempo requiere. Tras la obtención de un conjunto de datos es necesario aplicarle a este diferentes operaciones antes de poder utilizar un estimador.\n",
    "Por lo que su automatización favorecería el aumento considerable de la productividad. La librería scikit learn permite con una función encadenar las transformaciones que se van a realizar a los datos en lo que se llaman rutinas de trabajo o *pipelines*. Las tuberías se pueden utilizar posteriormente como si fuesen un estimador más. A continuación tenemos un ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53eadbf-e119-42d2-b046-f9a6b3d1cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())# Se define el pipeline\n",
    "p = pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "\n",
    "pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4760a-caa6-47dd-91c9-a82d53c6623b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distribuciones de Probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823dc229-ae4d-4071-adb3-b048149e62de",
   "metadata": {},
   "source": [
    "Cuando trabajamos con variables aleatorias (aquellas en las que el valor es resultado de un evento aleatorio) podemos calcular las  distribuciones de probabilidad que describen la posibilidad de que un evento ocurra, que a veces es más interesante que saber cuál es el resultado en sí. \n",
    "La distribución de probabilidad es una función que asigna a cada suceso definido sobre la variable aleatoria la probabilidad de que dicho suceso ocurra.\n",
    "\n",
    "Hay dos tipos de variables aleatorias y dependiendo de él tendremos unos tipos de distribuciones u otros.\n",
    "\n",
    "- Las *variables aleatorias discretas* son aquellas que solo toman ciertos valores (frecuentemente enteros) y que resultan principalmente del conteo realizado. Algunas distribuciones para variables discretas son:\n",
    "    - Distribución binomial\n",
    "    - Distribución de Poisson\n",
    "    - Distribución hipergeométrica \n",
    "\n",
    "\n",
    "- Las *variables aleatorias continuas* son aquellas que resultan generalmente de la medición y pueden tomar cualquier valor dentro de un intervalo dado. La distribución más común de las variables continuas es:\n",
    "    - Distribución normal o *gaussiana*\n",
    "\n",
    "![Choice distribution](images/t9_distributions_choice.png)\n",
    "*Una cosa que tenemos que saber es que las probabilidades en una distribución siempre suman 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f1d599-3b7d-4fc8-a095-e4c850bac480",
   "metadata": {},
   "source": [
    "### Representación distribuciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2589a-3baf-41be-9216-9030e80a22e1",
   "metadata": {},
   "source": [
    "Una de las mejores maneras de describir una variable es representar los valores que aparecen en el conjunto de datos y el número de veces que aparece cada valor. La representación más común de una distribución es un **histograma**, que es un gráfico que muestra la frecuencia de los valores que caen en cada bin. Ya hemos visto en el capítulo 7 cómo hacerlo con la librería *seaborn* pero también podemos usar la librería *matplotlib*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65229d-bac7-47ed-8425-9c03ab3c9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69694c-a062-456e-9740-000238ae3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando histograma\n",
    "mu, sigma = 0, 0.2 # media y desvio estandar\n",
    "datos = np.random.normal(mu, sigma, 1000) #creando muestra de datos (distribución normal)\n",
    "\n",
    "# histograma de distribución normal.\n",
    "cuenta, cajas, ignorar = plt.hist(datos, 20)\n",
    "plt.ylabel('frequencia')\n",
    "plt.xlabel('valores')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f07583-550a-4ca6-be69-7d1d6fdf6c78",
   "metadata": {},
   "source": [
    "Otra forma de representar a las distribuciones discretas es utilizando su **Función de Masa de Probabilidad o PMF** en inglés, la cual relaciona cada valor con su probabilidad en lugar de su frecuencia como vimos anteriormente. Esta función es normalizada de forma tal que el valor total de probabilidad sea 1. La ventaja que nos ofrece utilizar la FMP es que podemos comparar dos distribuciones sin necesidad de ser confundidos por las diferencias en el tamaño de las muestras. También debemos tener en cuenta que FMP funciona bien si el número de valores es pequeño;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09111269-08ee-43f0-9dce-d49581c73901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando FMP\n",
    "n, p = 30, 0.4 # parametros de forma de la distribución binomial\n",
    "n_1, p_1 = 20, 0.3 # parametros de forma de la distribución binomial\n",
    "x = np.arange(stats.binom.ppf(0.01, n, p),\n",
    "              stats.binom.ppf(0.99, n, p))\n",
    "x_1 = np.arange(stats.binom.ppf(0.01, n_1, p_1),\n",
    "              stats.binom.ppf(0.99, n_1, p_1))\n",
    "fmp = stats.binom.pmf(x, n, p) # Función de Masa de Probabilidad\n",
    "fmp_1 = stats.binom.pmf(x_1, n_1, p_1) # Función de Masa de Probabilidad\n",
    "plt.plot(x, fmp, '--')\n",
    "plt.plot(x_1, fmp_1)\n",
    "plt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\n",
    "plt.vlines(x_1, 0, fmp_1, colors='g', lw=5, alpha=0.5)\n",
    "plt.title('Función de Masa de Probabilidad')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe8b62-332a-4812-a0e1-805e20dac4d2",
   "metadata": {},
   "source": [
    "El equivalente a la PMF para distribuciones continuas es la **Función de Densidad de Probabilidad o PDF**. Esta función es la derivada de la Función de Distribución Acumulada. Por ejemplo, para la distribución normal que graficamos anteriormente, su PDF es la siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a299df-d4c8-43d6-9aed-b4af06c0a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando Función de Densidad de Probibilidad con Python\n",
    "FDP_normal = stats.norm(10, 1.2).pdf(x_1) # FDP\n",
    "plt.plot(x_1, FDP_normal, label='FDP nomal')\n",
    "plt.title('Función de Densidad de Probabilidad')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50f3c3-52cb-4fb1-b446-a2b59cc27ea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Si queremos evitar los problemas que se generan con FMP cuando el número de valores es muy grande, podemos recurrir a utilizar la **Función de Distribución Acumulada o CDF**, para representar a nuestras distribuciones, tanto discretas como continuas. Esta función relaciona los valores con su correspondiente percentil; es decir que va a describir la probabilidad de que una variable aleatoria X sujeta a cierta ley de distribución de probabilidad se sitúe en la zona de valores menores o iguales a x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82b2fc-37f4-4336-99e9-fc14d461b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando Función de Distribución Acumulada con Python\n",
    "x_1 = np.linspace(stats.norm(10, 1.2).ppf(0.01),\n",
    "                  stats.norm(10, 1.2).ppf(0.99), 100)\n",
    "fda_binom = stats.binom.cdf(x, n, p) # Función de Distribución Acumulada\n",
    "fda_normal = stats.norm(10, 1.2).cdf(x_1) # Función de Distribución Acumulada\n",
    "plt.plot(x, fda_binom, '--', label='FDA binomial')\n",
    "plt.plot(x_1, fda_normal, label='FDA nomal')\n",
    "plt.title('Función de Distribución Acumulada')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6d03c-d78f-4562-8f1a-e72524e35813",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13031e68-8209-48d3-8ed3-f0e712bdb93e",
   "metadata": {},
   "source": [
    "**Distribución binomial:**\n",
    "\n",
    "Distribución de probabilidad discreta que cuenta el número de éxitos en una secuencia de $n$ ensayos de Bernoulli independientes entre sí con una probabilidad fija $p$ de ocurrencia de éxito entre los ensayos. Un experimento de Bernoulli se caracteriza por ser dicotómico, esto es, solo dos resultados son posibles, a uno de estos se le denomina “éxito” y tiene una probabilidad de ocurrencia $p$ y al otro se le denomina “fracaso” y tiene una probabilidad $q=1-p$\n",
    "    \n",
    "<ins>Función de probabilidad</ins>\n",
    "    \n",
    "$$ P \\left[\\begin{array}{r}X=x\\end{array}\\right]= \\left(\\begin{array}{rr}n \\\\ x \\end{array}\\right)p^x \\left(1-p\\right)^{n-x} $$\n",
    "    \n",
    "<ins>Función de distribución acumulada</ins>\n",
    "\n",
    "$$ P \\left[\\begin{array}{r}X\\leq x\\end{array}\\right]=\\sum_{k=0}^{x} \\left(\\begin{array}{rr}n \\\\ k \\end{array}\\right)p^k \\left(1-p\\right)^{n-k} $$\n",
    "\n",
    "En python podemos hallar la distribución binomial con la función [`scipy.stats.binom`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5daf2-3bca-480f-ba62-b196178d1d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando Binomial\n",
    "N, p = 30, 0.4 # parametros de forma \n",
    "binomial = stats.binom(N, p) # Distribución\n",
    "x = np.arange(binomial.ppf(0.01),\n",
    "              binomial.ppf(0.99))\n",
    "fmp = binomial.pmf(x) # Función de Masa de Probabilidad\n",
    "plt.plot(x, fmp, '--')\n",
    "plt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\n",
    "plt.title('Distribución Binomial')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360f580-6c44-4458-81f4-d480639069e4",
   "metadata": {},
   "source": [
    "**Distribución Poisson:**\n",
    "\n",
    "La Distribución Poisson describe la probabilidad de encontrar exactamente r eventos en un lapso de tiempo si los acontecimientos se producen de forma independiente a una velocidad constante μ. Es una de las distribuciones más utilizadas en estadística con varias aplicaciones; como por ejemplo describir el número de fallos en un lote de materiales o la cantidad de llegadas por hora a un centro de servicios\n",
    "    \n",
    "<ins>Función de probabilidad</ins>\n",
    "    \n",
    "$$ P \\left(r;\\mu\\right)= \\frac{\\mu^r e^{-\\mu}}{r!} $$\n",
    "\n",
    "En dónde r es un entero (r≥0) y μ es un número real positivo.\n",
    "\n",
    "En python podemos hallar la distribución de poisson con la función [`scipy.stats.poisson`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ddfd2-7736-4203-83e9-26379b50be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando Poisson\n",
    "mu =  3.6 # parametro de forma \n",
    "poisson = stats.poisson(mu) # Distribución\n",
    "x = np.arange(poisson.ppf(0.01),\n",
    "              poisson.ppf(0.99))\n",
    "fmp = poisson.pmf(x) # Función de Masa de Probabilidad\n",
    "plt.plot(x, fmp, '--')\n",
    "plt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\n",
    "plt.title('Distribución Poisson')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.show()\n",
    "\n",
    "# histograma\n",
    "aleatorios = poisson.rvs(1000)  # genera aleatorios\n",
    "cuenta, cajas, ignorar = plt.hist(aleatorios, 20)\n",
    "plt.ylabel('frequencia')\n",
    "plt.xlabel('valores')\n",
    "plt.title('Histograma Poisson')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3717c77-a6f0-4a66-8e45-cfa6dbaf0674",
   "metadata": {},
   "source": [
    "**Distribución Hipergeométrica:**\n",
    "\n",
    "La Distribución Hipergeométrica describe experimentos en donde se seleccionan los elementos al azar sin reemplazo (se evita seleccionar el mismo elemento más de una vez). Más precisamente, supongamos que tenemos N elementos de los cuales M tienen un cierto atributo (y N−M no tiene). Si escogemos n elementos al azar sin reemplazo, p(r) es la probabilidad de que exactamente r de los elementos seleccionados provienen del grupo con el atributo.\n",
    "    \n",
    "<ins>Función de probabilidad</ins>\n",
    "    \n",
    "$$ P \\left(r;n,N,M\\right)= \\frac{\\left(\\begin{array}{rr} M \\\\ r \\end{array}\\right) \\left(\\begin{array}{rr} N-M \\\\ n-r \\end{array}\\right)}{\\left(\\begin{array}{rr}N \\\\ n \\end{array}\\right)} $$\n",
    "\n",
    "En dónde el valor de r esta limitado por max(0,n−N+M) y min(n,M) inclusive; y los parámetros n (1≤n≤N), N (N≥1) y M (M≥1) son todos números enteros.\n",
    "\n",
    "En python podemos hallar la distribución hipergeométrica con la función [`scipy.stats.hypergeom`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.hypergeom.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2cc85-cab8-45d6-9e98-acb3109497b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graficando Hipergeométrica\n",
    "M, n, N = 30, 10, 12 # parametros de forma \n",
    "hipergeometrica = stats.hypergeom(M, n, N) # Distribución\n",
    "x = np.arange(0, n+1)\n",
    "fmp = hipergeometrica.pmf(x) # Función de Masa de Probabilidad\n",
    "plt.plot(x, fmp, '--')\n",
    "plt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\n",
    "plt.title('Distribución Hipergeométrica')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.show()\n",
    "\n",
    "# histograma\n",
    "aleatorios = hipergeometrica.rvs(1000)  # genera aleatorios\n",
    "cuenta, cajas, ignorar = plt.hist(aleatorios, 20)\n",
    "plt.ylabel('frequencia')\n",
    "plt.xlabel('valores')\n",
    "plt.title('Histograma Hipergeométrica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf158fb-d02e-44cd-9ce4-7448c4e7ac59",
   "metadata": {},
   "source": [
    "**Distribución Normal o Gaussiana:**\n",
    "Es la distribución más utilizada en estadística\n",
    "Algunos ejemplos de variables asociadas a fenómenos naturales que siguen el modelo de la Distribución Normal son:\n",
    "\n",
    "- Características morfológicas de individuos, como la estatura;\n",
    "- Características sociológicas, como el consumo de cierto producto por un mismo grupo de individuos;\n",
    "- Características psicológicas, como el cociente intelectual;\n",
    "- Nivel de ruido en telecomunicaciones;\n",
    "\n",
    "<ins>Función de probabilidad</ins>\n",
    "    \n",
    "$$ P \\left(r;\\mu ,\\sigma ^2\\right)= {\\frac{1}{\\sigma \\sqrt{2\\pi}}} e^{\\frac{-1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right) ^2} $$\n",
    "\n",
    "En dónde $\\mu$ es el parámetro de ubicación, y va a ser igual a la media aritmética y $\\sigma ^2$ es el desvío estándar. \n",
    "\n",
    "En python podemos hallar la distribución gaussiana con la función [`scipy.stats.norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20082d90-f7a8-4900-a14e-e0eae2f53d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando Normal\n",
    "mu, sigma = 0, 0.2 # media y desvio estandar\n",
    "normal = stats.norm(mu, sigma)\n",
    "x = np.linspace(normal.ppf(0.01),\n",
    "                normal.ppf(0.99), 100)\n",
    "fp = normal.pdf(x) # Función de Probabilidad\n",
    "plt.plot(x, fp)\n",
    "plt.title('Distribución Normal')\n",
    "plt.ylabel('probabilidad')\n",
    "plt.xlabel('valores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# histograma\n",
    "aleatorios = normal.rvs(1000) # genera aleatorios\n",
    "cuenta, cajas, ignorar = plt.hist(aleatorios, 20)\n",
    "plt.ylabel('frequencia')\n",
    "plt.xlabel('valores')\n",
    "plt.title('Histograma Normal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
