{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb7e28d",
   "metadata": {},
   "source": [
    "<font size=6>\n",
    "\n",
    "<b>Curso de Análisis de Datos con Python</b>\n",
    "</font>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "Curso de formación interna, CIEMAT. <br/>\n",
    "Madrid, Octubre de 2021\n",
    "\n",
    "Antonio Delgado Peris y Cristina Labajo Villaverde\n",
    "</font>\n",
    "\n",
    "https://github.com/andelpe/curso-intro-python/\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a947c",
   "metadata": {},
   "source": [
    "# Tema 9. Análisis estadístico y Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d1571-d65a-46e5-876d-3b5902efce0e",
   "metadata": {},
   "source": [
    "El aprendizaje automático o *Machine Learning* (ML) es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos masivos y elaborar predicciones (análisis predictivo). Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma, es decir, sin necesidad de ser programados, consiguiendo esto gracias a su capacidad de analizar los datos, generalizar y asociar.\n",
    "\n",
    "El machine learning contribuye a buscar soluciones a problemas muy variados repartidos en distintos campos. Destacan:\n",
    "\n",
    "- **Sistemas de recomendación**: \n",
    "Se usa en plataformas online para ofrecer productos basados en anteriores compras, o por supermercados para hacer ofertas personalizadas a sus clientes. Otras plataformas, como Netflix o Spotify lo usan para recomendarte canciones, películas y series que se acomoden a tus gustos. La manera de obtener estos resultados es comparando las consumiciones de un individuo con las que han hecho otras personas de gustos similares. \n",
    "\n",
    "<br/>\n",
    "\n",
    "![ML_Recomendación_Netflix](images/t10_netflix.png)\n",
    "  \n",
    "<br/>\n",
    "\n",
    "\n",
    "- **Ciberseguridad**: \n",
    "El machine learning ayuda a los nuevos antivirus a escanear de manera más eficiente, aumentando la velocidad de detección y reconociendo anomalías.\n",
    "- **Automoción**:\n",
    "Los coches inteligentes son capaces de aprender automáticamente y serán capaces de ofrecer comodidades a los pasajeros basandose en conocimientos previos, como la inclinación del respaldo, la temperatura del coche, el sistema de seguridad para no salirse de la carretera, etc.\n",
    "\n",
    "<br/>\n",
    "\n",
    "![ML_automoción](images/t10_automocion.jpg)\n",
    "  \n",
    "<br/>\n",
    "\n",
    "- **Procesamiento de Lenguaje Natural (PLN)**: \n",
    "Es el algoritmo que permite a Siri entender lo que se le pide y hacer búsquedas satisfactorias, comprendiendo el lenguaje humano. Otro uso muy útil es la de traducir papeleo legal a un lenguaje más de a pie. \n",
    "- **Medicina**: \n",
    "En el campo de la medicina el ML tiene muchas aplicaciones y en los años venideros será de vital importancia. Ya se usa para detectar el cáncer de mama con mayor antelación, o diferenciar imágenes de rayos X con neumonía. También permitirá estandarizar muchos procesos que en la actualidad dependen de la pericia del médico observador.\n",
    "<br/>\n",
    "\n",
    "![ML_deteccion_cancer_mama](images/t10_cancer_mama.png)\n",
    "  \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71a20e-f318-439b-9227-c043d5f29d79",
   "metadata": {},
   "source": [
    "## Categorías\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff081d-9030-49ae-915c-aeb130276f3a",
   "metadata": {},
   "source": [
    "Las dos categorías más comunes dentro del ML son el aprendizaje supervisado y el aprendizaje no supervisado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53778a55-6e5a-40c6-aa43-996c1e9194e0",
   "metadata": {},
   "source": [
    "### Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977830ef-cb67-43a6-b8c8-973d7076b718",
   "metadata": {},
   "source": [
    "Aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones,por ejemplo imágenes de perros y gatos. Gracias al ML un ordenador sería capaz de identificar nuevas fotos, aunque para esto es necesario que seres humanos coloquen estas etiquetas con anterioridad para asegurar la efectividad y calidad de los datos.\n",
    "\n",
    "La idea es que las computadoras aprendan de una multitud de ejemplos, y a partir de ahí puedan hacer el resto de cálculos necesarios para que nosotros no tengamos que volver a ingresar ninguna información.\n",
    "\n",
    "A su vez dentro de la categoría de aprendizaje supervisado podemos enfrentarnos a dos tipos de problemas:\n",
    "\n",
    "- Regresores\n",
    "\n",
    "    Los regresores son los problemas que intentan predecir un valor continuo, o un conjunto de valores, a partir de unos valores de entrada o *inputs* previos. Por ejemplo: Tratar de predecir la cantidad de emisión de CO2 de un motor de coche dadas otras características de este, como el número de cilindrada. \n",
    "\n",
    "Las series temporales son un tipo de predictores específico.\n",
    "\n",
    "\n",
    "- Clasificadores \n",
    "\n",
    "    Son los problemas en los que, recibiendo como entrada cierta información de un objeto, es necesario indicar la categoría o clase a la que pertenece de entre un número acotado de clases posibles. Por ejemplo, podemos clasificar e-mails en spam o no. O podemos recibir como entrada la imagen de un animal y decir si se trata de un gato, un perro o un loro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60fad2d-c740-4dcf-9d9a-e5dbe14ee473",
   "metadata": {},
   "source": [
    "Estos dos grupos de problemas se pueden resolver usando la estadística mezclada con la inteligencia artificial. Por poner un par de ejemplos, introduciré de manera teórica los Árboles de decisiones y las Redes Neuronales , modelos de aprendizaje automático complejos que solucionan los tipos de problemas antes mencionados, pero que son complicados y no se cubrirá en este curso su desarrollo e implementación. \n",
    "\n",
    "- Árboles de decisión (regresores o clasificadores):\n",
    "\n",
    "Los algoritmos de aprendizaje basados en árboles se consideran uno de los mejores y más utilizados métodos de aprendizaje supervisado. Los métodos basados en árboles potencian los modelos predictivos con alta precisión, estabilidad y facilidad de interpretación y mapean bastante bien las relaciones no lineales. Son adaptables para resolver cualquier tipo de problema (clasificación o regresión).\n",
    "\n",
    "\n",
    "![Decission Trees](./images/t9_DT.JPG)\n",
    "\n",
    "\n",
    "Los árboles de decisión se construyen dividiendo el conjunto de entrenamiento en distintos nodos, donde un nodo contiene toda o la mayor parte de una categoría de datos. Se trata de probar un atributo y ramificar los casos según el resultado de la prueba. Cada nodo interno corresponde a una prueba, y cada rama corresponde a un resultado de la prueba, y cada nodo hoja asigna un elemento a una clase.\n",
    "Un aspecto esencial de los algoritmos basado en árboles de decisión es la elección del mejor criterio para la división de los datos.\n",
    "\n",
    "Se pueden construir considerando los atributos uno por uno. Primero, se elije un atributo del conjunto de datos. Se calcula la importancia del atributo en la división de los datos. \n",
    "Después, se dividen los datos según el valor del mejor atributo, luego hay que ir a cada rama y repetir el proceso para el resto de los atributos. \n",
    "\n",
    "Después de construir el árbol, este se puede usar para predecir la clase de casos desconocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933f020-0032-4ab8-8594-8de910f3265d",
   "metadata": {},
   "source": [
    "\n",
    "- Redes neuronales\n",
    "\n",
    "Las redes neuronales artificiales (también conocidas como sistemas conexionistas) son un modelo computacional evolucionado a partir de diversas aportaciones científicas que están registradas en la historia. Consiste en un conjunto de unidades, llamadas neuronas artificiales, conectadas entre sí para transmitirse señales. La información de entrada atraviesa la red neuronal (donde se somete a diversas operaciones) produciendo unos valores de salida.\n",
    "Los modelos de redes neuronales son capaces de encontrar relaciones (patrones) de forma inductiva por medio de los algoritmos de aprendizaje basado en los datos existentes más que requerir la ayuda de un modelador para especificar la forma funcional y sus interacciones.\n",
    "\n",
    "![redes neuronales](./images/t9_red_neuronal.jpg)\n",
    "\n",
    "Cada neurona está conectada con otras a través de unos enlaces. En estos enlaces el valor de salida de la neurona anterior es multiplicado por un valor de peso. Estos pesos en los enlaces pueden incrementar o inhibir el estado de activación de las neuronas adyacentes. Del mismo modo, a la salida de la neurona, puede existir una función limitadora o umbral, que modifica el valor resultado o impone un límite que no se debe sobrepasar antes de propagarse a otra neurona. Esta función se conoce como función de activación.\n",
    "\n",
    "Estos sistemas aprenden y se forman a sí mismos, en lugar de ser programados de forma explícita, y sobresalen en áreas donde la detección de soluciones o características es difícil de expresar con la programación convencional. Para realizar este aprendizaje automático, normalmente, se intenta minimizar una función de pérdida que evalúa la red en su total. Los valores de los pesos de las neuronas se van actualizando buscando reducir el valor de la función de pérdida. Este proceso se realiza mediante la propagación hacia atrás.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568a51e-1246-4d2b-b492-8a1ee4e6fa31",
   "metadata": {},
   "source": [
    "### Aprendizaje no supervisado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3583f-6f3e-45a7-8242-11c06215f6dc",
   "metadata": {},
   "source": [
    "Estos algoritmos no cuentan con un conocimiento previo. Se enfrentan al caos de datos con el objetivo de encontrar patrones que permitan organizarlos de alguna manera para ver si encuentra soluciones que el ojo humano no pueda ver. \n",
    "\n",
    "Por ejemplo, en el campo del marketing se utilizan para extraer patrones de datos masivos provenientes de las redes sociales y crear campañas de publicidad.\n",
    "\n",
    "Generalmente los algoritmos usados para el aprendizaje no supervisado, en comparación con el aprendizaje supervisado,  cuenta con menos modelos y menos sistemas de evaluación de esos modelos.  \n",
    "\n",
    "Una de las técnicas más populares pertenecientes al aprendizaje no supervisado es el *clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dbb20-61aa-48ab-90ce-c8123aaf7ebf",
   "metadata": {},
   "source": [
    "- Clustering\n",
    "\n",
    "![Clustering](./images/t10_clustering.png)\n",
    "Este proceso desarrolla una acción fundamental que le permite a los algoritmos de aprendizaje automatizado entrenar y conocer de forma adecuada los datos con los que desarrollan sus actividades.\n",
    "\n",
    "Este proceso ayuda a las máquinas a generar capacidades de análisis de forma rápida, en grandes volúmenes y con la menor cantidad de errores posibles.\n",
    "cuyo objetivo es formar grupos cerrados y homogéneos a partir de un conjunto de elementos que tienen diferentes características o propiedades, pero que comparten ciertas similitudes.\n",
    "\n",
    "El clustering es una tarea que tiene como finalidad principal lograr el agrupamiento de conjuntos de objetos no etiquetados, para lograr construir subconjuntos de datos conocidos como Clusters. Cada cluster está formado por una colección de objetos o datos que a términos de análisis resultan similares entre si, pero que poseen elementos diferenciales con respecto a otros objetos pertenecientes al conjunto de datos y que pueden conformar un cluster independiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f6946-75ef-4512-8db5-83e57eda34f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelos e hipótesis\n",
    "\n",
    "Cuando trabajamos con datos es una práctica común el crear modelos e intentar ajustar nuestros datos a esos modelos. Un modelo de aprendizaje automático se ha entrenado para reconocer determinados tipos de patrones. Puede entrenar un modelo con un conjunto de datos, y proporcionarle un algoritmo que puede usar para averiguar y obtener información de esos datos\n",
    "\n",
    "Tendremos que entrenar nuestos modelos con decenas,cientos, o miles de datos similares antes de que hagan buenas prediciones o clasificaciones. \n",
    "\n",
    "Un aspecto importante a tener en cuenta es que los datos con los que trabajemos deben ser buenos. Si son malos, o no tienen ninguna relación entre ellos, los modelos que construyamos y los resultamos que obtengamos carecerán de significado. *La programación debe ir de la mano con el sentido común*.\n",
    "\n",
    "Además deberemos de disponer de los suficientes ejemplos como para entrenar nuestros modelos, pues poca diversidad en nuestros datos puede influir en el resultado de nuestras predicciones, y también tenemos que tener en cuenta cómo se han obtenido nuestros datos, pues eso puede contribuir a tener respuestas erróneas.\n",
    "\n",
    "Otro problema que debemos evitar es el sobreaprendizaje u *overfitting*. Nuestro modelo tiene que ser capaz de aprender y generalizar para diferentes sets de datos, si sólo funciona con los que le hemos entrenado podemos tener un caso de sobreaprendizaje y sería un mal modelo. \n",
    "\n",
    "Hay muchos tipos de modelos que se usan en la actualidad, pero aquí vamos a ver los modelos de Regresión y Clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6732ca",
   "metadata": {},
   "source": [
    "### Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e63f43",
   "metadata": {},
   "source": [
    "La regresión es el proceso de predecir un valor continuo. En regresión hay dos tipos de valores:\n",
    "\n",
    "- La **variable dependiente** (y) es aquella que queremos averiguar y predecir, lo que buscamos hallar. Es necesario que sea un valor continuo. \n",
    "- Las **variables independientes** (x) se pueden ver como las causas de las que depende la variable dependiente. Pueden ser valore continuos o discretos, numéricos o categóricos. \n",
    "\n",
    "Nuestro modelo de regresión lo que hará será relacionar nuestra variable dependiente con nuestra variable o variables independientes. Construimos nuestro modelo de regresión usando los datos de los que disponemos y después de entrenarlo nuestro modelo será capaz de predecir el valor de la variable dependiente de un nuevo evento dadas las variables independientes.\n",
    "\n",
    "Dentro de la regresión podemos encontrar dos tipos:\n",
    "- Regresión simple:\n",
    "Es cuando una sóla variable independiente se usa para estimar la variable dependiente. A su vez la regresión simple puede ser **lineal** o **no lineal**. La linearidad de la regresión depende de la naturaleza de la relación entre variables dependientes e independientes.\n",
    "\n",
    "- Regresión múltiple:\n",
    "La regresión es múltiple cuando hay más de una variable independiente y, como en el caso de la regresión simple puede ser **lineal** o **no lineal**\n",
    "\n",
    "Algunas de las aplicaciones de regresión lineal son la de predecir la lista de la compra de las personas con variables independientes como la edad, los estudios, el país, etc. También se usa para estimar el precio de la vivienda de una localidad, por ejemplo.\n",
    "\n",
    "*Hay muchos ejemplos distintos de algoritmos que se pueden aplicar para crear modelos de regresión*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6268a-ee3b-46ae-a12b-9a5d72d825cd",
   "metadata": {},
   "source": [
    "#### Regresión Lineal Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a3b68-6a03-43b0-ba69-da12c83b4b0c",
   "metadata": {},
   "source": [
    "Vamos a empezar con un ejemplo de un dataset sobre coches, sus características, consumo y emisiones de CO2.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26fb6f-9106-459f-b50e-24c9e668e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error## Creo que esto sobraría\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87666d-990a-44b4-813e-9e6ec68ea1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/FuelConsumption.csv\")\n",
    "cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] \n",
    "cdf ## Datasat reducido con variables de interés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf8eac-243c-484a-98a2-328323173fd4",
   "metadata": {},
   "source": [
    "##### Construir el modelo\n",
    "Usando la regresión linear podemos predecir el valor de nuestra *'y'* (CO2EMISSIONS) usando otras variables. \n",
    "La regresión lineal simple nos dirá la relación entre dos variables y cómo el cambio de unas afectan a otras. \n",
    "\n",
    "En este ejemplo nuestra variable dependiente será la emisión de CO2, aquello que queremos predecir y elegiremos como variable independiente el tamaño del motor (ENGINESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714cfb7-55a3-4f15-b482-464b5ff2dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cdf.ENGINESIZE\n",
    "y=cdf.CO2EMISSIONS\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(x, y,edgecolor='black')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Scatter Engine size vs CO2 emissions\")\n",
    "\n",
    "# plt.scatter(x=cdf.ENGINESIZE, y=cdf.CO2EMISSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a97f4-589d-43cd-aa99-1672044f010d",
   "metadata": {},
   "source": [
    "Este plot indica que las variables están relacionadas linealmente pues, usando regresión lineal, podemos pintar una recta que representaría la relación entre ellas. La línea de ajuste se representa como un polinomio, con la ecuación de una recta. \n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1} \\cdot x_{1}$$\n",
    "\n",
    "En esta ecuación, $\\hat{y}$ es el valor de la variable dependiente que vamos a predecir, $x_{1}$ es la variable independiente y $\\theta_{0}$ y $\\theta_{1}$ son los parámetros que determinan la pendiente y la  intersección de la recta con el eje de ordenadas.\n",
    "\n",
    "Con la regresión lineal se estiman los parámetros $\\theta_{0}$ y $\\theta_{1}$ para hayar la recta que mejor se ajusta a nuestros datos. \n",
    "\n",
    "Aunque podemos hallar la recta de mejor ajuste usando las matemáticas, scikit learn tiene funciones para ello. A continuación se muestra un ejemplo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50ec3b-cc14-47d1-97e2-152b2c490c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestras variables dependiente e independiente\n",
    "x1=x.values.reshape(-1, 1)  #Sklearn requiere que los datos estén en la forma (n,1)\n",
    "y1=y.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ff75f-76d4-48a2-abb8-f3964c872168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# Ajustamos nuestros datos de entrenamiento al modelo.  \n",
    "model.fit(x1, y1)\n",
    "\n",
    "# Hacemos predicciones sobre la variable independiente para obtener la variable dependiente. \n",
    "y_pred = model.predict(x1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bf2b3-cd85-43f0-8eee-c6992cd9028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los coeficientes o parámetros\n",
    "print('Intercept: \\n', model.intercept_) #theta0\n",
    "print('Coefficients: \\n', model.coef_) #theta1\n",
    "\n",
    "# # Evaluar la precisión con el error cuadrático medio\n",
    "# print('Mean squared error: %.2f'\n",
    "#       % mean_squared_error(y1, y_pred))\n",
    "# # The coefficient of determination: 1 is perfect prediction\n",
    "# print('Coefficient of determination: %.2f'\n",
    "#       % r2_score(y1, y_pred))\n",
    "\n",
    "# Mostramos los resultados\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(x1, y1,  color='black')\n",
    "axs.plot(x1, y_pred, color='blue', linewidth=3)\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Scatter Engine size vs CO2 emissions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f025799-0e5e-4767-ab9e-ffe34f727633",
   "metadata": {},
   "source": [
    "Cuando usamos aprendizaje automático es necesario entrenar el modelo con nuestros datos, por lo tanto tendremos datos de entrenamiento. Después hay que probar nuestro modelo, por lo que tendremos que dejar parte de nuestros datos para ello. Esta porción de datos recibe el nombre datos de test y son a los que aplicaremos nuestro modelo y compararemos con las soluciones que ya teniamos anteriormente para evaluar la precisión de nuestro modelo. Hay varias formas de dividir nuestro dataset original para obtener nuestros subsets de entrenamiento (train) y prueba (test).  \n",
    "\n",
    "- <ins>Train y test del mismo dataset</ins>\n",
    "\n",
    "    Entrenamos nuestro modelo con todo nuestro dataset y luego lo probamos con una parte de él. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0837eb-1c3a-4018-bba7-d27d35ba465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra variable independiente es el tamaño del motor. Definimos el train y el test \n",
    "X_train1 = cdf.ENGINESIZE.values.reshape(-1, 1)\n",
    "X_test1 = cdf.ENGINESIZE[-100:].values.reshape(-1, 1)#Para el test elegimos los últimos 100 valores\n",
    "\n",
    "# Nuestra variable dependiente son las emisiones de CO2. Definimos el train y el test \n",
    "y_train1 = cdf.CO2EMISSIONS.values.reshape(-1, 1)\n",
    "y_test1 = cdf.CO2EMISSIONS[-100:].values.reshape(-1, 1)#Para el test elegimos los últimos 100 valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdff3d-46f3-40f3-9855-c7dd87693e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto de regresión lineal\n",
    "regr1 = linear_model.LinearRegression()\n",
    "\n",
    "# Entrenamos el modelo usando los sets de entrenamiento\n",
    "regr1.fit(X_train1,y_train1)\n",
    "\n",
    "# Hacemos predicciones usando el set de test\n",
    "y_pred1 = regr1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643810e-f809-4d44-bffb-1a7d427f3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Score:', regr1.score(X_test1, y_test1))\n",
    "# Los coeficientes\n",
    "print('Intercept: \\n', regr1.intercept_)#theta0\n",
    "print('Coefficient: \\n', regr1.coef_[0])#theta1\n",
    "\n",
    "\n",
    "# # El error cuadrático medio (Mean Squared Error(MSE))\n",
    "# print('Mean squared error: %.2f'\n",
    "#       % metrics.mean_squared_error(y_test1,y_pred1))\n",
    "# # El coeficiente de determinación (R^2): 1 el la predicción perfecta\n",
    "# print('Coefficient of determination: %.2f'\n",
    "#       % metrics.r2_score(y_test1,y_pred1))\n",
    "\n",
    "\n",
    "# Mostramos los resultados\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(X_test1, y_test1,  color='black')\n",
    "axs.plot(X_test1,y_pred1, color='blue', linewidth=3, label='regression line')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Test Engine size vs CO2 emissions\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a263a3-e08f-4d1b-909c-a6c68e914cca",
   "metadata": {},
   "source": [
    "Cuando ya hemos entrenado y probado nuestro modelo debemos de evaluar la precisión de nuestras predicciones. Como tenemos los verdaderos valores de la variable independiente podemos evaluar el error de nuestras predicciones, por ejemplo usando la ecuación del Error Absoluto Medio (Mean Absolute Error (MAE)).\n",
    "$$Mean Absolute Error= \\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\hat{y_{j}} \\mid $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b7636-cc5f-4e07-939a-853fce372127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El error absoluto medio (Mean Absolute Error(MAE))\n",
    "print('Mean Absolute Error: %.2f'\n",
    "      % metrics.mean_absolute_error(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadaba14-9ab2-4ec9-9ea5-b82673436f7c",
   "metadata": {},
   "source": [
    "Otra métrica que es ampliamente utilizada en estadística es el coeficiente de determinación o $R^2$ que explicaremos más adelante, de momento lo único que necesitamos saber es que cuanto más cercano a 1 sea, significa que nuestro modelo es más preciso. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9403b5-bb61-43ba-a7c3-48f438c8eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f'\n",
    "      % metrics.r2_score(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec79fb-eb89-437f-abbd-51d890a8249e",
   "metadata": {},
   "source": [
    "Este método tiene un <span style=\"color:green\">alto porcentaje de precisión de entrenamiento</span> que es el porcentaje de predicciones correctas que hace nuestro modelo cuando usamos el subset de _train_. Pero no siempre un alto porcentaje de precisión significa que nuestro modelo es bueno, usando este método de train/test podemos dar lugar a un <span style=\"color:red\">*overfitting*</span> o sobreajuste, que significa que nuestro modelo ha aprendido demasiado bien los ejemplos que le hemos dado, puede capturar ruido y no sería capaz de generalizar si se le dieran otros datos distintos. \n",
    "\n",
    "Por lo tanto tiene <span style=\"color:red\">baja precisión con datos que no están en el dataset de entrenamiento</span>. Esto se soluciona haciendo una división distinta de train y test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70612da1-ee68-474c-8c8a-1244dee71147",
   "metadata": {},
   "source": [
    "- <ins>Train/test split</ins>:\n",
    "\n",
    "    Cogemos nuestro dataset y lo dividimos en dos grupos, el train y el test. Entrenamos con el train y luego predecimos el test para ver cómo de buena es nuestra predicción. La diferencia es que, en este caso, nuestro modelo no ha visto los ejemplos del test con anterioridad, lo que evita el overfitting. \n",
    "    \n",
    "    A mano se haría de la siguiente forma:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf2ae8-aa1a-4679-a551-58af0d760d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra variable independiente es el tamaño del motor. Definimos el train y el test \n",
    "X_train2 = cdf.ENGINESIZE[:-100].values.reshape(-1, 1)\n",
    "X_test2 = cdf.ENGINESIZE[-100:].values.reshape(-1, 1)#Para el test elegimos los últimos 100 valores\n",
    "\n",
    "# Nuestra variable dependiente son las emisiones de CO2. Definimos el train y el test \n",
    "y_train2 = cdf.CO2EMISSIONS[:-100].values.reshape(-1, 1)\n",
    "y_test2 = cdf.CO2EMISSIONS[-100:].values.reshape(-1, 1)#Para el test elegimos los últimos 100 valores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a8712-7911-4b18-b1a4-1246692edb4b",
   "metadata": {},
   "source": [
    "Pero dependiendo de la naturaleza de nuestros datos puede que nos interese obtener muestras aleatorias y hacer nuestros subdatasets variados, por lo que podríamos usar la función [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de la librería sklearn directamente. A esta función tenemos que darle la proporción de muestras que queremos dejar en el test y nos sacaría las 4 variables (X_train, X_test, y_train, y_test) a la vez, también podemos pedirle que mezcle nuestros datos con el atributo `random_state`igualado a un entero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106104f-423a-4455-89f5-d3ea49436051",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cdf.ENGINESIZE.values.reshape(-1, 1)\n",
    "y=cdf.CO2EMISSIONS.values.reshape(-1, 1)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef79c3-8494-4f1a-a5f7-a4a540bde913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto de regresión lineal\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# Entrenamos el modelo usando los sets de entrenamiento\n",
    "regr2.fit(X_train2,y_train2)\n",
    "\n",
    "# Hacemos predicciones usando el set de test\n",
    "y_pred2 = regr2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14b24c-0eb4-4ffa-9da7-4add0bf6757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los coeficientes\n",
    "print('Intercept: \\n', regr2.intercept_)#theta0\n",
    "print('Coefficient: \\n', regr2.coef_[0])#theta1\n",
    "\n",
    "# El error cuadrático medio (Mean Squared Error(MSE))\n",
    "# print('Mean squared error: %.2f'\n",
    "#       % metrics.mean_squared_error(y_test2,y_pred2))\n",
    "# # El coeficiente de determinación (R^2): 1 el la predicción perfecta\n",
    "# print('Coefficient of determination: %.2f'\n",
    "#       % metrics.r2_score(y_test2,y_pred2))\n",
    "\n",
    "\n",
    "# Mostramos los resultados\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(X_test2, y_test2,  color='black')\n",
    "axs.plot(X_test2,y_pred2, color='blue', linewidth=3, label='regression line')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Test Engine size vs CO2 emissions\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a767b3b-cfc0-4941-b0ca-2ecd86dc7752",
   "metadata": {},
   "source": [
    "Como los ejemplos de test y train son mutuamente excluyentes esto hace que aumente la precisión para ejemplos que no están originalmente en el dataset. Aún así el modelo sigue siendo bastante dependiente del conjunto de datos que tenemos y con el que entrenamos el modelo. \n",
    "\n",
    "Además no debemos de olvidar entrenar el modelo con los ejemplos que hemos dejado en el test, porque si no estaríamos desperdiciando datos válidos con los que contruir un modelo más robusto. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc6f7e-fe29-4e7b-8e76-02578f4379c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr2.fit(X_test2,y_test2)\n",
    "\n",
    "# # Hacemos predicciones usando el set de test\n",
    "# y_pred2 = regr2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9a679-65a8-4457-806a-b2a03ca381e4",
   "metadata": {},
   "source": [
    "- <ins>K-fold cross validation</ins>:\n",
    "\n",
    "    Otro método para dividir el dataset en subdatasets train y test es el método *K-fold cross validation*. Consiste en dividir los datos de forma aleatoria en k grupos de aproximadamente el mismo tamaño, k-1 grupos se emplean para entrenar el modelo y uno de los grupos se emplea como validación. Este proceso se repite k veces utilizando un grupo distinto como validación en cada iteración. El proceso genera k estimaciones del error cuyo promedio se emplea como estimación final.\n",
    "    <center>\n",
    "    <img src=\"./images/t9_kfold_crossvalidation.JPG\"  style=\"width: 300px;\"/>\n",
    "    </center>\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598569f1-39d4-42ff-b25e-93e261be4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cdf.ENGINESIZE.values.reshape(-1, 1)\n",
    "y=cdf.CO2EMISSIONS.values.reshape(-1, 1)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X,y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e85635-59ac-4f06-bcef-118c10f9757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)#Definimos cuantos loops vamos a hacer de Kfold\n",
    "regr3 = linear_model.LinearRegression()\n",
    "regr3.fit(X_train3, y_train3)\n",
    "y_pred3 = regr3.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b029b1f-3dd5-43f0-ae31-8110d8799b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = regr3.score(X_train3,y_train3)\n",
    "print(\"Metrica del modelo\", score)\n",
    "scores = cross_val_score(regr3, X_train3, y_train3, scoring='r2', cv=kf)\n",
    "print(\"Metricas cross_validation\", scores)\n",
    "print(\"Media de cross_validation\", scores.mean())\n",
    "\n",
    "score_pred = metrics.r2_score(y_test3,y_pred3)\n",
    "print(\"Metrica en Test\", score_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a556031-0c4e-4662-a4ab-0d7013f4746d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Evaluación del modelo. Cálculo de errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95776447-f4f9-440d-a8ff-5f5b2efbeb69",
   "metadata": {},
   "source": [
    "Entendemos como error lo lejos que está un valor de la recta de mejor ajuste. A mayor error, menor precisión de nuestro modelo. Como nos interesa saber la fiabilidad de nuestro modelo tenemos que saber cómo es el error de nuestra predicción. \n",
    "\n",
    "Existen varios tipos de error. Elegiremos la métrica apropiada dependiendo del modelo, del tipo de datos que tengamos, de la naturaleza de estos o del dominio de los datos que poseemos. A continuación nombramos algunos errores y cómo usar las librerías Scikit y Numpy para hallarlos: \n",
    "\n",
    "- Mean Absolute Error (MAE):\n",
    "    $$Mean Absolute Error= \\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\hat{y_{j}} \\mid $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b1151-b717-4218-a112-81c115d645dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El error absoluto medio (Mean Absolute Error(MAE))\n",
    "print('Mean Absolute Error: %.2f'\n",
    "      % metrics.mean_absolute_error(y_test3,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76626fab-8515-4c62-83ff-762b6b97f535",
   "metadata": {},
   "source": [
    "- Mean Squared error (MSE): \n",
    "\n",
    "    Se usa cuando queremos dar más peso a los errores mayores, pues aumenta exponencialmente con el error.\n",
    "    $$Mean Squared Error= \\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} - \\hat{y_{j}} \\right) ^ 2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d987dc-8901-439b-94a3-8e682968a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El error cuadrático medio (Mean Squared Error(MSE))\n",
    "print('Mean squared error: %.2f'\n",
    "      % metrics.mean_squared_error(y_test3,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71d995-5a49-4f95-b060-b56fd7d6ed37",
   "metadata": {},
   "source": [
    "- Root Mean Squared Error (RMSE): \n",
    "\n",
    "    Interpretable en las mismas unidades que el vector respuesta\n",
    "     $$Root Mean Squared Error=\\sqrt {\\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} - \\hat{y_{j}} \\right) ^ 2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a9370-1673-4195-b0a8-75634d258a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La raíz del error cuadrático medio (Root Mean Squared Error(RMSE))\n",
    "print('Root mean squared error: %.2f'\n",
    "      % metrics.mean_squared_error(y_test3,y_pred3, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7379c2-d2ff-46fd-9abf-497b5b1ab7e7",
   "metadata": {},
   "source": [
    "- Relative Absolute Value (RAE): \n",
    "\n",
    "    MAE normalizado \n",
    "     $$Relative Absolute Error= \\frac{\\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\hat{y_{j}}\\mid}{\\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\overline{y} \\mid} $$\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c640f1e-49bb-4b03-8478-97382673419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tenemos que usar numpy\n",
    "RAE_numpy = np.sum(np.abs(np.subtract(y_test3,y_pred3))) / np.sum(np.abs(np.subtract(y_test3, np.mean(y_test3))))\n",
    "print (\"RAE using Numpy: % \", RAE_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a438de3-8e51-4add-b681-a8c362c3916e",
   "metadata": {},
   "source": [
    "- Relative Squared Error: \n",
    "$$Relative Squared Error= \\frac{\\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} - \\hat{y_{j}} \\right) ^ 2}{\\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} -\\overline{y} \\right) ^ 2} $$\n",
    "\n",
    "Este cálculo se usa para calcular $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f8b5e-9c53-49fe-89a1-0950ea9ade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usando numpy\n",
    "\n",
    "RSE_numpy = np.sum(np.square(np.subtract(y_test3,y_pred3))) / np.sum(np.square(np.subtract(y_test3, np.mean(y_test3))))\n",
    "print (\"RSE using Numpy: % \", RSE_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca12a63-3a5b-4b73-b850-1cbc36aeb7b0",
   "metadata": {},
   "source": [
    "- Coeficiente de determinación ($R^2$):\n",
    "\n",
    "    No es un error puramente hablando pero es una medida muy utilizada por los científicos para cuantificar la precisión de un modelo.\n",
    "    Representa como de cercanos a la recta de regresión son los valores. Cuanto más cercano a uno, mejor será nuestro modelo. \n",
    "    $$R^2= 1- RSE$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ba3da-1782-45d0-9492-6813e24d94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El coeficiente de determinación (R^2): 1 el la predicción perfecta\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % metrics.r2_score(y_test3,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872773e-839e-42c4-a617-78aa271d76f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e9_1:** \n",
    "\n",
    "Dado el dataset `casas` que contiene precios de casas dados las características de las mismas. Queremos predecir el precio de nuevas casas (y) basándonos en sus pies cuadrados habitables (x).  \n",
    "\n",
    "Contruir el modelo siguiendo los siguientes pasos:\n",
    "- Definir variables x e y\n",
    "- Dividir dataset en train y test\n",
    "- Definir modelo Simple linear regression\n",
    "- Ajustar datos\n",
    "- Mostrar coeficientes hallados\n",
    "- Dibujar recta de regresión lineal en scatter plot\n",
    "- Evaluar usando Coeficiente de determinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca5953-becc-4889-99aa-b4510dd57add",
   "metadata": {},
   "outputs": [],
   "source": [
    "casas=pd.read_csv('./data/kc_house_data.csv', header='infer')\n",
    "casas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580d5f1-668a-4cc0-95b5-cb5edb2710a5",
   "metadata": {},
   "source": [
    "#### Regresión Lineal Múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480846e-a1b2-42b5-8a60-cd2da5365be5",
   "metadata": {},
   "source": [
    "La regresión lineal múltiple se diferencia de la simple por tener más de una variable independiente que influyen en el valor de la variable dependiente. \n",
    "La regresión lineal múltiple se puede usar para estudiar el grado de influencia que el cambio en las distintas variables tienen en la variable dependiente. Por ejemplo, en los resultados de un examen, ¿tienen efecto, y si lo tienen en qué grado, las horas de sueño, el tiempo de repaso o la asistencia a clase? \n",
    "También se usa para predecir como la variable dependiente cambia cuando lo hacen las variables independientes.\n",
    "\n",
    "En este caso la variable dependiente $y$ es una combinación lineal de las variables independientes X. Por ejemplo, podemos predecir la emisión de CO2 de un coche fijándonos no sólo en el tamaño del motor, sino en el consumo de carburante o el número de cilindros. \n",
    "\n",
    "En este caso la ecuación sería:\n",
    "\n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$$\n",
    "\n",
    "Que viene a ser lo mismo que una multiplicación de 2 vectores, un vector de parámetros y otro de variables, siendo el primer elemento del vector X igual a 1 para mantener el término independiente $\\theta_{0}$.\n",
    "\n",
    "$$\\hat{y} = \\theta^T X$$\n",
    "\n",
    "El objetivo de la regresión lineal múltiple es encontrar los parámetros que minimizan el MSE. Hay dos modos de hacerlo:\n",
    "\n",
    "- Mínimos cuadrados ordinarios. \n",
    "    \n",
    "    Intenta estimar los coeficientes minimizando el MSE usando operaciones algebraicas. Es un proceso complejo y que lleva tiempo, sobretodo con datasets grandes \n",
    "   \n",
    "- Algoritmo de optimización. \n",
    "\n",
    "    Se minimiza el error usando algoritmos iterativos. Un ejemplo muy común es el llamado Gradiente Descendente (Gradient Descent) que seleciona los coeficientes aleatoriamente y va calculando los errores y cambiando parámetros hasta que encuentra los valores que minimizan este error. Suele funcionar bien en datasets grandes.\n",
    "    \n",
    "Debemos controlar las variables independientes que usamos, no es conveniente usar demasiadas pues podemos causar overfitting. Es importante conocer la naturaleza de nuestros datos y evitar usar variables que no sean de utilidad o redundantes. \n",
    "Otro punto a tener en cuenta son las variables categoricas. Podemos usar variables categóricas como variables independientes pero deben ser convertidas a numéricas con antelación. Para ello podemos usar el método de One-Hot Encoding que vimos en el tema 7 usando la función `get_dummies()`. \n",
    " \n",
    "Veamos un ejemplo de implementación de Regresión Lineal Múltiple: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b77823-81b6-4f08-ba84-5a3db89fbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9620123-f6d3-4514-9d24-879946281c14",
   "metadata": {},
   "source": [
    "Como hemos visto anteriormente las variables que usemos en la regresión lineal múltiple tienen que ser variables que tengan un mínimo de influencia en el resultado de nuestra variable independiente. Esto se ve dibujando las importancias de las variables o la correlación entre ellas con el coeficiente de Pearson como hemos visto con anterioridad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e433137-7092-4319-8042-adfa330c291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = cdf.select_dtypes(include=['float64', 'int']).corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c8d91-cc23-4def-98cc-860aacb335f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot     = True,\n",
    "    cbar      = False,\n",
    "    annot_kws = {\"size\": 8},\n",
    "    vmin      = -1,\n",
    "    vmax      = 1,\n",
    "    center    = 0,\n",
    "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
    "    square    = True,\n",
    "    ax        = ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation = 45,\n",
    "    horizontalalignment = 'right',\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da434687-a449-489f-9e1f-19cadb44dd3b",
   "metadata": {},
   "source": [
    "Hay gran correlación entre ellas, con lo cuál podemos usarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e9793-ddb7-4abb-8115-5987deea20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos las variables independientes\n",
    "xm = cdf.drop('CO2EMISSIONS',axis=1)\n",
    "#Separamos nuestra variable dependiente CO2EMISSIONS \n",
    "ym = cdf['CO2EMISSIONS']\n",
    "\n",
    "# Dividimos el dataset en train/test\n",
    "xm_train, xm_test, ym_train, ym_test = train_test_split(xm, ym, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d95f02-9a3f-4fb3-9deb-fd1ea71a1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto clase Regresión Lineal\n",
    "mregr = linear_model.LinearRegression()\n",
    "# Ajustamos con datos de entrenamiento\n",
    "mregr.fit(xm_train,ym_train)\n",
    "ym_pred = mregr.predict(xm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b7f1e-a38c-42b4-af67-40953b97c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos errores\n",
    "score=metrics.r2_score(ym_test,ym_pred)\n",
    "print('r2 score is',score)\n",
    "print('mean_sqrd_error is==',metrics.mean_squared_error(ym_test,ym_pred))\n",
    "print('root_mean_squared error of is==',np.sqrt(metrics.mean_squared_error(ym_test,ym_pred)))\n",
    "\n",
    "# Coeficientes\n",
    "\n",
    "print('Intercept: \\n', mregr.intercept_)#theta0\n",
    "print('Coeficientes: \\n', mregr.coef_)#theta_1-theta_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a6d75-3f59-4b25-8403-17faa3679d64",
   "metadata": {},
   "source": [
    "### Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876a298-c2ba-4e71-8fe0-2b5aa9797389",
   "metadata": {},
   "source": [
    "Los problemas de clasificación son un tipo de problemas de aprendizaje supervisado en el que se categorizan o clasifican elementos en dos o más clases distintas. Un ejemplo de esto es un clasificador que te diferencia imágenes de gato o perro.\n",
    "\n",
    "Por lo tanto la variable que se quiere conocer es una variable categórica y lo que hace el algoritmo es tratar de establecer relaciones entre otras variables y la de interés para ver la influencia de estas otras variables en el resultado de la clasificación.\n",
    "\n",
    "Como en el caso de la regresión, la clasificación se puede hacer tanto para un ejemplo de dos clases distintas como para varias. \n",
    "Esto proporciona una amplia gama de posibilidades para la clasificación. Por ejemplo, la clasificación se puede utilizar para filtrado de correo electrónico, reconocimiento de voz, reconocimiento de escritura a mano, identificación biométrica, clasificación de documentos y mucho más.\n",
    "\n",
    "Hay varios algoritmos de clasificación. Aquí se explicara el método K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1fb5d-5aed-475a-9d8c-ab196f89f684",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656125a7-2673-47e5-aa5c-da4dc20c7ce0",
   "metadata": {},
   "source": [
    "Cuando tenemos un dataset con etiquetas ya puestas podemos plotear nuestros datos según las clases de los datos más próximos.\n",
    "\n",
    "El algoritmo de los K-vecinos próximos (K-Nearest Neighbors) es un método de clasificación que coge puntos ya etiquetados y los usa para aprender a etiquetar datos desconocidos. Se basa en la similitud con casos cercanos. \n",
    "<center>\n",
    "<td> <img src=\"./images/t9_knn.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</center>\n",
    "\n",
    "Cómo funciona:\n",
    "\n",
    "1. Elegimos un valor para K\n",
    "2. Calculamos la distancia entre el nuevo caso y los demás casos ya etiquetados del dataset\n",
    "3. Escogemos los K eventos que tienen una menor distancia con el valor desconocido\n",
    "4. Predecir el valor del nuevo evento con el valor más popular de los K puntos más cercanos.\n",
    "\n",
    "Para calcular la distancia de los valores al punto desconocido se utiliza la distancia euclidiana en el espacio de los elementos. También se pueden usar pesos para ponderar las distancias y así dar más importancia a los valores más cercanos.\n",
    "\n",
    "Otro aspecto importante para este algoritmo es la elección de una k correcta. \n",
    "Una K muy baja crea un modelo muy complejo que puede resultar en *overfitting*, impidiendo la generalización con nuevos datos. No se puede confiar en las predicciones de este tipo de modelos.\n",
    "Por otro lado, una K muy alta entonces el modelo es muy complejo y generaliza demasiado por lo que tampoco es bueno. \n",
    "\n",
    "La solución es reservar una parte de los daros para entrenar y otra para probar la precisión del modelo. Después de probar con varias Ks se evaluará el modelo y se elegirá la K que mejor convenga. \n",
    "\n",
    "Vamos a ver un ejemplo usando un dataset de pacientes con diabetes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0120c8a-5ef5-476f-8118-a4621cfe3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el dataset\n",
    "diab = pd.read_csv('./data/diabetes_data.csv')\n",
    "\n",
    "#Comprobamos los datos\n",
    "\n",
    "print('Dimensiones dataset:', diab.shape)\n",
    "diab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d01583-dc8c-4ce6-8f1b-3863c7727664",
   "metadata": {},
   "source": [
    "Nuestro objetivo con este dataset es poder clasificar los pacientes en Diabetes o No diabetes, con lo que nuestra variable dependiente será `diabetes` y las demás variables formarán nuestra X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70427906-0c2e-4310-ae4b-294098dadfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos nuestra X con todas las variables excepto la variable de interés.\n",
    "Xc = diab.drop(columns=['diabetes'])\n",
    "\n",
    "#Comprobamos que hemos eliminado la columna diabetes\n",
    "Xc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d322760-deeb-4256-82e1-543a29196488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos la variable de interés\n",
    "yc = diab['diabetes'].values\n",
    "\n",
    "#Comprobamos\n",
    "yc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d42590-ee72-4d7d-9fc7-b0971a57d68c",
   "metadata": {},
   "source": [
    "Separamos los ejemplos de nuesto dataset en train y test para poder entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abd646-42ad-48ce-818c-3f2a6ac0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split dataset into train and test data\n",
    "X_trainc, X_testc, y_trainc, y_testc = train_test_split(Xc, yc, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c5a82-9fa5-4061-b77e-c7b40b9a2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Definimos el modelo KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# Ajstamos nuestros datos al clasificador\n",
    "knn.fit(X_trainc,y_trainc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e97f6-bcfe-4ccd-b92b-753571079b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos la predicción con nuestro test y mostramos los 5 primeros resultados\n",
    "y_predc=knn.predict(X_testc)\n",
    "y_predc[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75314e42-f23a-49fe-b3f5-4c2325273db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos la precisión de nuestro modelo en los datos de test\n",
    "\n",
    "knn.score(X_testc, y_testc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ffad3-3cf7-4983-892e-86ad922bf6ad",
   "metadata": {},
   "source": [
    "Esta resultado de precisión ha sido obtenido usando una porción concreta de nuestro data set, pero una representación más adecuada de la precisión de nuestro modelo requeriría hacer una validación cruzada. Esto es repetir el mismo proceso pero cada vez seleccionando una porción distinta de dataset como test y luego haciendo la media de las evaluaciones de predicción. \n",
    "\n",
    "Lo que obtengamos como media será una representación más fiable de cómo se comportará nuestro modelo con nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a46bff-7a59-4b11-a31a-fdb2b752cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "#create a new KNN model\n",
    "knn_cv = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#Entrenando el modelo con una validación cruzada de 5 \n",
    "cv_scores = cross_val_score(knn_cv, Xc, yc, cv=5)\n",
    "\n",
    "#Mostramos cada una de las evaluaciones de precisión y la media de todas\n",
    "print(cv_scores)\n",
    "print('cv_scores mean:{}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6871e73-08c1-4255-812f-95193885937c",
   "metadata": {},
   "source": [
    "#### Evaluación de los modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed61d4-4997-43d0-9ab0-e36364d84119",
   "metadata": {},
   "source": [
    "Hay diferentes métricas para evaluar modelos de clasificación, pero aquí vamos a ver dos de ellos: Índice de Jaccard and F1-score.\n",
    "\n",
    "<ins>Índice de Jaccard:</ins> \n",
    "\n",
    "Jaccard es la intersección de los valores predecidos y los reales divididos por la union de los mismos.\n",
    "\n",
    "![Índice_Jaccard](./images/t9_index_jaccard.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b24e69-fcd7-44d3-9603-33d97c7b8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "y_true = np.array([[0, 1, 1],\n",
    "                   [1, 1, 0]])\n",
    "y_pred = np.array([[1, 1, 1],\n",
    "                   [1, 0, 0]])\n",
    "\n",
    "jaccard_score(y_true, y_pred, average=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeb096-dbb7-4b2d-92db-cc461db250cb",
   "metadata": {},
   "source": [
    "<ins>F1-score:</ins>\n",
    "\n",
    "Para calcular el F1-score es necesario saber las medidas de precisión y sensibilidad de un modelo.\n",
    "\n",
    "La exactitud (precision en inglés) es una medida de la precisión del modelo que se obtiene al predecir una clase que ya estaba etiquetada. Se define de la siguiente manera:\n",
    "\n",
    "$$Precision=\\frac{True Positive}{True Positive + False Positive}$$\n",
    "\n",
    "La Sensibilidad , exhaustividad o (*Recall*), también se conoce como Tasa de Verdaderos Positivos (True Positive Rate) (TP). Es la proporción de casos positivos que fueron correctamente identificadas por el algoritmo. Se calcula según la ecuación: \n",
    "$$Recall=\\frac{True Positive}{True Positive + False Negative}$$\n",
    "Ahora que sabemos estas dos medidas podemos calcular los pesos F1 (F1 scores) para cada etiqueta basados en su exactitud y sensibilidad.\n",
    "La puntuación F1 es el promedio armónico de exactitud y sensibilidad, donde una puntuación F1 alcanza su mejor valor en 1 (que representa exactitud y sensibilidad perfecta) y su peor valor en 0. Admite casos con más de dos clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5b03b-c724-49e0-827e-5f180d6959c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 0, 0, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1]\n",
    "\n",
    "metrics.f1_score(y_true, y_pred,average=None)\n",
    "\n",
    "#average{'micro', 'macro', 'samples','weighted', 'binary'} or None\n",
    "#determina el tipo de promedio al que se someten los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb409ab1-7d77-495b-817d-ad21a8332559",
   "metadata": {},
   "source": [
    "\n",
    "Internamente las f1-scores usan una matriz de confusión .\n",
    "\n",
    "Una matriz de confusion es herramienta muy útil para valorar cómo de bueno es un modelo clasificación basado en aprendizaje automático. En particular, sirve para mostrar de forma explícita y visual cuándo una clase es confundida con otra, lo cual nos, permite trabajar de forma separada con distintos tipos de error.\n",
    "\n",
    "Por ejemplo, podemos definir una función que nos calcule la matriz de confusión junto con los valores de precisión, sensibilidad y la puntuación f1. Es útil para saber cómo se comporta nuestro modelo y tenemos esa información recogida de manera muy visual. Se haría de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1830c0-49e4-4622-90ba-afafaa9c3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_resultados(y_test, y_pred):\n",
    "    conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\");\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "mostrar_resultados(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3ce4a-d1fb-4c00-8da7-49362b94b171",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e9_2:** \n",
    "\n",
    "Dado el dataset `clientes` que tiene a los clientes categorizados en distintos servicios, dependiendo de sus patrones de consumo y valores demográficos (custcat), ajustar los datos usando el modelo K-Nearest Neighbors para predecir en qué grupo categorizar a nuevos clientes.\n",
    "\n",
    "El ejemplo se centra en el uso de datos demográficos, como la región, la edad y el matrimonio, para predecir los patrones de uso.\n",
    "\n",
    "El campo de destino, denominado custcat, tiene cuatro valores posibles que corresponden a los cuatro grupos de clientes, de la siguiente manera: \n",
    "    \n",
    "1- Servicio básico \n",
    "    \n",
    "2- Servicio electrónico \n",
    "    \n",
    "3- Servicio Plus \n",
    "    \n",
    "4- Servicio total\n",
    "\n",
    "Contruir el modelo siguiendo los siguientes pasos:\n",
    "    \n",
    "- Dividir dataset en train y test\n",
    "- Definir modelo K-NN eligiendo el parámetro k\n",
    "- Ajustar datos\n",
    "- Evaluar usando una matriz de confusión y los F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea563e90-648c-48a0-8727-2043e01e1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clientes = pd.read_csv(\"./data/teleCust1000t.csv\")\n",
    "clientes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df11ff-2ad3-4b47-8cc6-acb86cd9bef8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Otras funcionalidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986fd79-8ac6-4feb-b2b9-776e2e83b29b",
   "metadata": {},
   "source": [
    "#### Tests estadísticos\n",
    "\n",
    "Una prueba estadística proporciona un mecanismo para tomar decisiones cuantitativas sobre un proceso o procesos. Es una forma de evaluar la evidencia que proporcionan los datos frente a una hipótesis.\n",
    "\n",
    "En el siguiente diagrama se puede ver qúe test estadístico usar dependiendo de los datos que tengamos aunque eso es decisión del científico. Aquí explicaremos cómo implementar algunos de ellos usando python.\n",
    "\n",
    "<center>\n",
    "<td> <img src=\"./images/t9_tests.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e687f8b4-1fe3-4971-b73c-889668a83fc4",
   "metadata": {},
   "source": [
    "Una prueba estadística examina dos hipótesis opuestas sobre una población: la hipótesis nula y la hipótesis alternativa.\n",
    "\n",
    "La **hipótesis nula** establece que un parámetro de población (como la media, la desviación estándar, etc.) es igual a un valor hipotético.\n",
    "La **hipótesis alternativa** establece que un parámetro de población es menor, mayor o diferente que el valor hipotetizado en la hipótesis nula.\n",
    "\n",
    "- <ins> Prueba T de **una** muestra </ins>\n",
    "La prueba t para una muestra determina si la media muestral es estadísticamente diferente de una media poblacional conocida o hipotética. La prueba t para una muestra es una prueba paramétrica. Se utiliza principalmente en conjunto de datos que seguirían una distribución normal y podrían tener varianzas desconocidas.\n",
    "Ejemplo: tiene 10 edades y está comprobando si la edad promedio es 30 o no.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c1acf-4e53-41ea-951a-3b0e9abca4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "ages_mean= np.mean(clientes.age)\n",
    "print(ages_mean)\n",
    "tset, pval = ttest_1samp(clientes.age, 30)\n",
    "print('p-values',pval)\n",
    "if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "    print(\"Rechazamos la hipótesis nula\")\n",
    "else:\n",
    "    print(\"Aceptamos la hipótesis nula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fc22b-c76c-4fb2-9d5a-a7603a252a02",
   "metadata": {},
   "source": [
    "- <ins> Prueba Z de **una** muestra </ins>\n",
    "\n",
    "La prueba Z de una muestra se utiliza para comparar la media poblacional de una variable categórica con una muestra. Se debe conocer la desviación estándar de la población para realizar la prueba Z y los datos deben estar distribuidos normalmente.\n",
    "\n",
    "Usaría una prueba Z si:\n",
    "- El tamaño de la muestra es superior a 30. De lo contrario, utilice una prueba t.\n",
    "- Los puntos de datos deben ser independientes entre sí. En otras palabras, un punto de datos no está relacionado o no afecta a otro punto de datos.\n",
    "- Sus datos deben distribuirse normalmente. Sin embargo, para tamaños de muestra grandes (más de 30) esto no siempre importa.\n",
    "- Sus datos deben seleccionarse al azar de una población, donde cada elemento tiene la misma probabilidad de ser seleccionado.\n",
    "- Los tamaños de las muestras deben ser iguales si es posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ac1f7-4dfe-4d39-92c9-9a6226d3fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import weightstats as stests\n",
    "\n",
    "blood = pd.read_csv(\"./data/blood_pressure.csv\")\n",
    "ztest ,pval = stests.ztest(blood['bp_before'], x2=None, value=156)\n",
    "print(float(pval))\n",
    "if pval<0.05:\n",
    "    print(\"Rechazamos la hipótesis nula\")\n",
    "else:\n",
    "    print(\"Aceptamos la hipótesis nula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9579fb-d4dc-4a1d-8c24-ba5bd296a87e",
   "metadata": {},
   "source": [
    "- <ins> ANNOVA (F-test) </ins>\n",
    "La prueba t funciona bien cuando se trata de dos grupos, pero a veces queremos comparar más de dos grupos al mismo tiempo. Por ejemplo, si quisiéramos probar si la edad de los votantes difiere en función de alguna variable categórica como la raza, tenemos que comparar las medias de cada nivel o agrupar la variable. Podríamos realizar una prueba t separada para cada par de grupos, pero cuando realiza muchas pruebas aumenta las posibilidades de falsos positivos. El análisis de varianza o ANOVA es una prueba de inferencia estadística que le permite comparar varios grupos al mismo tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb13c3-8754-44a6-88eb-63d1ebe0b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anova = pd.read_csv('./data/PlantGrowth.csv')\n",
    "df_anova = df_anova[['weight','group']]\n",
    "grps = pd.unique(df_anova.group.values)\n",
    "d_data = {grp:df_anova['weight'][df_anova.group == grp] for grp in grps}\n",
    " \n",
    "F, p = stats.f_oneway(d_data['ctrl'], d_data['trt1'], d_data['trt2'])\n",
    "print(\"p-value for significance is: \", p)\n",
    "if p<0.05:\n",
    "    print(\"Rechazamos la hipótesis nula\")\n",
    "else:\n",
    "    print(\"Aceptamos la hipótesis nula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b93a3-8d0a-470f-8718-68328d0d1fef",
   "metadata": {},
   "source": [
    "- <ins> Chi cuadrado </ins>\n",
    "\n",
    "Se utiliza para decidir si existe una relación entre dos variables de una población. Útil al analizar los resultados de la encuesta de 2 variables categóricas.\n",
    "H₀: Las dos variables categóricas no tienen relación\n",
    "H₁: Existe una relación entre dos variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bf6f6-178f-49e6-a14e-fa1d76f3ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data according to survey\n",
    "data = [['18-29', 'Conservative'] for i in range(141)] + \\\n",
    "        [['18-29', 'Socialist'] for i in range(68)] + \\\n",
    "        [['18-29', 'Other'] for i in range(4)] + \\\n",
    "        [['30-44', 'Conservative'] for i in range(179)] + \\\n",
    "        [['30-44', 'Socialist'] for i in range(159)] + \\\n",
    "        [['30-44', 'Other'] for i in range(7)] + \\\n",
    "        [['45-65', 'Conservative'] for i in range(220)] + \\\n",
    "        [['45-65', 'Socialist'] for i in range(216)] + \\\n",
    "        [['45-65', 'Other'] for i in range(4)] + \\\n",
    "        [['65 & older', 'Conservative'] for i in range(86)] + \\\n",
    "        [['65 & older', 'Socialist'] for i in range(101)] + \\\n",
    "        [['65 & older', 'Other'] for i in range(4)]\n",
    "df = pd.DataFrame(data, columns = ['Age Group', 'Political Affiliation']) \n",
    "\n",
    "# create contingency table\n",
    "data_crosstab = pd.crosstab(df['Age Group'],\n",
    "                            df['Political Affiliation'],\n",
    "                           margins=True, margins_name=\"Total\")\n",
    "\n",
    "# significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Calcualtion of Chisquare test statistics\n",
    "chi_square = 0\n",
    "rows = df['Age Group'].unique()\n",
    "columns = df['Political Affiliation'].unique()\n",
    "for i in columns:\n",
    "    for j in rows:\n",
    "        O = data_crosstab[i][j]\n",
    "        E = data_crosstab[i]['Total'] * data_crosstab['Total'][j] / data_crosstab['Total']['Total']\n",
    "        chi_square += (O-E)**2/E\n",
    "\n",
    "# The p-value approach\n",
    "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
    "p_value = 1 - stats.norm.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
    "conclusion = \"Failed to reject the null hypothesis.\"\n",
    "if p_value <= alpha:\n",
    "    conclusion = \"Null Hypothesis is rejected.\"\n",
    "        \n",
    "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
    "print(conclusion)\n",
    "    \n",
    "# The critical value approach\n",
    "print(\"\\n--------------------------------------------------------------------------------------\")\n",
    "print(\"Approach 2: The critical value approach to hypothesis testing in the decision rule\")\n",
    "critical_value = stats.chi2.ppf(1-alpha, (len(rows)-1)*(len(columns)-1))\n",
    "conclusion = \"Failed to reject the null hypothesis.\"\n",
    "if chi_square > critical_value:\n",
    "    conclusion = \"Null Hypothesis is rejected.\"\n",
    "        \n",
    "print(\"chisquare-score is:\", chi_square, \" and p value is:\", critical_value)\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e83aa-2885-4a6c-800e-704b19273aac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Underfitting/Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4ae92-564e-4b1c-8289-8888ad8d78ad",
   "metadata": {},
   "source": [
    "Cuando entrenamos nuestros modelos computacionales con un conjunto de datos de entrada estamos haciendo que el algoritmo sea capaz de generalizar un concepto para que al consultarle por un nuevo conjunto de  datos desconocido éste sea capaz de sintetizarlo, comprenderlo y devolvernos un resultado fiable dada su capacidad de generalización.\n",
    "\n",
    "Intentamos “hacer encajar” -fit en inglés- los datos de entrada entre ellos y con la salida. Tal vez se pueda traducir overfitting como “sobreajuste” y underfitting  como “subajuste” y hacen referencia al fallo de nuestro modelo al generalizar -encajar- el conocimiento que pretendemos que adquieran.\n",
    "\n",
    "Si nuestros datos de entrenamiento son muy pocos nuestra máquina no será capaz de generalizar el conocimiento y estará incurriendo en **underfitting**. Este es el caso en el que le enseñamos sólo una raza de perros y  pretendemos que pueda reconocer a otras 10 razas de perros distintas. También es ejemplo de “subajuste” cuando la máquina reconoce todo lo que “ve” como un perro, tanto una foto de un gato o un coche.\n",
    "Si en el conjunto de Test sólo se acierta un tipo de clase (por ejemplo “peras”) o el único resultado que se obtiene es siempre el mismo valor será que se produjo un problema de underfitting.\n",
    "\n",
    "Se da **overfitting** cuando nuestra máquina sólo se ajuste a aprender los casos particulares que le enseñamos y sea incapaz de reconocer nuevos datos de entrada. En nuestro conjunto de datos de entrada muchas veces introducimos muestras anómalas o con “ruido/distorsión” en alguna de sus dimensiones, o muestras que pueden no ser del todo representativas. Cuando “sobre-entrenamos” nuestro modelo y caemos en el overfitting, nuestro algoritmo estará considerando como válidos sólo los datos idénticos a los de nuestro conjunto de entrenamiento –incluidos sus defectos– y siendo incapaz de distinguir entradas buenas como fiables si se salen un poco de los rangos ya prestablecidos.\n",
    "Si el modelo entrenado con el conjunto de train tiene un 90% de aciertos y con el conjunto de test tiene un porcentaje muy bajo, esto señala claramente un problema de overfitting.\n",
    "\n",
    "Regresión Lineal\n",
    "\n",
    "\n",
    "![Underfitting_Overfitting](./images/t9_under_over.JPG)\n",
    "\n",
    "\n",
    "Clasificación\n",
    "\n",
    "\n",
    "![Knn_Underfitting_Overfitting](./images/t9_knn_under_over.png)\n",
    "\n",
    "\n",
    "Para reconocer este problema deberemos subvididir nuestro conjunto de datos de entrada para entrenamiento en dos: uno para entrenamiento y otro para la Test que el modelo no conocerá de antemano. Esta división se suele hacer del 80% para entrenar y 20%.\n",
    "\n",
    "Para lograr que nuestro modelo dé buenos resultados iremos revisando y contrastando nuestro entrenamiento con el conjunto de Test y su tasa de errores, utilizando más o menos iteraciones, etc. hasta dar con buenas predicciones y sin tener los problemas de over-under-fitting.\n",
    "\n",
    "Para intentar que estos problemas nos afecten lo menos posible, podemos llevar a cabo diversas acciones:\n",
    "\n",
    "- Cantidad mínima de muestras tanto para entrenar el modelo como para validarlo.\n",
    "- Clases variadas y equilibradas en cantidad, datos de entrenamiento estén balanceados.\n",
    "- Conjunto de Test de datos. Siempre subdividir nuestro conjunto de datos y mantener una porción del mismo “oculto” a nuestra máquina entrenada. Esto nos permitirá obtener una valoración de aciertos/fallos real del modelo y también nos permitirá detectar fácilmente efectos del overfitting /underfitting.\n",
    "- Ajuste de Parámetros: deberemos experimentar sobre todo dando más/menos “tiempo/iteraciones” al entrenamiento y su aprendizaje hasta encontrar el equilibrio.\n",
    "-  A veces conviene reducir la cantidad de características que implementamos en el modelo el modelo. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556dce2-b970-4fd5-9851-9d0e63e22404",
   "metadata": {},
   "source": [
    "#### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c9b19-1cc9-45be-99ec-c0490200d5e1",
   "metadata": {},
   "source": [
    "Los hiperparámetros de un modelo son los valores de las configuraciones utilizadas durante el proceso de entrenamiento. Son valores que generalmente no se obtienen de los datos, por lo que suelen ser indicados por el científico de datos.\n",
    "\n",
    "El valor óptimo de un hiperparámetro no se puede conocer a priori para un problema dado. Por lo que se tiene que utilizar reglas probabilísticas, los valores que han funcionado anteriormente en problemas similares o buscar la mejor opción mediante prueba y error. Siendo una buena opción buscar los hiperparámetros por la validación cruzada.\n",
    "\n",
    "Al entrenar un modelo de aprendizaje automático se fijan los valores de los hiperparámetros para que con estos se obtengan los parámetros. Algunos ejemplos de hiperparámetros utilizados para entrenar los modelos son:\n",
    "\n",
    "- La ratio de aprendizaje en el algoritmo del descenso del gradiente.\n",
    "- El número de vecinos en k-vecinos más cercanos (k-nn).\n",
    "- La profundidad máxima en un árbol de decisión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f658d-e26e-4afb-a729-f8e16f8f716f",
   "metadata": {},
   "source": [
    "## Aplicación Machine Learning con Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc9c04-9687-4114-baa5-1645290a30a9",
   "metadata": {},
   "source": [
    "Además de la teoría, necesaria para saber que tipo de problema tengo entre manos y que estrategias hay para su resolución tenemos que saber que Python ofrece una serie de librerías."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c4dc8-7b40-4d3f-a689-bad33683ec4b",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a726f-4537-4d20-8562-9791594751b9",
   "metadata": {},
   "source": [
    "- <ins> Machine Learning </ins>\n",
    "\n",
    "    Hemos visto en el capítulo anterior que construir un modelo de Machine Learning consume relativamente poco tiempo. Esto es así porque ya existen librerías de aprendizaje automático. Aunque hay varias, Scikit-Learn es la más utilizada.\n",
    "\n",
    "    **Scikit-learn** es una librería de python para Machine Learning y Análisis de Datos. Está basada en NumPy, SciPy y Matplotlib. La ventajas principales de scikit-learn son su facilidad de uso y la gran cantidad de técnicas de aprendizaje automático que implementa.\n",
    "\n",
    "    Con scikit-learn podemos realizar aprendizaje supervisado y no supervisado. Podemos usarlo para resolver problemas tanto de clasificación y como de regresión.\n",
    "\n",
    "    Es muy fácil de usar porque tiene una interfaz simple y muy consistente. El interfaz es muy fácil de aprender. Te das cuenta que el interfaz es consistente cuando puedes cambiar de técnica de machine learning cambiando sólo una línea de código.\n",
    "    Estas son algunas de las técnicas de aprendizaje automático que podemos usar con scikit-learn:\n",
    "\n",
    "    - regresión lineal y polinómica\n",
    "    - regresión logística\n",
    "    - máquinas de vectores de soporte\n",
    "    - árboles de decisión\n",
    "    - bosques aleatorios (random forests)\n",
    "    - agrupamiento (clustering)\n",
    "    - basados en instancias\n",
    "    - clasificadores bayesianos\n",
    "    - reducción de dimensionalidad\n",
    "    - detección de anomalías\n",
    "    - etc.\n",
    "    \n",
    "    **Imbalance Learning** es una librería que sirve para balancear datos. Otro caso que se da con mucha frecuencia al hacer tareas de clasificación es contar con una cantidad desbalanceada de muestras de cada clases. Casos típicos son la detección de alguna enfermedad en donde la mayoría de las muestras son negativas y pocas positivas o en set de datos para detección de fraudes.\n",
    "\n",
    "    Para que un algoritmo supervisado de ML pueda aprender, deberá poder generalizar el conocimiento y para ello, deberá de ver cantidad suficiente de muestras de cada clase ó será incapaz de discernir.\n",
    "\n",
    "    Allí aparece esta librería al rescate con diversos algoritmos para el re-muestreo de nuestra información. Con ello podremos disminuir al conjunto mayoritario (sin afectar al resultado del entrenamiento), aumentar al conjunto minoritario (creando muestras artificiales “con sentido”) ó aplicar técnicas  combinadas de oversampling y subsampling a la vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d7d08-f5e9-425f-94c1-32e97f432367",
   "metadata": {},
   "source": [
    "- <ins> Deep Learning </ins>\n",
    "\n",
    "    **TensorFlow** es una librería de python, desarrollada por Google, para realizar cálculos numéricos mediante diagramas de flujo de datos. Esto puede chocar un poco al principio, porque en vez de codificar un programa, codificaremos un grafo. Los nodos de este grafo serán operaciones matemáticas y las aristas representan los tensores (matrices de datos multidimensionales).\n",
    "\n",
    "    Con esta computación basada en grafos, TensorFlow puede usarse para deep learning y otras aplicaciones de cálculo científico.\n",
    "\n",
    "    **Keras** es un interfaz de alto nivel para trabajar con redes neuronales. El interfaz de Keras es mucho más fácil de usar que el de TensorFlow. Esta facilidad de uso es su principal característica.\n",
    "\n",
    "    Con Keras es muy fácil comprobar si nuestras ideas tendrán buenos resultados rápidamente. Entre las características de Keras destacan su interfaz simple y optimizada para casos comunes, la modularidad mediante el uso de bloques y la facilidad de adaptar estos bloques para aplicar nuevos descubrimientos estado del arte.\n",
    "    Keras utiliza otras librerías de deep learning (TensorFlow, CNTK o Theano) de forma transparente para hacer el trabajo que le digamos.\n",
    "\n",
    "    **Pytorch** es una biblioteca de Python desarrollada por Facebook que permite realizar cálculos numéricos eficientes en la CPU y la GPU.\n",
    "\n",
    "    Piensa en PyTorch como una biblioteca que demuestra el poder de NumPy en una GPU. Esto significa que si tu tarjeta gráfica tiene un procesador gráfico (como el último NVIDIA), tu código puede funcionar entre 10 y 20 veces más rápido.\n",
    "\n",
    "    El aprendizaje profundo utiliza la matriz y la computación diferencial paralela masiva en las GPU. Por lo tanto, PyTorch también se especializa en el aprendizaje profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6005c4-278d-4444-a2ca-18cc782bad19",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Referencias:\n",
    "- Sklearn Preprocessing. https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Probability distributions. https://relopezbriega.github.io/blog/2016/06/29/distribuciones-de-probabilidad-con-python/\n",
    "- Validación de datasets. Dividir datos en train/test. https://www.cienciadedatos.net/documentos/30_cross-validation_oneleaveout_bootstrap \n",
    "- Árboles de decisión. https://www.cienciadedatos.net/documentos/py07_arboles_decision_python.html\n",
    "- Tests estadísticos. https://manualestutor.com/ciencia-de-datos/pruebas-estadisticas-con-python/\n",
    "- Test estadísticos2. https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
