{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52eb5e08-ef6d-4663-97b6-f29b7e229ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=6>\n",
    "\n",
    "<b>Curso de Análisis de Datos con Python</b>\n",
    "</font>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "Curso de formación interna, CIEMAT. <br/>\n",
    "Madrid, Junio de 2023\n",
    "\n",
    "Antonio Delgado Peris (Cristina Labajo Villaverde)\n",
    "</font>\n",
    "\n",
    "https://github.com/andelpe/curso-python-analisis-datos\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a947c",
   "metadata": {},
   "source": [
    "# Tema 9. Análisis estadístico y Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d1571-d65a-46e5-876d-3b5902efce0e",
   "metadata": {},
   "source": [
    "El aprendizaje automático o *Machine Learning* (ML) es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos masivos y elaborar predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma, es decir, sin necesidad de ser programados, gracias a su capacidad de analizar los datos, generalizar y asociar.\n",
    "\n",
    "El machine learning contribuye a buscar soluciones a problemas muy variados repartidos en distintos campos. Destacan:\n",
    "\n",
    "- **Sistemas de recomendación**: \n",
    "Se usa en plataformas online para ofrecer productos basados en anteriores compras, o por supermercados para hacer ofertas personalizadas a sus clientes. Otras plataformas, como Netflix o Spotify lo usan para recomendarte canciones, películas y series que se acomoden a tus gustos. La manera de obtener estos resultados es comparando las consumiciones de un individuo con las que han hecho otras personas de gustos similares. \n",
    "\n",
    "<br/>\n",
    "\n",
    "![ML_Recomendación_Netflix](images/t10_netflix.png)\n",
    "  \n",
    "<br/>\n",
    "\n",
    "\n",
    "- **Ciberseguridad**: \n",
    "El machine learning ayuda a los nuevos antivirus a escanear de manera más eficiente, aumentando la velocidad de detección y reconociendo anomalías.\n",
    "\n",
    "- **Automoción**:\n",
    "Los coches inteligentes son capaces de aprender automáticamente y ofrecer asistencia a la conducción, o, en un futuro, conducción autónoma (sin conductor).\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "- **Procesamiento de Lenguaje Natural (PLN)**: \n",
    "Es el algoritmo que permite a un ordenador (p. ej. a *Siri*) entender lo que se le pide y hacer búsquedas satisfactorias, comprendiendo el lenguaje humano.\n",
    "\n",
    "- **Medicina**: \n",
    "En el campo de la medicina el ML tiene muchas aplicaciones y en los años venideros será de vital importancia. Ya se usa para detectar el cáncer de mama con mayor antelación, o diferenciar imágenes de rayos X con neumonía. También permitirá estandarizar muchos procesos que en la actualidad dependen de la pericia del médico observador.\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/t10_cancer_mama.png\" width=\"400\"/>\n",
    "  \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71a20e-f318-439b-9227-c043d5f29d79",
   "metadata": {},
   "source": [
    "## Categorías\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff081d-9030-49ae-915c-aeb130276f3a",
   "metadata": {},
   "source": [
    "Las dos categorías más comunes dentro del ML son el aprendizaje supervisado y el aprendizaje no supervisado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53778a55-6e5a-40c6-aa43-996c1e9194e0",
   "metadata": {},
   "source": [
    "### Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977830ef-cb67-43a6-b8c8-973d7076b718",
   "metadata": {},
   "source": [
    "Aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones,por ejemplo imágenes de perros y gatos. Gracias al ML un ordenador sería capaz de identificar nuevas fotos, aunque para esto es necesario que seres humanos coloquen estas etiquetas con anterioridad para asegurar la efectividad y calidad de los datos.\n",
    "\n",
    "La idea es que las computadoras aprendan de una multitud de ejemplos, y a partir de ahí puedan hacer el resto de cálculos necesarios para que nosotros no tengamos que volver a ingresar ninguna información.\n",
    "\n",
    "A su vez dentro de la categoría de aprendizaje supervisado podemos enfrentarnos a dos tipos de problemas:\n",
    "\n",
    "- Regresores\n",
    "\n",
    "    Los regresores son los problemas que intentan predecir un valor continuo, o un conjunto de valores, a partir de unos valores de entrada o *inputs* previos. Por ejemplo: Tratar de predecir la cantidad de emisión de CO2 de un motor de coche dadas otras características de este, como el número de cilindrada. \n",
    "\n",
    "  - Las series temporales son un tipo de predictores específico.\n",
    "\n",
    "\n",
    "- Clasificadores \n",
    "\n",
    "    Son los problemas en los que, recibiendo como entrada cierta información de un objeto, es necesario indicar la categoría o clase a la que pertenece de entre un número acotado de clases posibles. Por ejemplo, podemos clasificar e-mails en spam o no. O podemos recibir como entrada la imagen de un animal y decir si se trata de un gato, un perro o un loro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60fad2d-c740-4dcf-9d9a-e5dbe14ee473",
   "metadata": {},
   "source": [
    "Estos dos grupos de problemas se pueden resolver usando la estadística mezclada con la inteligencia artificial. Como ejemplos, podemos citar *Árboles de decisión* y *Redes Neuronales*, modelos de aprendizaje automático complejos que solucionan los tipos de problemas antes mencionados. No profundizaremos en ellos.\n",
    "\n",
    "- Árboles de decisión (regresores o clasificadores):\n",
    "\n",
    "Los algoritmos de aprendizaje basados en árboles potencian los modelos predictivos con alta precisión, estabilidad y facilidad de interpretación y mapean bastante bien las relaciones no lineales. Son adaptables para resolver cualquier tipo de problema (clasificación o regresión).\n",
    "\n",
    "\n",
    "<img src=\"images/t9_DT.jpg\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Los árboles de decisión se construyen dividiendo el conjunto de entrenamiento en distintos nodos, donde un nodo contiene toda o la mayor parte de una categoría de datos. Se trata de probar un atributo y ramificar los casos según el resultado de la prueba. Cada nodo interno corresponde a una prueba, y cada rama corresponde a un resultado de la prueba, y cada nodo hoja asigna un elemento a una clase.\n",
    "\n",
    "Un aspecto esencial de los algoritmos basado en árboles de decisión es la elección del mejor criterio para la división de los datos.\n",
    "\n",
    "Se pueden construir considerando los atributos uno por uno. Primero, se elije un atributo del conjunto de datos. Se calcula la importancia del atributo en la división de los datos. Después, se dividen los datos según el valor del mejor atributo, luego hay que ir a cada rama y repetir el proceso para el resto de los atributos. \n",
    "\n",
    "Después de construir el árbol, este se puede usar para predecir la clase de casos desconocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933f020-0032-4ab8-8594-8de910f3265d",
   "metadata": {},
   "source": [
    "\n",
    "- Redes neuronales\n",
    "\n",
    "Las redes neuronales artificiales son un modelo computacional _inspirado_ en los sistemas neuronales humanos. Consiste en un conjunto de unidades, llamadas neuronas artificiales, conectadas entre sí. La información de entrada atraviesa la red neuronal (donde se somete a diversas operaciones) produciendo unos valores de salida.\n",
    "\n",
    "Los modelos de redes neuronales son capaces de encontrar relaciones (patrones) de forma inductiva por medio de los algoritmos de aprendizaje basado en los datos existentes.\n",
    "\n",
    "<img src=\"images/t9_red_neuronal.jpg\" width=\"450\"/>\n",
    "\n",
    "Cada neurona está conectada con otras a través de unos enlaces. En estos enlaces el valor de salida de la neurona anterior es multiplicado por un valor de peso. Estos pesos en los enlaces pueden incrementar o inhibir el estado de activación de las neuronas adyacentes. Del mismo modo, a la salida de la neurona, puede existir una función limitadora o umbral, que modifica el valor resultado o impone un límite que no se debe sobrepasar antes de propagarse a otra neurona. Esta función se conoce como función de activación.\n",
    "\n",
    "Estos sistemas aprenden y se forman a sí mismos, en lugar de ser programados de forma explícita, y sobresalen en áreas donde la detección de soluciones o características es difícil de expresar con un modelado explícito.\n",
    "\n",
    "Para realizar este aprendizaje automático, normalmente, se intenta minimizar una función de pérdida que evalúa la red en su total. Los valores de los pesos de las neuronas se van actualizando buscando reducir el valor de la función de pérdida. Este proceso se realiza mediante la propagación hacia atrás."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568a51e-1246-4d2b-b492-8a1ee4e6fa31",
   "metadata": {},
   "source": [
    "### Aprendizaje no supervisado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3583f-6f3e-45a7-8242-11c06215f6dc",
   "metadata": {},
   "source": [
    "Estos algoritmos no cuentan con un conocimiento previo. Se enfrentan al caos de datos con el objetivo de encontrar patrones que permitan organizarlos de alguna manera para ver si encuentra soluciones que el ojo humano no pueda ver. \n",
    "\n",
    "Una de las técnicas más populares pertenecientes al aprendizaje no supervisado es el *clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dbb20-61aa-48ab-90ce-c8123aaf7ebf",
   "metadata": {},
   "source": [
    "- Clustering\n",
    "\n",
    "<img src=\"images/t10_clustering.png\" width=\"700\"/>\n",
    "\n",
    "El clustering es una tarea que tiene como finalidad principal lograr el agrupamiento de conjuntos de objetos no etiquetados, para lograr construir subconjuntos de datos conocidos como *clusters*. Cada cluster independiente está formado por una colección de objetos o datos que a términos de análisis resultan similares entre si, pero que poseen elementos diferenciales con respecto a otros objetos pertenecientes al conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5545d-fbd4-40b1-828c-15530a1a723d",
   "metadata": {},
   "source": [
    "## Machine Learning con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07633a-9b7d-43b6-b1ae-0958bc84d546",
   "metadata": {},
   "source": [
    "Python ofrece una serie de librerías para trabajar con algoritmos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a8e45-49c5-4d69-afab-1097b469fb69",
   "metadata": {},
   "source": [
    "#### Generales\n",
    "\n",
    "**Scikit-learn** es una librería de python para algoritmos clásicos de Machine Learning y Análisis de Datos. Está muy bien integrada con NumPy, SciPy y Matplotlib. La ventajas principales de scikit-learn son su flexibilidad, facilidad de uso y la gran cantidad de técnicas de aprendizaje automático que implementa.\n",
    "\n",
    "Con scikit-learn podemos realizar aprendizaje supervisado y no supervisado. Podemos usarlo para resolver problemas tanto de clasificación y como de regresión.\n",
    "\n",
    "Ofrece un interfaz simple y muy consistente, lo que facilita su aprendizaje y utilización con diferentes técnicas o algoritmos.\n",
    "    \n",
    "Estas son algunas de las técnicas de aprendizaje automático que podemos usar con scikit-learn:\n",
    "\n",
    "- Regresión lineal y polinómica\n",
    "- Regresión logística\n",
    "- Máquinas de vectores de soporte (_support vector machines_ (SVM)\n",
    "- Árboles de decisión\n",
    "- Bosques aleatorios (_random forests_)\n",
    "- Agrupamiento (_clustering_)\n",
    "- Clasificadores bayesianos\n",
    "- Reducción de dimensionalidad\n",
    "- Detección de anomalías\n",
    "    \n",
    "    \n",
    "**Imbalance Learning** es una librería que sirve para tratos con datos que no están bien _balanceados_, es decir que tenemos muchos datos de algunas clases, pero pocos de otros (p.ej. muchos ejemplos \"negativos\" y solo unos pocos \"positivos\"). En general, esto dificulta que nuestro modelo pueda generalizar bien y detectar correctamente ambos casos.\n",
    "\n",
    "Esta librería ayuda con el re-muestro de la información disponible, para intentar mejorar el equilibrio entre clases (p.ej. creando muestras artificiales, o aplicando over/subsampling).\n",
    "\n",
    "**Theano** puede usarse para definir, evaluar y optimizar expresiones matemáticas utilizando arrays multi-dimensionales de manera eficiente, incluyendo optimización para CPU y GPU. Es utilizada frecuentemente como base algoritmos complejos de machine learning.\n",
    "\n",
    "**Natural Language Toolkit (NLTK)**: librería para procesamiento del lenguaje natural. Ofrece interfaces simples y multitud de algoritmos para lexicográficos. Algunas de sus principales fortalezas son: búsqueda de palabras clave, tokenización y clasificación de textos, reconocimiento de voz, búsqueda de lemas y raíces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d7aca3-7c7d-4b4a-9b84-148db18ea7c7",
   "metadata": {},
   "source": [
    "#### Redes neuronales y Deep Learning\n",
    "\n",
    "**TensorFlow** es una librería de python, desarrollada por Google, optimizada para realizar cálculos numéricos con tensores (expresados en forma de diagramas de flujo), que se puede utilizar para diversas tareas de aprendizaje automático y científicas (procesamiento del lenguaje natural, resolución de ecuaciones diferenciales parciales, etc.), y, especialmente para la creación, el entranamiento y el uso de redes neuronales.\n",
    "\n",
    "**Keras** es un interfaz de alto nivel para trabajar con redes neuronales. El interfaz de Keras es mucho más fácil de usar que el de TensorFlow, lo que constituye su principal característica.\n",
    "\n",
    "Con Keras es muy fácil comprobar si nuestras ideas tendrán buenos resultados rápidamente. Entre las características de Keras destacan su interfaz simple y optimizada para casos comunes, la modularidad mediante el uso de bloques y la facilidad de adaptar estos bloques para aplicar nuevos descubrimientos estado del arte.\n",
    "    \n",
    "Keras es capaza de trabajar con diversas librerías de deep learning (TensorFlow, CNTK o Theano) de forma transparente para hacer el trabajo que le digamos.\n",
    "\n",
    "**Pytorch** librería desarrollada por Facebook, que permite realizar cálculos numéricos eficientes en la CPU y la GPU. De nuevo, puede ser utilizada para acelerar el procesamiento necesario para diversas aplicaciones, incluido procesamiento de lenguaje natural, o visión por computador, y, muy especialmente, redes neuronales para deep learning.\n",
    "\n",
    "**MXnet** es un framework para deep learning, de Apache. Se le considera escalable, para conseguir entrenamientos rápidos, y flexible para utilizar diferentes modelos y lenguajes de programación (incluido Python). Es capaz de aprovechar varios ordenadores o GPUs disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6732ca",
   "metadata": {},
   "source": [
    "### Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e63f43",
   "metadata": {},
   "source": [
    "La regresión es el proceso de predecir un valor continuo. En regresión hay dos tipos de valores:\n",
    "\n",
    "- La **variable dependiente** (y) es aquella que queremos averiguar y predecir, lo que buscamos hallar.\n",
    "\n",
    "- Las **variables independientes** (x) se pueden ver como las causas de las que depende la variable dependiente. Pueden ser valores continuos o discretos, numéricos o categóricos. \n",
    "\n",
    "Nuestro modelo de regresión relacionará la variable dependiente con la variable o variables independientes. Construimos nuestro modelo de regresión usando los datos de los que disponemos y después de entrenarlo nuestro modelo será capaz de predecir el valor de la variable dependiente de un nuevo evento.\n",
    "\n",
    "Dentro de la regresión podemos encontrar dos tipos:\n",
    "- Regresión simple:\n",
    "Es cuando una sóla variable independiente se usa para estimar la variable dependiente. A su vez, la regresión simple puede ser *lineal* o *no lineal*, según la naturaleza de la relación entre variables dependientes e independientes.\n",
    "\n",
    "- Regresión múltiple:\n",
    "La regresión es múltiple cuando hay más de una variable independiente y, como en el caso de la regresión simple puede ser lineal o no lineal.\n",
    "\n",
    "Existen muchos algoritmos diferentes usados para crear modelos de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602373d7-cbd0-4a31-b1a0-ed40a5e68444",
   "metadata": {},
   "source": [
    "## La librería Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15498d8-a2ea-4f58-b51a-958e202dfd5e",
   "metadata": {},
   "source": [
    "Para un análisis estadístico complejo o para trabajar con Machine Learning (ML),  **Scikit Learn** (sklearn) es una librería muy utilizada, pues pose muchas herramientas útiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1aade6-f566-464a-93d8-c7e363c11188",
   "metadata": {},
   "source": [
    "Características de Scikit Learn: \n",
    "\n",
    "- Librería gratuita de análisis estadístico y Machine Learning para Python\n",
    "- Incluye muchos los algoritmos de regresión, clasificación y clustering más populares\n",
    "- Diseñada para trabajar con las librerías numéricas, científicas y de datos (Numpy, Scipy y Pandas)\n",
    "- Bien documentada con gran cantidad de ejemplos\n",
    "\n",
    "<img src=\"images/t9_pipeline.jpg\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd055fe-73d4-4ea6-87d7-099fbca11142",
   "metadata": {},
   "source": [
    "### Preprocesado de datos con Sklearn\n",
    "Aunque ya hemos visto como limpiar y preprocesar un dataset usando Pandas, Sklearn también ofrece utilidades para muchas de estas tareas.\n",
    "\n",
    "Conviene comentar antes de nada la estructura de los datos de entrada que suelen usarse con Sklearn. En general, nos encontramos con una serie de muestras (`samples`), compuestas, cada una, por una serie de variables o características(`features`). Se suelen organizar en una matriz (`X`), donde cada fila representa una muestra, y cada columna es una característica. `X` será bien un array NumPy bidimensional, bien un Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62741818-b233-4318-8265-56defe9e68ba",
   "metadata": {},
   "source": [
    "#### Escalado de datos\n",
    "Muchos algoritmos de ML requieren que los datos se escalen de cierta manera. A continuación mostramos varios métodos de escalado diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbbb7f-8e68-45d7-96b5-9713e42e23d8",
   "metadata": {},
   "source": [
    "<ins>**Estandarización**</ins>\n",
    "\n",
    "La estandarización es un escalado de los vectores de características (columnas) para conseguir un valor medio 0 y una desviación estándar de 1. Utiliza el _Z-score_ que vimos anteriormente.\n",
    "\n",
    "El módulo `preprocessing` de la librería Sklearn contiene la clase `StandardScaler` aplicable a datos dados en la forma *array*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f8553-c41f-421e-9bf3-c7c90a4d6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.model_selection as skmodels\n",
    "import sklearn.linear_model as sklinear\n",
    "import sklearn.preprocessing as skpreprocess\n",
    "import sklearn.neighbors as skneighbors\n",
    "import sklearn.datasets as skdatasets\n",
    "import sklearn.pipeline as skpipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d480dca-1278-481e-9c88-632531e1ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "             [ 2.,  0.,  0.],\n",
    "             [ 0.,  1., -1.]])\n",
    "scaler = skpreprocess.StandardScaler()\n",
    "sc = scaler.fit(X)\n",
    "print(sc.mean_)\n",
    "sc.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf66683-abaa-4b02-b867-8f989065f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da9c41-a833-427e-a1b1-7008a3cf5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Medias de cada columna:')\n",
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637324dc-4e0a-42e2-8519-92add7c3139e",
   "metadata": {},
   "source": [
    "<p/>\n",
    "\n",
    "<ins>**Escalado MinMax**</ins>\n",
    "\n",
    "Un escalado alternativo sería ajustar las `features` para que su rango vaya del mínimo al máximo, siendo estos 0 y 1.\n",
    "\n",
    "Para ello se puede usar la clase `MinMaxScaler` del mismo módulo que antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81281ec-576f-4477-8bea-dde4e29ebb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = skpreprocess.MinMaxScaler()\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "X_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c1f8d-5ad2-4754-a568-0a13566cb112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mínimo y máximo de cada columna:')\n",
    "print(X_minmax.min(axis=0))\n",
    "print(X_minmax.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab3e95-e38a-47ac-8c5c-4caff25dadc8",
   "metadata": {},
   "source": [
    "<p/>\n",
    "\n",
    "<ins>**Normalización**</ins>\n",
    "\n",
    "En este escalado se ajustan los valores de las muestras (filas), para que cada una tenga una norma igual a 1 (se suelen usar normas _l1_, _l2_, o _max_)).\n",
    "\n",
    "Lo hacemos con la función `normalize` de nuevo del módulo _preprocessing_. En este ejemplo usamos norma _l2_ (euclídea).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f466a-c853-42f0-80fe-cc58430182bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "X_normalized = skpreprocess.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e51009-3992-4f7e-94ec-dfcbb908e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Norma de cada fila:')\n",
    "np.sum(np.power(X_normalized, 2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dc576-412e-47be-afe9-2b568a56eda4",
   "metadata": {},
   "source": [
    "#### Conversión de variables categóricas a numéricas\n",
    "\n",
    "\n",
    "Para convertir variables categóricas a números enteros con sklearn, podemos usar `OrdinalEncoder`. Este estimador transforma cada variable categórica en una nueva variable de enteros (entre `0` y `n - 1`, donde `n` es el número de variables diferentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a6e89-d82b-4b21-8425-c2a87103970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = skpreprocess.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "print('Categories:')\n",
    "for cat in enc.categories_:\n",
    "    print('-- values:', cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b8fbb-1bce-48aa-8497-826a885bd592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación\n",
    "enc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6383227-c420-4c58-b37c-87822a8a1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifiquemos una muestra concreta\n",
    "enc.transform([['female', 'from US', 'uses Safari']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd6e5e-ad3b-4a78-91cf-74b55da4e114",
   "metadata": {},
   "source": [
    "También podemos usar la transformación usando `OneHotEncoder`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a047a8-047a-4194-ab85-61c789bc120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = skpreprocess.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "print('Categories:')\n",
    "for cat in enc.categories_:\n",
    "    print('-- values:', cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b12364-d39e-400a-b9b1-fb9be649d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f640c-322a-4c20-8ced-46b6421aa35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([['female', 'from US', 'uses Safari']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab962c8-ccd3-4d13-a15d-975540237bc3",
   "metadata": {},
   "source": [
    "#### Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45c0a9-6aa7-4179-8ad3-d5594c9791aa",
   "metadata": {},
   "source": [
    "En aprendizaje automático la creación de un modelo es un proceso complejo que requiere llevar a cabo múltiples pasos: limpieza de los datos, escalado, etc. La automatización de estos procesos puedo suponer un aumento considerable de la productividad.\n",
    "\n",
    "La librería Sklearn permite encadenar las transformaciones que se van a realizar a los datos, en lo que se llaman rutinas de trabajo o tuberias (_pipelines_). Estas pipelines se pueden utilizar posteriormente como si fuesen un estimador más.\n",
    "\n",
    "A continuación tenemos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f48d1-430e-4cf5-b210-74d57aac7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = skdatasets.make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = skmodels.train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Se define el pipeline\n",
    "pipe = skpipeline.make_pipeline(skpreprocess.StandardScaler(), sklinear.LogisticRegression())\n",
    "\n",
    "# Apply scaling on training data\n",
    "p = pipe.fit(X_train, y_train)  \n",
    "\n",
    "# Apply scaling on testing data, without leaking training data.\n",
    "pipe.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6268a-ee3b-46ae-a12b-9a5d72d825cd",
   "metadata": {},
   "source": [
    "### Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c401d-433d-4199-b5c1-dbdc12cf6d45",
   "metadata": {},
   "source": [
    "#### Regresión Lineal Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a3b68-6a03-43b0-ba69-da12c83b4b0c",
   "metadata": {},
   "source": [
    "Vamos a empezar con un ejemplo de un dataset sobre coches, sus características, consumo y emisiones de CO2.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26fb6f-9106-459f-b50e-24c9e668e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87666d-990a-44b4-813e-9e6ec68ea1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/FuelConsumption.csv\")\n",
    "cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] \n",
    "cdf.head() ## Dataset reducido con variables de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb11386-9647-4fee-9605-1d429d8b8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf8eac-243c-484a-98a2-328323173fd4",
   "metadata": {},
   "source": [
    "##### Construir el modelo\n",
    "Usando la regresión sklinear podemos predecir el valor de nuestra `y` (`CO2EMISSIONS`) usando otras variables. \n",
    "La regresión lineal simple nos dirá la relación entre dos variables y cómo el cambio de unas afectan a otras. \n",
    "\n",
    "En este ejemplo nuestra variable dependiente será la emisión de CO2, aquello que queremos predecir y elegiremos como variable independiente el tamaño del motor (`ENGINESIZE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714cfb7-55a3-4f15-b482-464b5ff2dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cdf.ENGINESIZE\n",
    "y = cdf.CO2EMISSIONS\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(x, y, edgecolor='black')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Scatter Engine size vs CO2 emissions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a97f4-589d-43cd-aa99-1672044f010d",
   "metadata": {},
   "source": [
    "Este plot parece indicar que las variables están relacionadas linealmente.\n",
    "\n",
    "Vamos a aplicar una regresión lineal para ver cómo ajustaría una línea recta a estos datos. La ecuación de una recta es:\n",
    "\n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1} \\cdot x_{1}$$\n",
    "\n",
    "En esta ecuación, $\\hat{y}$ es el valor de la variable dependiente que vamos a predecir, $x_{1}$ es la variable independiente y $\\theta_{0}$ y $\\theta_{1}$ son los parámetros que determinan la pendiente y la  intersección de la recta con el eje de ordenadas.\n",
    "\n",
    "Con la regresión lineal se estiman los parámetros $\\theta_{0}$ y $\\theta_{1}$ para hallar la recta que mejor se ajusta a nuestros datos. \n",
    "\n",
    "Lo haremos con Sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305de458-5e20-4c2b-9e83-43fea856452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape  # cdf.ENGINESIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50ec3b-cc14-47d1-97e2-152b2c490c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestras variables dependiente e independiente\n",
    "x1 = x.values.reshape(-1, 1)  #Sklearn requiere que los datos estén en la forma (n,1)\n",
    "y1 = y.values.reshape(-1, 1)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ff75f-76d4-48a2-abb8-f3964c872168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "model = sklinear.LinearRegression()\n",
    "\n",
    "# Ajustamos nuestros datos de entrenamiento al modelo.  \n",
    "model.fit(x1, y1)\n",
    "\n",
    "# Calculamos los valores predecidos por el modelo para los valores independientes que tenemos\n",
    "y_pred = model.predict(x1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f41c0-f547-42b3-bdd7-98edff7b575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora lo mostramos sobre el plot anterior\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(x1, y1,  color='black')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Scatter Engine size vs CO2 emissions\")\n",
    "\n",
    "# La recta\n",
    "axs.plot(x1, y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bf2b3-cd85-43f0-8eee-c6992cd9028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamoms los coeficientes o parámetros\n",
    "print('Intercept:', model.intercept_) #theta0\n",
    "print('Coefficients:', model.coef_) #theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f025799-0e5e-4767-ab9e-ffe34f727633",
   "metadata": {},
   "source": [
    "##### Entrenamiento y evaluación\n",
    "\n",
    "En aprendizaje automático es común dividir los datos de los que disponemos en conjunto de entrenamiento y conjunto de evaluación (test). De esta manera podremos comprobar la adecuación de nuestro modelo.\n",
    "\n",
    "Hay varias formas de dividir nuestro dataset original para obtener nuestros subsets de entrenamiento (train) y prueba (test).  \n",
    "\n",
    "- <ins>Train y test del mismo dataset</ins>: Entrenamos nuestro modelo con todo nuestro dataset y luego lo probamos con una parte de él. Este método puede ofrece un alto porcentaje de precisión de entrenamiento cuando usamos el subset de _train_, pero puede dar lugar a un *overfitting* o sobreajuste, que significa que nuestro modelo ha aprendido demasiado bien los ejemplos que le hemos dado, puede capturar ruido y no sería capaz de generalizar si se le dieran otros datos distintos. \n",
    "\n",
    "- <ins>Train/test split</ins>: Dividimos nuestros datos en dos grupos, el train y el test. Entrenamos con el train y luego predecimos el test para ver cómo de buena es nuestra predicción. En este caso, nuestro modelo no ha visto los ejemplos del test con anterioridad, lo que reduce el riesgo de overfitting. \n",
    "\n",
    "- <ins>K-fold cross validation</ins>: Consiste en dividir los datos de forma aleatoria en _k_ grupos de aproximadamente el mismo tamaño, k-1 grupos se emplean para entrenar el modelo y el restante se emplea como validación. Este proceso se repite k veces utilizando un grupo distinto como validación en cada iteración. El proceso genera k estimaciones del error cuyo promedio se emplea como estimación final.\n",
    "    <center>\n",
    "    <img src=\"images/t9_kfold_crossvalidation.jpg\"  style=\"width: 400px;\"/>\n",
    "    </center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d91c3-1720-4878-b2fa-7fabbdbfcd93",
   "metadata": {},
   "source": [
    "Veamos un ejemplo del primer caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0837eb-1c3a-4018-bba7-d27d35ba465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El train es el conjunto de los datos\n",
    "X_train1 = cdf.ENGINESIZE.values.reshape(-1, 1)\n",
    "y_train1 = cdf.CO2EMISSIONS.values.reshape(-1, 1)\n",
    "\n",
    "# Para el test elegimos los últimos 100 valores\n",
    "X_test1 = cdf.ENGINESIZE[-100:].values.reshape(-1, 1) \n",
    "y_test1 = cdf.CO2EMISSIONS[-100:].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdff3d-46f3-40f3-9855-c7dd87693e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto de regresión lineal\n",
    "regr1 = sklinear.LinearRegression()\n",
    "\n",
    "# Entrenamos el modelo usando los sets de entrenamiento\n",
    "regr1.fit(X_train1,y_train1)\n",
    "\n",
    "# Hacemos predicciones usando el set de test\n",
    "y_pred1 = regr1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643810e-f809-4d44-bffb-1a7d427f3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los resultados\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(X_test1, y_test1,  color='black')\n",
    "axs.plot(X_test1,y_pred1, color='blue', linewidth=3, label='regression line')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Test Engine size vs CO2 emissions\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a263a3-e08f-4d1b-909c-a6c68e914cca",
   "metadata": {},
   "source": [
    "Cuando ya hemos entrenado y probado nuestro modelo debemos de evaluar la precisión de nuestras predicciones. Como tenemos los verdaderos valores de la variable independiente podemos evaluar el error de nuestras predicciones, por ejemplo usando el Error Absoluto Medio (MAE), o el coeficiente de determinación ($R^2$), que definiremos más adelante (en este caso, un valor más cercano a 1 significa que nuestro modelo es más preciso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b7636-cc5f-4e07-939a-853fce372127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error: %.2f'\n",
    "      % sk.metrics.mean_absolute_error(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9403b5-bb61-43ba-a7c3-48f438c8eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f'\n",
    "      % sk.metrics.r2_score(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70612da1-ee68-474c-8c8a-1244dee71147",
   "metadata": {},
   "source": [
    "<p/>\n",
    "\n",
    "Probemos ahora con el método *Train/test split*, en el que dividimos los datos en dos grupos, el train y el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf2ae8-aa1a-4679-a551-58af0d760d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos con todos los datos menos los últimos 100\n",
    "X_train2 = cdf.ENGINESIZE[:-100].values.reshape(-1, 1)\n",
    "y_train2 = cdf.CO2EMISSIONS[:-100].values.reshape(-1, 1)\n",
    "\n",
    "# Para el test elegimos los últimos 100 valores\n",
    "X_test2 = cdf.ENGINESIZE[-100:].values.reshape(-1, 1)\n",
    "y_test2 = cdf.CO2EMISSIONS[-100:].values.reshape(-1, 1) # Para el test elegimos los últimos 100 valores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a8712-7911-4b18-b1a4-1246692edb4b",
   "metadata": {},
   "source": [
    "Pero dependiendo de la naturaleza de nuestros datos puede que nos interese obtener muestras aleatorias y hacer nuestros subdatasets variados, por lo que podríamos usar la función [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de la librería Sklearn, que, dada la proporción de muestras que queremos dejar en el test, nos devuelve los 4 grupos (`X_train, X_test, y_train, y_test`).  Con el atributo `random_state` podemos indicar que incluya cierta aleatoriedad en la selección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106104f-423a-4455-89f5-d3ea49436051",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cdf.ENGINESIZE.values.reshape(-1, 1)\n",
    "y=cdf.CO2EMISSIONS.values.reshape(-1, 1)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = skmodels.train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef79c3-8494-4f1a-a5f7-a4a540bde913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos un objeto de regresión lineal\n",
    "regr2 = sklinear.LinearRegression()\n",
    "\n",
    "# Entrenamos el modelo usando los sets de entrenamiento\n",
    "regr2.fit(X_train2, y_train2)\n",
    "\n",
    "# Hacemos predicciones usando el set de test\n",
    "y_pred2 = regr2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14b24c-0eb4-4ffa-9da7-4add0bf6757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los resultados\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(X_test2, y_test2,  color='black')\n",
    "axs.plot(X_test2,y_pred2, color='blue', linewidth=3, label='regression line')\n",
    "axs.set_ylabel('y: Emissions CO2')\n",
    "axs.set_xlabel('x: Engine size')\n",
    "axs.set_title(\"Test Engine size vs CO2 emissions\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbc9ae-77ef-4503-8523-e9e73cd42978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error: %.2f'\n",
    "      % sk.metrics.mean_absolute_error(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77aef9-7030-4c44-9852-67ebccb4e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f'\n",
    "      % sk.metrics.r2_score(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a556031-0c4e-4662-a4ab-0d7013f4746d",
   "metadata": {},
   "source": [
    "<p/>\n",
    "\n",
    "##### Evaluación del modelo. Cálculo de errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95776447-f4f9-440d-a8ff-5f5b2efbeb69",
   "metadata": {},
   "source": [
    "Entendemos como error lo lejos que está un valor de la recta de mejor ajuste. A mayor error, menor precisión de nuestro modelo. Como nos interesa saber la fiabilidad de nuestro modelo tenemos que saber cómo es el error de nuestra predicción. \n",
    "\n",
    "Existen varios tipos de error. Elegiremos la métrica apropiada dependiendo del modelo, del tipo de datos que tengamos, de la naturaleza de estos o del dominio de los datos que poseemos.\n",
    "\n",
    "- Mean Absolute Error (MAE):\n",
    "    $$Mean Absolute Error= \\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\hat{y_{j}} \\mid $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76626fab-8515-4c62-83ff-762b6b97f535",
   "metadata": {},
   "source": [
    "- Mean Squared error (MSE): \n",
    "    $$Mean Squared Error= \\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} - \\hat{y_{j}} \\right) ^ 2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71d995-5a49-4f95-b060-b56fd7d6ed37",
   "metadata": {},
   "source": [
    "- Root Mean Squared Error (RMSE): \n",
    "$$Root Mean Squared Error=\\sqrt {\\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} - \\hat{y_{j}} \\right) ^ 2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9fc39-6a2a-4f1c-ae3b-f03a7d1e1454",
   "metadata": {},
   "source": [
    "- Relative Absolute Value (RAE): \n",
    "     $$Relative Absolute Error= \\frac{\\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\hat{y_{j}}\\mid}{\\frac {1}{n} \\sum_{j=1}^n \\mid y_{j} - \\overline{y} \\mid} $$\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e599d-732b-472c-8447-5d43143ef7be",
   "metadata": {},
   "source": [
    "- Relative Squared Error: \n",
    "$$Relative Squared Error= \\frac{\\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} - \\hat{y_{j}} \\right) ^ 2}{\\frac {1}{n} \\sum_{j=1}^n \\left( y_{j} -\\overline{y} \\right) ^ 2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af72be-6444-47aa-b02f-711cc4b40438",
   "metadata": {},
   "source": [
    "- Coeficiente de determinación ($R^2$):\n",
    "    $$R^2= 1- RSE$$\n",
    "No es un error puramente hablando, es una medida de precisión. En este caso, representa como de cercanos a la recta de regresión son los valores. Cuanto más cercano a uno, mejor será nuestro modelo.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b1151-b717-4218-a112-81c115d645dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error: %.2f'\n",
    "      % sk.metrics.mean_absolute_error(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d987dc-8901-439b-94a3-8e682968a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean squared error: %.2f'\n",
    "      % sk.metrics.mean_squared_error(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a9370-1673-4195-b0a8-75634d258a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Root mean squared error: %.2f'\n",
    "      % sk.metrics.mean_squared_error(y_test2, y_pred2, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c640f1e-49bb-4b03-8478-97382673419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAE_numpy = np.sum(np.abs(np.subtract(y_test2,y_pred2))) / np.sum(np.abs(np.subtract(y_test2, np.mean(y_test2))))\n",
    "print (\"RAE using Numpy: % \", RAE_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f8b5e-9c53-49fe-89a1-0950ea9ade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSE_numpy = np.sum(np.square(np.subtract(y_test2,y_pred2))) / np.sum(np.square(np.subtract(y_test2, np.mean(y_test2))))\n",
    "print (\"RSE using Numpy: % \", RSE_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ba3da-1782-45d0-9492-6813e24d94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f'\n",
    "      % sk.metrics.r2_score(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872773e-839e-42c4-a617-78aa271d76f5",
   "metadata": {},
   "source": [
    "<p/>\n",
    "\n",
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e9_1:** \n",
    "\n",
    "Dado el dataset `casas` que contiene precios de casas dados las características de las mismas. Queremos predecir el precio de nuevas casas (y) basándonos en sus pies cuadrados habitables (x).  \n",
    "\n",
    "Contruir el modelo siguiendo los siguientes pasos:\n",
    "- Definir variables x e y\n",
    "- Dividir dataset en train y test\n",
    "- Definir modelo Simple linear regression\n",
    "- Ajustar datos\n",
    "- Mostrar coeficientes hallados\n",
    "- Dibujar recta de regresión lineal en scatter plot\n",
    "- Evaluar usando Coeficiente de determinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca5953-becc-4889-99aa-b4510dd57add",
   "metadata": {},
   "outputs": [],
   "source": [
    "casas = pd.read_csv('data/kc_house_data.csv', header='infer')\n",
    "casas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580d5f1-668a-4cc0-95b5-cb5edb2710a5",
   "metadata": {},
   "source": [
    "#### Regresión Lineal Múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480846e-a1b2-42b5-8a60-cd2da5365be5",
   "metadata": {},
   "source": [
    "La regresión lineal múltiple se diferencia de la simple por tener más de una variable independiente que influyen en el valor de la variable dependiente. \n",
    "La regresión lineal múltiple se puede usar para estudiar el grado de influencia que el cambio en las distintas variables tienen en la variable dependiente. También se usa para predecir como la variable dependiente cambia cuando lo hacen las variables independientes.\n",
    "\n",
    "En este caso la variable dependiente $y$ es una combinación lineal de las variables independientes X. Por ejemplo, podemos predecir la emisión de CO2 de un coche fijándonos no sólo en el tamaño del motor, sino en el consumo de carburante o el número de cilindros. \n",
    "\n",
    "En este caso la ecuación sería:\n",
    "\n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$$\n",
    "\n",
    "Que viene a ser lo mismo que una multiplicación de 2 vectores, un vector de parámetros y otro de variables, siendo el primer elemento del vector X igual a 1 para mantener el término independiente $\\theta_{0}$.\n",
    "\n",
    "$$\\hat{y} = \\theta^T X$$\n",
    "\n",
    "El objetivo de la regresión lineal múltiple es encontrar los parámetros que minimizan el MSE. Hay dos modos de hacerlo:\n",
    "\n",
    "- Mínimos cuadrados ordinarios. \n",
    "    \n",
    "    Intenta estimar los coeficientes minimizando el MSE usando operaciones algebraicas. Es un proceso complejo y que lleva tiempo, sobretodo con datasets grandes \n",
    "   \n",
    "- Algoritmo de optimización. \n",
    "\n",
    "    Se minimiza el error usando algoritmos iterativos. Un ejemplo muy común es el llamado Gradiente Descendente (_Gradient Descent_) que seleciona los coeficientes aleatoriamente y va calculando los errores y cambiando parámetros hasta que encuentra los valores que minimizan este error. Suele funcionar bien en datasets grandes.\n",
    "    \n",
    "Debemos controlar las variables independientes que usamos, no es conveniente usar demasiadas pues podemos causar overfitting. Es importante conocer la naturaleza de nuestros datos y evitar usar variables que no sean de utilidad o redundantes. \n",
    "Otro punto a tener en cuenta son las variables categoricas. Podemos usar variables categóricas como variables independientes pero deben ser convertidas a numéricas con antelación. Para ello podemos usar el método de One-Hot Encoding que ya vimos. \n",
    " \n",
    "Veamos un ejemplo de implementación de Regresión Lineal Múltiple: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b77823-81b6-4f08-ba84-5a3db89fbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9620123-f6d3-4514-9d24-879946281c14",
   "metadata": {},
   "source": [
    "Como hemos visto anteriormente las variables que usemos en la regresión lineal múltiple tienen que ser variables que tengan un mínimo de influencia en el resultado de nuestra variable independiente. Esto se ve dibujando las importancias de las variables o la correlación entre ellas con el coeficiente de Pearson como hemos visto con anterioridad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e433137-7092-4319-8042-adfa330c291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = cdf.select_dtypes(include=['float64', 'int'])\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ff38f-5390-483b-9211-382db9cf74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = cdf.select_dtypes(include=['float64', 'int']).corr(method='pearson')\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c8d91-cc23-4def-98cc-860aacb335f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot     = True,\n",
    "    cbar      = False,\n",
    "    annot_kws = {\"size\": 8},\n",
    "    vmin      = -1,\n",
    "    vmax      = 1,\n",
    "    center    = 0,\n",
    "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
    "    square    = True,\n",
    "    ax        = ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation = 45,\n",
    "    horizontalalignment = 'right',\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da434687-a449-489f-9e1f-19cadb44dd3b",
   "metadata": {},
   "source": [
    "Hay gran correlación entre ellas, con lo cuál podemos usarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e9793-ddb7-4abb-8115-5987deea20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las variables independientes\n",
    "xm = cdf.drop('CO2EMISSIONS',axis=1)\n",
    "xm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573b70e-5337-4263-8e12-387fc8320f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos nuestra variable dependiente CO2EMISSIONS \n",
    "ym = cdf['CO2EMISSIONS']\n",
    "ym.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c1d3c-7385-42b5-937b-10b0c953f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en train/test\n",
    "xm_train, xm_test, ym_train, ym_test = skmodels.train_test_split(xm, ym, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d95f02-9a3f-4fb3-9deb-fd1ea71a1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto clase Regresión Lineal\n",
    "mregr = sklinear.LinearRegression()\n",
    "\n",
    "# Ajustamos con datos de entrenamiento\n",
    "mregr.fit(xm_train, ym_train)\n",
    "\n",
    "ym_pred = mregr.predict(xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b7f1e-a38c-42b4-af67-40953b97c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos errores\n",
    "score=sk.metrics.r2_score(ym_test,ym_pred)\n",
    "print('r2 score is: ',score)\n",
    "print('mean_sqrd_error is: ',sk.metrics.mean_squared_error(ym_test,ym_pred))\n",
    "print('root_mean_squared error of is: ',np.sqrt(sk.metrics.mean_squared_error(ym_test,ym_pred)))\n",
    "print()\n",
    "\n",
    "# Coeficientes\n",
    "print('Intercept:', mregr.intercept_)#theta0\n",
    "print('Coeficientes:', mregr.coef_)#theta_1-theta_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5806d-6f70-4546-a0e8-90c3055e8fbe",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e9_2:** \n",
    "\n",
    "Intentar mejorar la predicción del precio del DataFrame `casas` del ejercicio `e9_1`, usando regresión múltiple en base a más de una característica. Elegir una o dos más características prometedoras, y ver si se mejora el resultado de R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a6d75-3f59-4b25-8403-17faa3679d64",
   "metadata": {},
   "source": [
    "### Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876a298-c2ba-4e71-8fe0-2b5aa9797389",
   "metadata": {},
   "source": [
    "Los problemas de clasificación son un tipo de problemas de aprendizaje supervisado en el que se categorizan o clasifican elementos en dos o más clases distintas. Un ejemplo de esto es un clasificador que te diferencia imágenes de gato o perro.\n",
    "\n",
    "Por lo tanto la variable que se quiere conocer es una variable categórica y lo que hace el algoritmo es tratar de establecer relaciones entre otras variables y la de interés para ver la influencia de estas otras variables en el resultado de la clasificación.\n",
    "\n",
    "Como en el caso de la regresión, la clasificación se puede hacer tanto para un ejemplo de dos clases distintas como para varias. \n",
    "Esto proporciona una amplia gama de posibilidades para la clasificación. Por ejemplo, la clasificación se puede utilizar para filtrado de correo electrónico, reconocimiento de voz, reconocimiento de escritura a mano, identificación biométrica, clasificación de documentos y mucho más.\n",
    "\n",
    "Hay varios algoritmos de clasificación. Aquí se explicara el método K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1fb5d-5aed-475a-9d8c-ab196f89f684",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656125a7-2673-47e5-aa5c-da4dc20c7ce0",
   "metadata": {},
   "source": [
    "Cuando tenemos un dataset con etiquetas ya puestas podemos plotear nuestros datos según las clases de los datos más próximos.\n",
    "\n",
    "El algoritmo de los K-vecinos próximos (K-Nearest Neighbors) es un método de clasificación que coge puntos ya etiquetados y los usa para aprender a etiquetar datos desconocidos. Se basa en la similitud con casos cercanos. \n",
    "<center>\n",
    "<td> <img src=\"images/t9_knn.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</center>\n",
    "\n",
    "Cómo funciona:\n",
    "\n",
    "1. Elegimos un valor para K\n",
    "2. Calculamos la distancia entre el nuevo caso y los demás casos ya etiquetados del dataset\n",
    "3. Escogemos los K eventos que tienen una menor distancia con el valor desconocido\n",
    "4. Predecir el valor del nuevo evento con el valor más popular de los K puntos más cercanos.\n",
    "\n",
    "Para calcular la distancia de los valores al punto desconocido se pueden utilizar diferentes métricas para la distancia. El ejemplo más típico es la euclidiana en el espacio de los elementos. También se pueden usar pesos para ponderar las distancias y así dar más importancia a los valores más cercanos.\n",
    "\n",
    "Otro aspecto importante para este algoritmo es la elección de una k correcta. \n",
    "Una K muy baja crea un modelo inestable, que puede resultar en *overfitting*, impidiendo la generalización con nuevos datos. No se puede confiar en las predicciones de este tipo de modelos.\n",
    "Por otro lado, una K muy alta entonces el modelo es demasiado complejo. \n",
    "\n",
    "Una solución es reservar una parte de los daros para entrenar y otra para probar la precisión del modelo. Después de probar con varias Ks se evaluará el modelo y se elegirá la K que mejor convenga. \n",
    "\n",
    "Vamos a ver un ejemplo usando un dataset de pacientes con diabetes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0120c8a-5ef5-476f-8118-a4621cfe3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el dataset\n",
    "diab = pd.read_csv('data/diabetes_data.csv')\n",
    "\n",
    "# Comprobamos los datos\n",
    "\n",
    "print('Dimensiones dataset:', diab.shape)\n",
    "diab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d01583-dc8c-4ce6-8f1b-3863c7727664",
   "metadata": {},
   "source": [
    "Nuestro objetivo con este dataset es poder clasificar los pacientes en Diabetes o No diabetes, con lo que nuestra variable dependiente será `diabetes` y las demás variables formarán nuestra X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70427906-0c2e-4310-ae4b-294098dadfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos nuestra X con todas las variables excepto la variable de interés.\n",
    "Xc = diab.drop(columns=['diabetes'])\n",
    "\n",
    "#Comprobamos que hemos eliminado la columna diabetes\n",
    "Xc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d322760-deeb-4256-82e1-543a29196488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos la variable de interés\n",
    "yc = diab['diabetes'].values\n",
    "\n",
    "#Comprobamos\n",
    "yc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d42590-ee72-4d7d-9fc7-b0971a57d68c",
   "metadata": {},
   "source": [
    "Separamos los ejemplos de nuesto dataset en train y test para poder entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abd646-42ad-48ce-818c-3f2a6ac0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test data\n",
    "X_trainc, X_testc, y_trainc, y_testc = skmodels.train_test_split(Xc, yc, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8668c3b-5101-47f3-b4df-7be49c02f6ac",
   "metadata": {},
   "source": [
    "Creamos un modelo KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c5a82-9fa5-4061-b77e-c7b40b9a2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo KNN classifier\n",
    "knn = skneighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# Ajustamos nuestros datos al clasificador\n",
    "knn.fit(X_trainc, y_trainc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17885e4c-97f4-4a3b-96a1-c6bcea326f93",
   "metadata": {},
   "source": [
    "Realizamos la predicción con nuestro test y mostramos los 5 primeros resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e97f6-bcfe-4ccd-b92b-753571079b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predc = knn.predict(X_testc)\n",
    "y_predc[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde0cc6-617b-416a-babe-3b07f6d8e0e8",
   "metadata": {},
   "source": [
    "Comprobamos la precisión de nuestro modelo en los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75314e42-f23a-49fe-b3f5-4c2325273db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_testc, y_testc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24369b52-3fb2-4f0f-adc3-ae46a50d412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_resultados(y_test, y_pred):\n",
    "    conf_matrix = sk.metrics.confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\");\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "    print (sk.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "mostrar_resultados(y_testc, y_predc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ffad3-3cf7-4983-892e-86ad922bf6ad",
   "metadata": {},
   "source": [
    "Esta resultado de precisión ha sido obtenido usando una porción concreta de nuestro data set, pero una representación más adecuada de la precisión de nuestro modelo requeriría hacer una validación cruzada. Esto es repetir el mismo proceso pero cada vez seleccionando una porción distinta de dataset como test y luego haciendo la media de las evaluaciones de predicción. \n",
    "\n",
    "Lo que obtengamos como media será una representación más fiable de cómo se comportará nuestro modelo con nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a46bff-7a59-4b11-a31a-fdb2b752cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new KNN model\n",
    "knn_cv = skneighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Entrenando el modelo con una validación cruzada de 5 \n",
    "cv_scores = skmodels.cross_val_score(knn_cv, X_trainc, y_trainc, cv=5)\n",
    "\n",
    "# Mostramos cada una de las evaluaciones de precisión y la media de todas\n",
    "print(cv_scores)\n",
    "print('\\ncv_scores mean: ', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6871e73-08c1-4255-812f-95193885937c",
   "metadata": {},
   "source": [
    "#### Evaluación de los modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed61d4-4997-43d0-9ab0-e36364d84119",
   "metadata": {},
   "source": [
    "Hay diferentes métricas para evaluar modelos de clasificación, pero aquí vamos a ver dos de ellos: Índice de Jaccard and F1-score.\n",
    "\n",
    "<ins>Índice de Jaccard:</ins> \n",
    "\n",
    "El índice de Jaccard mide la similitud entre el conjunto de valores predecidos (*A*) y el conjunto de valores reales correspondientes a esas predicciones (*B*). Se define como el cociente entre el número de elementos compartidos por *A* y *B* (intersección) y el número de elementos total de *A* y *B* (unión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b24e69-fcd7-44d3-9603-33d97c7b8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0, 1, 1],\n",
    "                   [1, 1, 0]])\n",
    "\n",
    "y_pred = np.array([[1, 1, 1],\n",
    "                   [1, 0, 0]])\n",
    "\n",
    "sk.metrics.jaccard_score(y_true, y_pred, average=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeb096-dbb7-4b2d-92db-cc461db250cb",
   "metadata": {},
   "source": [
    "<ins>F1-score:</ins>\n",
    "\n",
    "Para calcular el *F1-score* es necesario conocer las medidas de precisión y sensibilidad de un modelo.\n",
    "\n",
    "La exactitud (precision en inglés) es una medida de la precisión del modelo que se obtiene al predecir una clase que ya estaba etiquetada. Se define de la siguiente manera:\n",
    "\n",
    "$$Precision=\\frac{True Positive}{True Positive + False Positive}$$\n",
    "\n",
    "La Sensibilidad , exhaustividad o (*Recall*), también se conoce como Tasa de Verdaderos Positivos (True Positive Rate) (TP). Es la proporción de casos positivos que fueron correctamente identificadas por el algoritmo. Se calcula según la ecuación: \n",
    "\n",
    "$$Recall=\\frac{True Positive}{True Positive + False Negative}$$\n",
    "\n",
    "Ahora que sabemos estas dos medidas podemos calcular los pesos F1 (_F1 scores_) para cada etiqueta, basados en su exactitud y sensibilidad. La puntuación F1 es el promedio armónico de exactitud y sensibilidad, donde una puntuación F1 alcanza su mejor valor en 1 (que representa exactitud y sensibilidad perfecta) y su peor valor en 0. Admite casos con más de dos clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5b03b-c724-49e0-827e-5f180d6959c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 0, 0, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1]\n",
    "\n",
    "sk.metrics.f1_score(y_true, y_pred,average=None)\n",
    "\n",
    "#average{'micro', 'macro', 'samples','weighted', 'binary'} or None\n",
    "#determina el tipo de promedio al que se someten los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb409ab1-7d77-495b-817d-ad21a8332559",
   "metadata": {},
   "source": [
    "<ins>La matriz de confusión</ins>\n",
    "\n",
    "Una matriz de confusión es herramienta muy útil para valorar cómo de bueno es un modelo clasificación basado en aprendizaje automático. En particular, sirve para mostrar de forma explícita y visual cuándo una clase es confundida con otra, lo cual nos, permite trabajar de forma separada con distintos tipos de error.\n",
    "\n",
    "Por ejemplo, podemos definir una función que nos calcule la matriz de confusión junto con los valores de precisión, sensibilidad y la puntuación f1. Es útil para saber cómo se comporta nuestro modelo y tenemos esa información recogida de manera muy visual. Se haría de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1830c0-49e4-4622-90ba-afafaa9c3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_resultados(y_test, y_pred):\n",
    "    conf_matrix = sk.metrics.confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\");\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "    print (sk.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "mostrar_resultados(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3ce4a-d1fb-4c00-8da7-49362b94b171",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "\n",
    "**EJERCICIO e9_3:** \n",
    "\n",
    "Dado el dataset `clientes` que tiene a los clientes categorizados en distintos servicios, dependiendo de sus patrones de consumo y valores demográficos (custcat), ajustar los datos usando el modelo K-Nearest Neighbors para predecir en qué grupo categorizar a nuevos clientes.\n",
    "\n",
    "El ejemplo se centra en el uso de datos demográficos, como la región, la edad y el matrimonio, para predecir los patrones de uso.\n",
    "\n",
    "El campo de destino, denominado custcat, tiene cuatro valores posibles que corresponden a los cuatro grupos de clientes, de la siguiente manera: \n",
    "    \n",
    "1- Servicio básico \n",
    "    \n",
    "2- Servicio electrónico \n",
    "    \n",
    "3- Servicio Plus \n",
    "    \n",
    "4- Servicio total\n",
    "\n",
    "Contruir el modelo siguiendo los siguientes pasos:\n",
    "    \n",
    "- Dividir dataset en train y test\n",
    "- Definir modelo K-NN eligiendo el parámetro k\n",
    "- Ajustar datos\n",
    "- Evaluar usando una matriz de confusión y los F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea563e90-648c-48a0-8727-2043e01e1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes = pd.read_csv(\"./data/teleCust1000t.csv\")\n",
    "clientes.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df11ff-2ad3-4b47-8cc6-acb86cd9bef8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Otros aspectos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e83aa-2885-4a6c-800e-704b19273aac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Subajuste (underfitting) y sobreajuste (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4ae92-564e-4b1c-8289-8888ad8d78ad",
   "metadata": {},
   "source": [
    "Cuando entrenamos nuestros modelos computacionales con un conjunto de datos de entrada estamos haciendo que el algoritmo sea capaz de generalizar un concepto para que al consultarle por un nuevo conjunto de  datos desconocido éste sea capaz de sintetizarlo, comprenderlo y devolvernos un resultado fiable.\n",
    "\n",
    "Intentamos “hacer encajar” -_fit_, en inglés- los datos de entrada entre ellos y con la salida. Tal vez se pueda traducir _overfitting_ como “sobreajuste” y _underfitting_  como “subajuste”. hacen referencia a los problemas de nuestro algoritmo para generalizar su capacidad de representación de la realidad que modeliza.\n",
    "\n",
    "Si nuestros datos de entrenamiento son muy pocos, nuestro modelo fácilmente incurrirá en **underfitting**. Ejemplo: entrenamoms con una sola raza de perros y  pretendemos que pueda reconocer otras 10 razas de perros distintas. Un típico síntoma de _underfitting_ es un modelo que siempre cataloga cualquier entrada (independientemente de su tipo real) con la misma clase de salida.\n",
    "\n",
    "Se produce **overfitting** cuando sobre-entrenamos a nuestro modelo, y lo hacemoms ajustarse muy bien a casos particulares, pero es incapaz de reconocer nuevos datos de entrada. Nuestro algoritmo estará considerando como válidos sólo los datos idénticos a los de nuestro conjunto de entrenamiento –incluidos sus defectos– y siendo incapaz de aceptar otras entradas si se salen de los rangos que conoce. Un síntoma de _overfitting_ es un alto porcentaje de aciertos en el conjunto de entrenamiento, pero bajo en el de test.\n",
    "\n",
    "Regresión Lineal\n",
    "\n",
    "<img src=\"images/t9_under_over.jpg\" width=\"550\"/>\n",
    "\n",
    "\n",
    "Clasificación\n",
    "\n",
    "<img src=\"images/t9_knn_under_over.png\" width=\"550\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9a7fa-0284-4fe4-9579-a23ddee03126",
   "metadata": {},
   "source": [
    "Para reconocer y combatir este problema deberemos utilizar adecuadamente la división en datos de entrenamiento y de test, realizar pruebas sucesivas comprobando los resultaos para ajustar nuestro modelo.\n",
    "\n",
    "Debemoms tener en cuenta:\n",
    "\n",
    "- Garantizar una cantidad mínima de muestras tanto para entrenamiento como para test.\n",
    "- Usar clases variadas y equilibradas en cantidad en el entrenamiento (datos equilibrados).\n",
    "- Ajuste de Parámetros: experimentar con diferentes configuraciones, tiempo/iteraciones para el entrenamiento, etc. hasta encontrar el equilibrio.\n",
    "-  A veces conviene reducir la cantidad de características que implementamos en el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556dce2-b970-4fd5-9851-9d0e63e22404",
   "metadata": {},
   "source": [
    "#### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c9b19-1cc9-45be-99ec-c0490200d5e1",
   "metadata": {},
   "source": [
    "Los *hiperparámetros* de un modelo son configuraciones que controlan el proceso de entrenamiento, y determinan los parámetros de configuración que finalmente se fijan para el funcionamiento del sistema.\n",
    "\n",
    "Algunos ejemplos de hiperparámetros utilizados para entrenar los modelos son:\n",
    "\n",
    "- La ratio de aprendizaje en el algoritmo del descenso del gradiente\n",
    "- El número de vecinos en k-vecinos más cercanos (k-nn)\n",
    "- La profundidad máxima en un árbol de decisión\n",
    "\n",
    "El valor óptimo de un hiperparámetro no se conoce a priori para un problema dado. Por lo que se tiene que utilizar reglas heurísticas, usar valores que han funcionado anteriormente en problemas similares, o simplemente buscar mediante prueba y error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6005c4-278d-4444-a2ca-18cc782bad19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Referencias:\n",
    "- Sklearn Preprocessing. https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Probability distributions. https://relopezbriega.github.io/blog/2016/06/29/distribuciones-de-probabilidad-con-python/\n",
    "- Validación de datasets. Dividir datos en train/test. https://www.cienciadedatos.net/documentos/30_cross-validation_oneleaveout_bootstrap \n",
    "- Árboles de decisión. https://www.cienciadedatos.net/documentos/py07_arboles_decision_python.html\n",
    "- Tests estadísticos. https://manualestutor.com/ciencia-de-datos/pruebas-estadisticas-con-python/\n",
    "- Test estadísticos2. https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
