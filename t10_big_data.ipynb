{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9691cfd6-d037-484c-a859-814feb7f0926",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=6>\n",
    "\n",
    "<b>Curso de Análisis de Datos con Python</b>\n",
    "</font>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "Curso de formación interna, CIEMAT. <br/>\n",
    "Madrid, Junio de 2023\n",
    "\n",
    "Antonio Delgado Peris (Cristina Labajo Villaverde)\n",
    "</font>\n",
    "\n",
    "https://github.com/andelpe/curso-python-analisis-datos\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb444083-9f5c-4648-986d-1b1f28bc80a7",
   "metadata": {},
   "source": [
    "# Tema 10 - Manejo de grandes volúmenes de datos y computación paralela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845f2d5-f9ae-45f2-8575-16874e0772c8",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "TODO: finish this\n",
    "\n",
    "- Datasets de gran tamaño.\n",
    "\n",
    "- Larger-than-memory\n",
    "\n",
    "- Polars\n",
    "\n",
    "- Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fd37d-2a7f-43c1-8a3b-1b1a86bd723f",
   "metadata": {},
   "source": [
    "# Trabajando con datasets de gran tamaño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff77107-4948-407e-b2da-495d1bc91f9e",
   "metadata": {},
   "source": [
    "## Problema\n",
    "\n",
    "Al usar algo como ``numpy.load`` o ``pandas.read_*``, cargamos en memoria (a un ndarray o DataFrame) todos los datos de un fichero. Si el tamaño del ficheros es muy grande, estaremos ralentizando nuestro equipo (o, peor, un equipo compartido), o incluso podremos llegar a desbordar la RAM disponible, y producir un error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff247eb0-2f5e-4430-9c7c-b3ee74eb34bc",
   "metadata": {},
   "source": [
    "Vamos a limitar nuestra memoria artificalmente, para ilustrar el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e74f54-e2b6-4da3-bfb7-2fe7e2608b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a299ebba-cae4-428a-99f5-19d1626a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "print(soft, hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61df3e7-d711-469f-bfad-9cce20e70f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_LIMIT = 6*1024*1024*1024  # 6 GB !\n",
    "resource.setrlimit(resource.RLIMIT_AS, (LOW_LIMIT, hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66091ee8-3e22-4983-9f09-e9c89031f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "print(soft, hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f162c6b-e55a-4da5-a06e-a70438fc0fe8",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "También vamos a preparar una utilidad para controlar la memoria capturada por Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f082d-45d1-4cd4-9538-426fcceeb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "\n",
    "def getPythonMemory():\n",
    "    process = psutil.Process()\n",
    "    print(process.memory_info().rss/1024/1024)  # in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04edca-1548-45a1-b3d3-c6cc0fdde4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a44958-6390-4399-8c73-4f472ee33f1c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Ahora vamos a intentar leer ficheros de gran tamaño con Pandas.\n",
    "\n",
    "El primer caso debería funcionar sin problemas (fichero de 2.3 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb591f-bda4-4d4c-9db0-c9045b554d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fNormal = \"../data/kc_house_data.csv\"\n",
    "fBig = \"../data/BIG_house_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6069a-6620-4785-9c90-09c6b51a34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7f7b4-342c-40c3-b6cd-464d6b2ce4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37998ee-9dc4-4ee7-a58b-3907e49d29c9",
   "metadata": {},
   "source": [
    "Sin embargo, el siguiente ejemplo debería fallar (fichero de 1.2 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb94634-4512-46bb-949b-1179234ccbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fBig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a39a9-cd85-4f1a-a67c-f2af41f78491",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524a2d1-5aa9-46a2-a068-37e74c1cbb31",
   "metadata": {},
   "source": [
    "## Posibles soluciones\n",
    "\n",
    "Vamos a explorar algunas posibilidades para afrontar este problema (existen otras).\n",
    "\n",
    "1. Leer solo los datos que realmente necesitamos\n",
    "2. Utilizar tipos de datos más pequeños\n",
    "3. Trocear el archivo\n",
    "4. Usar otras librerías como Polars\n",
    "5. Utilizar sistemas distribuidos como Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db5477-7dfe-4979-a383-c0f3ec7c40ee",
   "metadata": {},
   "source": [
    "### Leer solo los datos que realmente necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1e0b7-7e98-47ad-a347-312835dcda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6f95a-1343-44f3-bcee-fe9e6b8691c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=False, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70999eb-8a32-48f5-a948-32b886b6942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9413d9-dfa7-43e4-b260-1aa7b3fd6bc6",
   "metadata": {},
   "source": [
    "Vamos a leer solo 3 columnas de nuestro fichero. Veremos que el DataFrame ocupa menos memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d6b71-8a35-4bf1-9e9d-416cd9f12893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(fNormal, usecols=[\"id\", \"price\", \"bedrooms\"])                               \n",
    "df2.info(verbose=False, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b0789-905c-4c72-aba6-c63c1a34bdd1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Ahora probamos lo mismo con el fichero de mayor tamaño. Esta vez deberíamos tener éxito.\n",
    "Nota: previamente, intentamos limpiar la memoria ocupada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509fdb5-5e75-495d-a121-2e5ad2a20518",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa866b-8382-467d-ab8e-7fab1df28979",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df2\n",
    "del pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc59e5-f43e-4bb6-97b5-69dc2302eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29002cf-e8b6-462f-85aa-35a2faff2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25498b63-a7ae-4924-82be-3abbe7757f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd80bdf-fb7a-486d-9998-9b0c26461d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(fBig, usecols=[\"id\", \"price\", \"bedrooms\"])                               \n",
    "df2.info(verbose=False, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6cad7-4c9e-44ef-9ceb-8337cac3599f",
   "metadata": {},
   "source": [
    "### Utilizar tipos de datos más pequeños"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a20ee-acdb-4a37-875c-76b33d2e34b0",
   "metadata": {},
   "source": [
    "Por defecto, Pandas utilizar `int64` para datos enteros, pero si el rango de nuestros valores es pequeño, podemos utilizar un tipo más reducido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0fea2-fe26-4d80-b7dd-0fe0a17b98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['bedrooms'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfae1d-bb05-440f-8199-487c7e8c2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['bedrooms'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec272f78-897f-4cdd-8cb7-4bbcdaad8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(fNormal, usecols=[\"id\", \"price\", \"bedrooms\"], dtype={'bedrooms': 'int8'})\n",
    "df3.info(verbose=False, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9e292-075c-4c55-b4f2-df7f8ffee812",
   "metadata": {},
   "source": [
    "<br/>\n",
    "En otras ocasiones, podemos aplicar un cambio similar aunque perdamos algo de precisión. P.ej. si pasamos\n",
    "de `float64` a `float32` (o incluso `int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a4dee-1fe3-4ade-b747-424d12bc55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb268f-5f25-4f38-8b50-e6137dee1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811aa7c-c3a2-45e5-91f1-12e762e5d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.price = df4.price.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b6654-09f7-42c0-9c6c-da47ec379808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info(verbose=False, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12717d85-64f1-4146-8b51-415acc7d98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que no hay diferencia entre las dos columnas\n",
    "df3.price.compare(df4.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb0796-f08a-4176-8366-146272a354b5",
   "metadata": {},
   "source": [
    "En otras ocasiones, podríamos aplicar un cambio similar aunque perdamos algo de precisión. P.ej. si pasamos\n",
    "de `float64` a `float32` (o incluso `int`).\n",
    "\n",
    "\n",
    "Finalmente, otra posibilidad sería convertir determinadas columnas numéricas a tipos categóricos, que como ya vimos en el tema 5, pueden necesitar menos memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfa917-896b-485b-b408-276548f76770",
   "metadata": {},
   "source": [
    "### Trocear el archivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f543ae7-b9b2-4209-9aab-3d3d60b1d4ac",
   "metadata": {},
   "source": [
    "Hasta ahora siempre hemos leído todas las líneas de un fichero de una sola vez. \n",
    "\n",
    "P. ej., así calcularíamos la longitud y el valor medio de la columna `price` de un fichero completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece9d8f-ffa8-45a5-b078-19359d20fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fNormal)\n",
    "print(f\"Longitud total es {len(df)}\")\n",
    "print(f\"Precio medio es {df.price.mean()/1000:.2f} k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08993772-946b-4a96-9f74-c81a4175b1e7",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Pero Pandas también nos permite leer `chunks`, \"trozos\", de un fichero, y, así, podríamos realizar el mismo proceso sobre trozos del fichero, iterativamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff327eb6-78bb-4633-9442-65cfbbcc1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlines = 1000\n",
    "total_length = 0\n",
    "\n",
    "iters = 0\n",
    "with pd.read_csv(fNormal, chunksize=nlines) as reader:\n",
    "    for chunk in reader:\n",
    "        total_length += len(chunk)\n",
    "        iters += 1\n",
    "\n",
    "print(f\"Número de iteraciones: {iters}\")\n",
    "print(f\"Longitud total es {total_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39f1e4-c4a2-46b0-9d80-9f156311c611",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "De nuevo, si intentáramos cargar un archivo muy grande de una sola vez, tendríamos un error. Pero, en este caso, podemos obtener los mismos resultados usando la lectura por partes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356f2e0-f0e0-44fc-8096-436ca1bf4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumPrice = 0\n",
    "nlines=50*1000\n",
    "\n",
    "iters = 0\n",
    "with pd.read_csv(fBig, chunksize=nlines) as reader:\n",
    "    for chunk in reader:\n",
    "        sumPrice += chunk.price.sum()\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce77ed-b94e-444d-b81f-3e18766d0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de iteraciones: {iters}\")\n",
    "avgPrice = sumPrice/(iters*nlines)\n",
    "print(f\"Precio medio es {avgPrice/1000:.2f} k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46120d7-ccbd-4084-b0d7-0a65463d6b84",
   "metadata": {},
   "source": [
    "### Usar librerías como Polars\n",
    "\n",
    "Librería para _DataFrames_ con énfasis en la velocidad de procesamiento. Está implementada en Rust y usa Apache Arrow para su modelo de memoria (columnar). Es una alternativa a Pandas, con un interfaz similar (aunque más parecido a `dplyr` de R), diseñado desde el principio para la computación paralela (multi-thread, en una máquina)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3226278-32b5-4e4d-9ef6-59fbb8a6bec6",
   "metadata": {},
   "source": [
    "En primer lugar, vamos a realizar un cálculo con pandas, y repetirlo con Polars, para ver los tiempos de ejecución que consiguen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34237ec5-1c72-4c16-aae1-29eaf54e8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pandas\n",
    "df = pd.read_csv(fNormal)\n",
    "res = df[df.price > 5000000]\n",
    "avg = res['price'].mean()\n",
    "print('Num expensive:', len(res))\n",
    "print('Average:', avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65432bda-36f3-4966-b9d9-75d240644663",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Ahora lo hacemos con Polars, y vemos que es más eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7dfad-6fe3-4aec-af1a-0dc6083f2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255010db-5f58-4a53-ae80-66bf5022b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Polars \n",
    "df = pl.read_csv(fNormal)\n",
    "res = df.filter(pl.col('price') > 5000000)\n",
    "avg = res['price'].mean()\n",
    "print('Num expensive:', len(res))\n",
    "print('Average:', avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e481b9-d060-41be-98a9-57794fe2c2b6",
   "metadata": {},
   "source": [
    "<br/>\n",
    "A continuación, vamos a intentar hacer lo mismo con el fichero de gran tamaño.\n",
    "\n",
    "Como antes, no podemos hacerlo directamente con Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5af28-8120-4536-b988-bf16f020fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pandas\n",
    "df = pd.read_csv(fBig)\n",
    "res = df[df.price > 5000000]\n",
    "avg = res['price'].mean()\n",
    "print('Num expensive:', len(res))\n",
    "print('Average:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7ecf2-1df1-4890-beff-8c6a23259a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f910e1-2806-4f43-821e-6e474b8723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b2d24-2c65-46df-a193-e96a692a863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec31e32-b634-4a91-84fc-41067e945fea",
   "metadata": {},
   "source": [
    " <br/>\n",
    " Podemos recurrrir, como antes, a leerlo por trozos (de nuevo, con Pandas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abe1ba-7505-448c-85db-e9346425cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sumPrice = 0\n",
    "nlines=50*1000\n",
    "numExpensive = 0\n",
    "\n",
    "iters = 0\n",
    "with pd.read_csv(fBig, chunksize=nlines) as reader:\n",
    "    for chunk in reader:\n",
    "        res = chunk[chunk.price > 5000000]\n",
    "        numExpensive += len(res)\n",
    "        sumPrice += res.price.sum()\n",
    "        iters += 1\n",
    "        \n",
    "print(f\"Número de iteraciones: {iters}\")\n",
    "avgPrice = sumPrice/(iters*nlines)\n",
    "print('Num expensive:', numExpensive)\n",
    "print('Average:', sumPrice/numExpensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e902b8-c158-43ac-a8c8-e6d21c190366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average:', sumPrice/numExpensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d1fe0-9db2-4c1f-8d1b-52cab95a16ff",
   "metadata": {},
   "source": [
    "Como vemos, esto nos obliga a complicar nuestro código, y además necesitamos mucho más tiempo de procesado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45537b76-caef-405e-8d40-b7ca73de1b0c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Con Polars, podemos usar el modo `streaming`, de manera más sencilla, y más eficiente (primero, liberamos memoria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3aced-3a7a-42a7-bed0-e5e05d5bf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5058b-ef0a-400c-a540-377de1bfeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7688f52-1730-4961-9261-e5b4c1a275d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97977a8-cfa5-4a7f-80bd-cf41a65dc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPythonMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d39b90-18d7-42cc-b9a9-8a8b42be5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f879228-3fb9-428c-bb00-0be7318e51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pl.scan_csv(fBig, low_memory=True)\n",
    "res = df.filter(pl.col('price') > 5000000).collect(streaming=True)\n",
    "avg = res['price'].mean()\n",
    "\n",
    "print('Num expensive:', len(res))\n",
    "print('Average:', avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bc11a-0e10-4177-b0ba-198c31ebfb2d",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Sin embargo, hay que tener cuidado. Este código Polars es más eficiente en memoria, pero también tiene sus exigencias.\n",
    "\n",
    "Si bajamos el límite más, llegaremos a desbordar la memoria disponible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4f100-9395-44ed-9a20-2c9dfd575c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_LIMIT = 3*1024*1024*1024  # 3 GB\n",
    "resource.setrlimit(resource.RLIMIT_AS, (LOW_LIMIT, hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bceddfa-536e-43d4-b558-ca09b718dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pl.scan_csv(fBig, low_memory=True)\n",
    "res = df.filter(pl.col('price') > 5000000).collect(streaming=True)\n",
    "avg = res['price'].mean()\n",
    "\n",
    "print('Num expensive:', len(res))\n",
    "print('Average:', avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05bb2a-187d-4bfa-bc6c-b2177fd0274f",
   "metadata": {},
   "source": [
    "### Usar un entorno distribuido como Dask\n",
    "Usando `Dask` podemos dividir el trabajo de procesar un Dataframe entre varios _workers_, de manera que cada uno de ellos necesitará menos memoria. Los `workers` pueden correr en la misma máquina, o bien en un cluster distribuido, en la nube, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb9a5c-ed1b-419f-8d6a-1e6354cfb961",
   "metadata": {},
   "source": [
    "Veremos el ejemplo con Dask en la siguiente sección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974c9e2-2114-4ef3-8234-12a54810b8e7",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7dec2-7769-4b0e-8737-8d29036841d7",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Dask es una librería para computación paralela y distribuida, que puede escalar el ecosistema existente de librerías de datos de Python. Además, Dask provee mecanismos para trabajar con datos de grandes dimensiones (_larger-than-memory_). Dask permite escalar desde una máquina a la nube.\n",
    "\n",
    "Dask ofrece APIs para trabajar a alto o bajo nivel:\n",
    "\n",
    "- Alto nivel: Dask ofrece _colecciones_, con interfaces similares a los de NumPy o Pandas, pero que pueden trabajar de forma paralela (rápida), y por partes (para soportar grandes conjuntos de datos). Las colecciones son `Array`, `Bag` y `DataFrame`.\n",
    "\n",
    "- Bajo nivel: Si las colecciones de alto nivel no son apropiadas para un determinado problema (p.ej. no queremos procesar unos datos datos, sino realizar determinada computación), entonces podemos usar estructuras de bajo nivel como _Delayed_ o _Futures_ para conseguir computaciones distribuidas personalizadas.\n",
    "\n",
    "Casi siempre que usemos Dask, estaremos utilizando un planificador de tareas distribuidas, en el contexto de un cluster Dask:\n",
    "\n",
    "<img src=\"images/t10_dask_cluster.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102ecda-8b01-4e80-b663-7f0dbb77bf32",
   "metadata": {},
   "source": [
    "## Dask scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5dd510-07fd-4a0a-a687-f746dcd37f55",
   "metadata": {},
   "source": [
    "### Prefacio: Python GIL\n",
    "TOOD: hablar sobre el GIL, y algunas maneras de evitar el problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f30b2-813c-4402-a267-8535989abdaf",
   "metadata": {},
   "source": [
    "### Schedulers en Dask\n",
    "TODO: completar con info desde https://docs.dask.org/en/stable/scheduling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f492b3-d9b6-4e9d-ac57-8a4b0aaccba5",
   "metadata": {},
   "source": [
    "## Dask Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153af0e-6e1a-4478-a760-9718c5efb0e8",
   "metadata": {},
   "source": [
    "Un Dataframe de Dask es una estructura distribuida compuesta por varios Dataframes de Pandas (separando por conjuntos de filas). Las diferentes partes de un Dataframe de Dask estarán repartidas entre varios _workers_ de Dask, que pueden residir en la misma máquina o en diferentes máquinas de un cluster.\n",
    "\n",
    "<img src=\"images/t10_dask_dataframe.png\" width=\"400\"/>\n",
    "\n",
    "Si el Dataframe de Dask está en una sola máquina pero es demasiado grande para ser cargado en memoria, Dask lo lee por partes desde el disco.\n",
    "\n",
    "Nota: si un Dataframe cabe en memoria, entonces usar Pandas directamente puede ser más eficiente, salvo que se requiera un procesado muy pesado que se beneficie del paralelismo de los _workers_ de Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64249a64-f048-4032-91ef-8c0839770616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos un cluster distribuido disponible (p.ej. lanzado desde la extensión Dask para Jupyter)\n",
    "# podemos conectarnos a él, usando el siguiente código (con la IP y puerto correctos)\n",
    "from dask.distributed import Client\n",
    "\n",
    "port = \"XXXX\"\n",
    "client = Client(\"tcp://127.0.0.1:\"+port)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fd198-7bc1-4f16-ad1c-2a0d366ec03e",
   "metadata": {},
   "source": [
    "TODO: dar la opción de usar un scheduling local (multi-thread), para entornos donde no hay un cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71f6c-94c9-43bc-be0c-559537ca638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4cc49-842f-49c0-90b4-aeaa4c5b27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fBig = \"../data/BIG_house_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e61c7-32a2-497e-93f8-f986f2a98bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(fBig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc6950-68c7-4676-a5d4-e18b99bbeb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b623c-6380-4520-bc1d-e8f796826eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfd627-a835-4666-85a9-aeac54338fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = df[df.price > 1000000].groupby(\"zipcode\").bedrooms.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5bcc2b-85a1-4188-9f86-3dfdc42af7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1950571-8ddb-4cdd-90ab-b1ec0e133880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28579c-fb25-4579-9110-348df3142d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08837b-15fd-42e8-8880-789e35eb1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf =  dd.read_csv(fBig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf5897-9bde-404b-adae-28d86af064d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e253e0b-7881-4a18-8758-0eaa75d0a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce66ae-ae82-4993-8381-d76b56aef089",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ac6a3-8a4c-4532-9feb-b28684d7ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf2 = ddf[ddf.price > 1000000].groupby(\"zipcode\").bedrooms.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1bcac-6432-4025-ba84-af6eb4124bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ddf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1fe99-18df-4041-94fd-f754c14aac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ecfe5-64fb-40af-84a8-495be6c714e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = ddf2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f47c4-d28d-49c7-b858-3a1fb0f0bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fce225-d2f5-4f6a-84bc-39ef1557f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3784a-861a-4d23-8271-91cd6ca81336",
   "metadata": {},
   "source": [
    "Ahora limitamos la memoria y comprobamos que Dask sigue pudiendo realizar la operación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb065dce-7959-4715-b714-f2264ddf1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "limit = 1*1024*1024*1024  # 1 GB !\n",
    "resource.setrlimit(resource.RLIMIT_AS, (limit, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e50de-d5cc-4db7-a0df-36b5eeda86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Lo hacemos de nuevo con Dask\n",
    "ddf =  dd.read_csv(fBig)\n",
    "ddf2 = ddf[ddf.price > 1000000].groupby(\"zipcode\").bedrooms.mean()\n",
    "res = ddf2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee0db1-de1d-4083-9e2f-8b0e0fafef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49dc83-c6fd-49f6-8658-317e80273094",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc178c2-ec89-4c10-a208-8c08e08c495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hacerlo con pandas directamente, ahora no es posible\n",
    "df = pd.read_csv(fBig)\n",
    "df2 = df[df.price > 1000000].groupby(\"zipcode\").bedrooms.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18557091-bc88-4007-8d3a-be1cb64936c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "resource.setrlimit(resource.RLIMIT_AS, (-1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c285ac-914e-4ead-872b-10d5717289b6",
   "metadata": {},
   "source": [
    "## Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401346d-fd5e-4eb7-95d8-cc34dc378c63",
   "metadata": {},
   "source": [
    "Como hemos comentado, con Dask _Delayed_ podemos paralelizar código arbitrario, sin usar una colección Dask de alto nivel.\n",
    "\n",
    "Partamos de un ejemplo no paralelizado (incluimos un _sleep_ para simular una computación costosa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11c5df-3de4-4d0e-b7d2-1eeed5309ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def calc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def merge(vals):\n",
    "    return sum(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc9b5-0592-470f-b505-25225fb1f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Esta función tardará 1 segundo por cada argumento incluido\n",
    "\n",
    "vals = [5, 10, 2, 7, 9]\n",
    "res = []\n",
    "for val in vals:\n",
    "    res.append(calc(val))\n",
    "merge(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf719463-41f7-43fe-a7fc-418507a0134c",
   "metadata": {},
   "source": [
    "Vamos ahora a hacer lo mismo con Dask Delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a27866-e884-41d4-93e4-77a58d6ac18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "def calc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "@dask.delayed\n",
    "def merge(vals):\n",
    "    return sum(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb6daa-cd66-4878-91f1-2a594b6c1e4c",
   "metadata": {},
   "source": [
    "El código siguiente es inmediato, porque solamente construye el gráfico de operaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158200d7-b18e-4c4a-b20a-bcdb5e3b3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vals = [5, 10, 2, 7, 9]\n",
    "res = []\n",
    "for val in vals:\n",
    "    res.append(calc(val))\n",
    "z = merge(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76119649-61c3-44ff-aa22-6c0f44798a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5909df-20e0-4c2e-a214-7c64be837f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7d615-d3f7-4f53-8afc-fae6853499d4",
   "metadata": {},
   "source": [
    "Ahora realicemos la verdadera computación (en paralelo!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c28ab-f1e8-460f-9fd8-f9b64fc38b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ahora \n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9b425-c557-42d7-bb92-65b72fcf36c7",
   "metadata": {},
   "source": [
    "Como todas las operaciones `calc` se han realizado a la vez, el tiempo total ha sido de solo 1 segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21621d4-ca1f-44c1-b868-48f8b00293b6",
   "metadata": {},
   "source": [
    "</b>\n",
    "\n",
    "Para acabar, cerraremos el cliente Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075b72c-e997-42f0-a170-4ab531a0d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
